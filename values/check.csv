Id,Body
448250,"<p>The kinds of requirements you need to store data is <em>very</em> different from the kind of requirements you need to run a set of computationally complex calculations on a data set, and while this is not the only use case it <em>especially</em> applies if you're running into exponentially expanding complexity (or close to it) due to the interactions in your large data set.</p>
<p>Just to create an example, it's fairly straightforward to store the data for customers, products, stores (including inventory) and purchases. Those are 4 CRUD endpoints that hardly interact with one another. CPU-wise, you don't need much.</p>
<p>But let's ask ourselves the question if the customer bought products optimally, i.e. in a way that the total commute distance from the customer's home to the store where they made a purchase is minimal (across all purchases made).<br />
This requires a lot of calculation complexity. Not only do you have to calculate distances, you also have to consider that if you find that Customer A lives closer to store B than to store A, and that these stores both sell the same product, you have to also account whether store B's inventory has one of these products to spare. And if there's multiple customers who could go to store B instead, which ones should we shift around to maximize our improvements?</p>
<p>I've intentionally picked an example that leads to many different possible subcomplexities and calculation to show you the difference between straight up storing data and operating on it on a per-entity basis, compared to the kind of interaction and complexity that reporting on that same data can bring with it.</p>
<p>AWS Redshift or Google BigQuery are tailored towards big data operations, and therefore will be able to run your reporting logic better and faster than your usual hardware can.</p>
<hr />
<p>There's also some side points:</p>
<ul>
<li>What if your usual hardware does not have the kind of spare capacity for running reports, i.e. if your normal use case does not accept any kind of performance degradation?</li>
<li>The point made above is not too dissimilar from asking why search engines exist as a remote third party service (as opposed to being run locally by either the searcher or the websites themselves), albeit that this is more blatantly obvious due to the network overhead in needing to call every site in existence.</li>
</ul>
<blockquote>
<p>wouldn't a failed assertion on data in the data warehouse mean that something is not right with the data from the data source(s)?</p>
</blockquote>
<p>Be very careful about negating that statement. Yes, if your extraction already fails, there's clearly something wrong. But if your extraction <em>doesn't</em> fail, does that prove that your data is therefore all correct? <strong>No</strong>. And that's the more important consideration of the two.</p>
<blockquote>
<p>wouldn't it be better if the ETL used my app's API to extract the data instead of directly connecting to the operational (in this case, Mongo) database?</p>
</blockquote>
<p>I generally favor considering the datastore a private implementation detail of the service and therefore routing everything via the service. However, this decision is scoped to <em>application use</em>. I do make exceptions for infrastructural operations, e.g. datastore backups. Your ETL can similarly be considered an infrastructural operation that gets special access to the raw data. There are justifications for this:</p>
<ul>
<li>Maybe it's because it would put an undue burden on the service whose performance would degrade.</li>
<li>Maybe the service has a complex business layer which is plainly irrelevant for the purposes of the ETL.</li>
<li>Maybe the service is only tailored towards end-user interactions (per-entity) as opposed to bulk data operations.</li>
<li>Maybe your datastore contains audit logs that are absent from the service's output by design.</li>
</ul>
<p>I don't know which one would apply in your case since I don't know your case.</p>
"
445186,"<p>It seems you are designing a simple database query engine, and are now looking for efficient cloud-native approaches to handle the backing storage.</p>
<p>This is ultimately a question of cost minimization, balancing factors such as storage costs, costs of executing a query, and development costs.</p>
<p>Some points to consider:</p>
<ul>
<li><p>Keeping the original CSV file is simple, but requires re-processing the entire file for each query, which might be a bit inefficient if those files are large.</p>
</li>
<li><p>Compression might reduce storage costs. Decompression tends to be really fast.</p>
</li>
<li><p>You might transcode the data into a different, more efficient format. For example, binary formats will be more compact especially for numerical data, and allow processing without having to re-parse the file. Data formats like Apache Parquet might be worth looking at. However, this might require that you can assign types to each column, which might be tricky in practice (e.g. phone numbers might look like integers but are not).</p>
</li>
<li><p>You might split up the data into individual columns or records. Now, executing a query will require multiple sub-queries, but you might be able to perform a query without having to load all of the data. For example with columnar storage, you only have to load the <code>age</code> column in order to evaluate an age filter. If you put the data into some existing database, this will simplify query evaluation for you. But since you don't have a clear data schema, you would be limited to non-relational databases like document databases, or would have to use an inefficient EAV design.</p>
</li>
</ul>
<p>I suspect that your volume of data is very low (only a couple of GB), that individual documents are fairly small (a couple of MB), and that your rate of queries is very low (only a couple of thousands of requests per day), so that your storage-related costs are in the cents-per-month region where significant optimization is infeasible. However, development costs are orders of magnitude larger, so that more complicated approaches are likely to be uneconomical. If you're looking for faster processing with moderate development effort, a sensible initial step would be to pre-process the uploaded data into a compressed binary data format like Parquet.</p>
"
444739,"<p>I get the feeling you are about to fall into infrastructure oversizing. Your doubts are reasonable, but they also say that you don't have any fact that you need such environment segmentation. At least not yet.</p>
<p>What do you have so far? You have a monolith, which is developed and operated across three environments. They are already provisioned with everything the solution needs to be functional. I can't think of a better starting point to test (and compare) the breakdown of the monolith.</p>
<p>As more services appear in the ecosystem, the more likely you need the flexibility to test different permutations of services. That flexibility is achieved by configuration and parameterization.</p>
<p>Bear also in mind that environments are not mere infrastructure segmentations. Environments are composed of runtime and data. Data on production is often subject to regulatory compliance, so you don't assume easily that &quot;development should use the production services&quot;. It's not that simple.</p>
<p>Think also of the costs. If you were to deploy production on some of the well-known public cloud platforms, you would realize how using production services for testing development can wipe your budget in a breath.</p>
<p>Another reason to avoid mixing up environments is metrics. Cross-env resource utilization can invalidate or distort the lecture. It's going to be hard to monitor Production for adaptions if random activity in Development (or other envs) is generating &quot;noise&quot;.</p>
<p>As an MS architect, your mind should always be on the big picture. That involves the software but also infrastructure, security, operations, cost-optimization, regulatory compliances, tracking, metrics, business goals, etc.</p>
"
444704,"<p>I am trying to figure out how to design SaaS system that offers subdomain per customer (e.g. <code>&lt;customer&gt;.example.com</code>), where each customer is on specific region.</p>
<p>Due to compliance and regulation, some customers must have all data in specific regions. There are two deployments of existing system (US and EU) with different domains. System is available only on these domains and customer must know on which system they are in order to access it.</p>
<p>Now, I want to have domains in form <code>&lt;customer-name&gt;.example.com</code> and have it automatically use deployment in appropriate region.</p>
<p>Two approaches come to mind:</p>
<ol>
<li><p>have single onboarding endpoint and during onboarding contact services in appropriate region, including configuring DNS to point subdomain to services in appropriate region.</p>
</li>
<li><p>have <code>*.example.com</code> DNS record point to global proxy that knows how to route requests to appropriate region based on <code>*</code> part of the URL by checking where the customer should go.</p>
</li>
</ol>
<p>Cloud provider is AWS, Route53 for DNS management and EKS is used. Current deployments are completely separated (except for CI, monitoring and such). I am looking for solution to utilise managed services as much as possible to reduce implementation time and maintenance overhead.</p>
<p>Option #1 is not ideal as it requires dynamically setting DNS records and potentially exposes clients names by enumerating existing DNS records.</p>
<p>Option #2 introduces additional latency and potential compliance problems (in which region is proxy hosted?).</p>
<p>Are there any other options that I am not seeing? Are there some useful articles or resources on this topic?</p>
"
442126,"<p>My team have recently inherited a very poorly written, business-critical, monolithic LAMP application with the goal of guaranteeing reasonably uptime and scalability targets in just a few weeks. Our deadline is somewhat fixed because it depends on user volumes over the peak season, which we forecast will start picking up around Black Friday.</p>
<p>The monolith is currently deployed to a single, &quot;snowflake&quot; EC2 in AWS. Deployments to this instance are entirely manual. The biggest operational risk we're currently trying to mitigate with this instance is that it currently has a dynamic IP attached, which the cloud provider would likely recover if the instance rebooted due to a server crash or maintenance work. We've spoken to AWS about it and they've confirmed they would not be able to give the IP back to us. This would break dozens of integrations that are (wrongly) currently pointing to this ephemeral IP address start failing. By some miracle though, the VM has been stable enough not to crash once so this situation has never happened before but that doesn't mean the risk of it happening is not there, especially during peak traffic.</p>
<p>The monolith is very tightly coupled to the Production environment in which it runs, as it has a ton of hardcoded references to its own Production ephemeral IP address. Moreover, all of its application config is hardcoded and duplicated across every dozens of file. All of these settings assume they're running in a Production environment too.</p>
<p>We want to make the minimum amount of changes to the application code to allow us to replace its dynamic IP with a reserved, static one. However, because the monolith is not modular at all and there is little to no code reuse in it, we've basically had to change hundreds of files just to be able to fetch all the application settings from a single config file. This has allowed us to deploy a version of the monolith with Test settings to a Test environment that we're able to run manual tests against.</p>
<p>Even though each individual change we've made is very small, we have basically touched almost every source file in the codebase. The people that wrote this legacy application and therefore had any good understanding as to how it works no longer work at our company. With no support from domain experts and no automated test coverage whatsoever, we're looking at a pretty risky, time-sensitive deployment that we know needs to happen as things could get pretty ugly for us if the instance reboots or dies for whatever reason and we lose its IP.</p>
<p>So the main question facing us now is, what would be the safest way to make this deployment happen as safely as the circumstances allow? We've also ruled out accomplishing this through multiple deployments because of how tightly coupled and messy the code is.</p>
<p>I'm not looking for any silver bullets here. Any ideas or suggestions would be more than welcome.</p>
"
441569,"<p>I'm currently developing a chatbot that will be used for booking trips. The particular party I'm interested in for now is the rider. I'm using AWS's DynamoDB, because it seemed to be the most flexible and fast for a system like this that is likely to get lots of concurrent traffic, and in which records won't have well-defined schemas.</p>
<p>The idea is, the rider greets the bot, the bot responds asking them if they're a rider or driver. For now, I'm only concerned about the rider. The first and most obvious thing is to create an item in the DB of the user, identified by the phone number of the user. in addition to that, I should also probably store the role of the user as an attribute, so that in the future they don't have to repeat this first step.</p>
<p>From here, the rider then has the option to view active trips (ones that have been booked but not completed), cancel a trip, or book a new trip. The trip details will all be stored on a different database I handle elsewhere. The only thing that I will store concerning the trip in the message history database is the trip's id, so that I can retrieve the relevant details of the trip from the other database using the id.</p>
<p>The next steps in the flow should be straightforward.</p>
<p>Here's my problem. I have to track context to know where in the flow the user is. How I'm thinking of doing this is by creating a conversation item in the database that then tracks the context/topic with a user. Then, when the user enters into a new part of the flow, the context for that conversation is updated, and the chatbot knows what response to then give.</p>
<p>From what I read in the AWS docs, they recommended that you use a single-table design for this. But I'm struggling to understand how exactly I would structure the database in this case, and particularly in an efficient manner. Would I set the partition key to be the user_id (phone number), and then store lists of messages and conversations associated with that number?</p>
<p>I'd appreciate any help with this problem.</p>
"
440067,"<p>Designing, implementing, and verifying application-side controls to manage which shard or cluster contains the data and that reads and writes are happening against the shard is probably going to be more expensive than taking advantage of out-of-the-box clustering and replication solution.</p>
<p>Availability begins with the data center. Using any reputable data center would get you things like physical security, redundant power and networking connectivity, fire control systems, and HVAC. You'd need the redundant power and networking to start to achieve the desired uptime, and the other physical safety controls would help in disaster scenarios.</p>
<p>Once you have the data center under control, the mainstream file and data storage tools offer built-in configurations around sharding and replication. These implementations are probably going to be more robustly designed and tested than any in-system features that you design. For open-source tools, they also <a href=""https://en.wikipedia.org/wiki/Linus%27s_law"" rel=""noreferrer"">benefit from many eyes</a>.</p>
<p>99% availability is really a lot of downtime. It's 00:14:24 per day - fourteen minutes and twenty-four seconds. Annually, it's over 3.5 days. Depending on your SLAs, planned downtime may or may not be considered against that 99% uptime. Regardless, that's a lot of downtime.</p>
<p>It would be probably be cheaper and less error prone to use a data storage solution's out-of-the-box sharding and replication functionality. Going with a managed service from a large provider that undergoes audits of their processes and capabilities, like Microsoft, Amazon, or Google, would increase the confidence that the desired uptime is being delivered.</p>
<p>Just as examples: A multi-availability zone deployment of AWS's RDS service for managed relational databases would get you to 99.5% availability. For files, AWS's S3 has a 99.9% monthly availability and grants 100% service credit if the availability is below 95% for a month. Going with services like these and allowing the developers to focus on your core competencies would probably pay for itself pretty quickly while exceeding your more strict availability requirements.</p>
"
436491,"<p>If you have requirements to keep data separate there are no common services. Common services are just great targets for exploit, they just increase risk and don't actually make thing easier for you. With a common service any compromised client now can potentially compromise all your clients (or you can accidentally expose one client to another's data). In addition to the security risk, any downtime on a common service affects all clients, so no you have more angry people calling or insane scheduling hurdles for planned maintenance. It's also likely that all clients will want different upgrade tempos, so it's possible you need to run multiple versions of common clients anyway because some are 2.0 and some are 1.0 and you now have tons of complexity.</p>
<p>If you have requirements to isolate something than everything that uses it needs to be isolated. It's better to think of however many microservices an app needs as a single unit in these cases. With cloud or virtual machines it's really not that different to deploy 15 services or 11 based on your examples. Cost shouldn't be dramatically different either as you should be able to downsize common services as they would get 1/3 of the traffic, and the expense of a potential breach of contract will absolutely exceed hosting costs.</p>
"
424900,"<p>We are working on a multi-tenant SaaS product that has a ledger of all tenant customer transactions, which we use to track invoice states on a line-item level. This particular application will result in a lot of upstream interactions with service providers which results in a lot of line-item debits and credits. The point being that a simple action such as generating an invoice and receiving payment, could quite easily lead to 50-100 database entries.</p>
<p>We use a normalized relational database as we a) need to do various different joins to display the data from different vantage points, and b) need to present the data at different levels of granularity.</p>
<p>For instance, the application allows different customers to execute purchases on different accounts. The system allows one to view a transaction history on an account level, or to view transactions on a customer level across accounts (these are just two examples, there are many more). Ito granularity, one might want to view all debits and credits for a single invoice, or one might want to view a debtors aging analysis, which will aggregate all records for a single tenant.</p>
<p>To ensure theses queries run in a timeous fashion, we've indexed the hell out of the database. This works reasonably well for small tenants where query times are in seconds, but the aggregate queries are into the minute mark for large customers. Ideally we would like all queries to run sub-second.</p>
<p>We've been toying with the idea of building indexed views, but given that this OLTP system is quite chatty, we are concerned that table locks (caused by the updates in referenced tables) will cause all tenants to suffer when changes are made (as naturally the indexed views would need to be recalculated).</p>
<p>Building a data warehouse, summary tables, or some sort of data cube is not ideal as it too would need to be updated in real-time since changes need to reflect immediately and we would no longer have a single source of truth. Furthermore, cost is a big concern too. Our cloud expenses are through the roof.</p>
<p>At this stage, we are thinking of building a hybrid data access layer which will cache certain views in Redis and persist the data to the source database. The views will then be updated after the TTL expires or if any records the view touches are updated. Any thoughts on this approach?</p>
"
423151,"<p>No, you should NOT expose the database publicly. This is very difficult to do safely, and is difficult to integrate directly with a HTTP-based API.</p>
<p>So what you're doing – having a web app backend that sits between the user and the database – is the typical architecture. This can deliver perfectly acceptable performance, in particular if the backend server can scale horizontally. But how performant is this will be more of a function of the networking hardware and network architecture than of the software architecture. And in most cases, the user's internet connection is the limiting factor, not your server.</p>
<p>What you can do is consider whether a SQL database has to store all of this data. If you have columns that contain large blobs (multiple MB) then you might want to consider storing the data externally. Blob storage / object storage such as S3-like storage could be more appropriate. Since every object has an URL, the user can fetch it directly without having to go through your backend. Access controls can be implemented with signed URLs that are only valid for a finite time. Your backend would then get a list of object IDs from the SQL DB, and generate signed URLs for the user.</p>
<p>Note that cloud egress charges can make such a design much more expensive than doing extensive processing and filtering within your backend. Sometimes the user will truly need access to the full data, but often they just need a summary or report of the data.</p>
<p>In this scenario it seems that you merely have a lot of data, not large blobs (average row size only about 30kB). So moving parts of the DB to external storage is likely not feasible, though it could be possible for the reference data set.</p>
"
420466,"<p>I very much agree with Todd with the advice:</p>
<blockquote>
<p>I strongly recommend that you don't overengineer your architecture.
Less is more.
YAGNI: Start simple and tunable, then update your architecture as
needed.</p>
</blockquote>
<p><strong>Specific Answers</strong></p>
<p>To answer your specific questions but reordered slightly:</p>
<blockquote>
<p>Missing posts in a user app like this in case of failure is a big no-no, so I wonder what is your view on this, and what is the best resolution of the problem.</p>
</blockquote>
<p>First, we need to be clear about the problem. Implicit in your question is the assumption that write throughput is a big problem to solve. Buffering will happen in Redis when the write-throughput has exceeded the single master write-path of a MongoDB cluster. You are then considering what might happen if Redis crashes. This is all good thinking. Yet is optimising write-throughput the major architectural challenge in your service?</p>
<p>You describe your service as news aggregation. We might expect that reads might dominate. Writes are be many orders of magnitude smaller for news. If it is indeed news you are publishing you can consider buffering writes at the client under high write (e.g., browser local storage). If you <a href=""https://medium.com/helpshift-engineering/load-shedding-in-web-services-9fa8cfa1ffe4"" rel=""nofollow noreferrer"">load-shed writes</a> you can add simple logic at the client can try again using an exponential back-off give your backend time to catch up (else heal from an outage). A tiny write message queue at every client is easy to implement. You can expect it to be reliable and easy to test. Only if your service is not accepting writes, and the user deletes their client app (else clears browser local storage), will they lose work.</p>
<p>If your service is a bit more like an Uber or Lyft ride-sharing service then write throughput will be critical. With ride-sharing there are a huge number of jobs published constantly and searched for in a geospatial manner. Write throughput and graceful outages are critical to those services. In which case you can research how those companies solve the write-path problem.</p>
<p>Below I will propose two solutions that I would investigate. One that optimises for write throughput and another that might be more suitable for a low budget &quot;news aggregation&quot; service that can scale up later.</p>
<blockquote>
<p>Does a high-availability cluster with write-behind cache solve all of my problems?</p>
</blockquote>
<p>Operational complexity and recovering from failures is likely to give you the biggest real-world business problems. Studies of big systems indicate that it is &quot;<a href=""https://blog.acolyer.org/2017/06/15/gray-failure-the-achilles-heel-of-cloud-scale-systems/"" rel=""nofollow noreferrer"">grey failures</a>&quot; are the most tricky sort of problems that lead to extended outages and data loss. The moment you integrate two products on your write path you can expect complex interactions under failures and load. In theory, yes, a highly-available cluster with a write-behind solves all your problems. In practice, I would hesitate to go with a HA Redis Cluster in front of a MongoDB Cluster.</p>
<blockquote>
<p>Does high-availability write to disk (Redis), guaranteeing that no data will be lost?</p>
</blockquote>
<p>Well, Redis is an excellent product. In the past, it has had some <a href=""https://aphyr.com/posts/283-jepsen-redis"" rel=""nofollow noreferrer"">hiccups</a> with data loss. I have used Redis myself as cache in a few architectures. Would I use Redis as a silver bullet to optimise the write-throughput a geospatial based web-scale system? No.</p>
<p>IMHO opensource products grow up to fulfil the core needs of their community. They then bolt on additional features that serve as wide a set of needs as possible. People then get into trouble when they push those additional features to the extreme. You then encounter bugs that haven’t been found and fixed by the wider community as you are the outlier. Redis excels at being a low-latency cache optimising the latency of the read path. Using it to optimise the write-path of a very high write throughput geospatial system would be something I would be very nervous about in practice. I think Redis is great and I will continue to deploy it to optimise the read-path. Yet I would go with a specialist solution to optimise the write-path.</p>
<p>If write-throughput is the major challenge than I would look at Apache Kafka or an alternative. It has an architecture that is very different from traditional massage brokers so that it can scale to &quot;IoT&quot; levels of writes. You can then have consumers that update your main data store and possibly a separate geospatial index. If a consumer was buggy/lossy you can easily replay the data stream to “fix-up” your geospatial index service or main store after you have fixed the bug in your consumers. I would then have a single product on the durable write path. If the secondary writes to the main database or index service fail the ability to easily replay a stream of events will be invaluable. Engineering to make it easy to recover from the unexpected is money better spent than trying to eliminate the unexpected. A large number of real-world outages are caused by human error so designing for failure recovery is critical.</p>
<blockquote>
<p>Even with write-behind cache, is it the safest course of action to add a message queue (eg RabbitMq) or is this generally not necessary even under high load?</p>
</blockquote>
<p>Well, RabbitMQ is an excellent product. I have used it myself. If you can buffer the writes at the client under high-load, and do load shedding in-front of RabbitMQ, then it might be a great fit.  It would be low-latency and allow you to update the main database and possibly a separate geospatial index. Using a message queue to decouple and independently scale microservices is a great strategy. Yet it will add operational complexity at go-live when it might not be needed until the service starts to take-off. So I would be tempted to start without it and then use it as a way to break apart a simple initial architecture into a more complex architecture at a later date.</p>
<p><strong>My Two Suggested Approaches</strong></p>
<p>(1) For a &quot;mega-scale&quot; Geospatial System with both high writes and high read:</p>
<p>If write-throughput is a big challenge I would evaluate both Apache Kafka and Apache Pulsar as the initial durable write-path. Those are very highly scalable messaging engines. Kafka is often used as the transport in asynchronous microservices architectures so we might expect it to have satisfactory latencies at high throughput. I would recommend having an edge &quot;news publish&quot; microservice that exposes RSocket over WebSockets to the clients. Clients would push the new news message over WebSockets as a request-response interaction. The RSocket server would simply apply security (validate the user), validate the payload, write it into Kafka/Pulsar, and respond. I would add logic at the client to hold the message in local storage and periodically retry if it got timeouts or errors. I would exponentially back-off in the retry logic to allow the service to recover.</p>
<p>A news aggregation service will need scalable reads. Your proposal is to use a MongoDB cluster. This is because it does geospatial queries. That would work. If you were looking to scale reads to a much higher level you could consider using a more scalable main database such as ScyllaDB and deploy separate geospatial query service such as Elastic Search. The consumers that read new news from Kafka can first write to the main ScyllaDB database. They can then update a geospatial index in Elastic Search. Elastic Search is often used as a secondary index for both free text search and geospatial indexing. As you are a news aggregation service deploying a dedicated free text search index may also be useful.</p>
<p>(2) For a &quot;start-up budget&quot; system with initial low writes and modest reads:</p>
<p>While we all want to build a service that can scale to huge loads from launch often the reality is that the main threat to a business at launch is over-engineering. It makes business sense to initially focus all engineering effort on the user experience while using a simple and cheap backend. Facebook was a college toy at go-live. Amazon started off selling books from a single workstation computer that beeped whenever an order came in. Demand scales up over time and only <em>after</em> you have a great product deployed. If it is a great business idea and a compelling product then a few hiccups as you grow and scale the architecture will be fine.</p>
<p>PostgreSQL is a traditional relational database. It now does binary JSON storage where you can index into the JSON fields. It also supports geospatial indexes. It also supports free-text searching. It is very easy to run locally to develop against. Better yet major cloud providers support it with lots of automation, high-availability, automated backup/restore, point-in-time restores, and monitoring dashboards. <a href=""https://aws.amazon.com/rds/postgresql/"" rel=""nofollow noreferrer"">Amazon RDS lets you run PostgreSQL</a> and they can seriously scale it up for you with a few clicks. You can start very cheaply and then add HA easily later then scale up to bigger and bigger database servers as you grow. That gives you time to then fix the real performance bottlenecks rather than guessed problems.</p>
<p>You can start with an edge microservice that will load-shed rather than do too many writes into PostgreSQL. The client can buffer the write and try again later. Then start off with simple code that does all of the writes against PostgreSQL for the main document and geospatial indexing. Spend the engineering effort on the frontend and business features. Later you can put RabbitMQ between the edge microservice and PostgreSQL. Then you can either break up or swap out PostgreSQL.</p>
<p>At a later date, you might create a separate geospatial index in Elastic Search. Later yet you might choose to move actual documents out of Postgres and into ScyllaDB. Do we know which things we should do in which order today? No, we cannot. Instead plan to evolve the architecture. Maybe just splitting from one PostgreSQL server into three servers, one dedicated to each of geospatial indexing, binary json, and free text search might work? That sounds like a great intermediate step before swapping in Elastic Search or ScyllaDB. We don't know but we can be flexible.</p>
"
418151,"<p>I have a monolithic application which can be divided into multiple steps and different steps have variable scaling requirement, however the input of the next step is the output of the current step. Currently, we are not able to handle the no of requests to our server and we need to have more servers/load balancer/etc. We are also thinking to re-architect our application. If we create separate services (for those steps) and deploy them as containerized application using Docker &amp; Kubernetes on cloud and use some distributed message broker (Queue) to pass the results from previous step service to next step service, we would be implementing a sort of microservices architecture. The only concern which I feel is that if Service1 container instance and service2 container instances are in different servers/hosts, then there would be some network latency which would be multiplied by the no. of requests.</p>
<p>So based upon my understanding the microservices architecture is not a good candidate for pipeline kind of requirement, if we are looking for real time performance. It will be better to keep those step based services in the same server and may be control the amount of resources which can be used by those services i.e. allocating more resources to service which needs them more and then we can auto scale the whole server based upon load. We can have in-memory queues between those services. Do we have a software which can help in dynamically allocating more resources to a service if the no. of items in their queue is high?</p>
"
415998,"<p>So... The architecture has recently been put under the reign of a Reference Architect.
The Reference architect, I will refer him as RA, started to work and the results were immediately visible: we stopped calling microservices microservices, but blocks.</p>
<p>As we used to be a mixed Java/Linux and C#/.net shop things were working pretty well until the RA issued a decree that no more development shall be done on Java and Linux. We tried to explain that when using microservices (ouch... blocks) the implementation stack is nor very relevant and having multiple stacks gives us more opportunities as there is room for evolution and the costs of Linux stacks is generally lower in cloud the RA sent a cease and desist memo that urged us to go to his favourite vendor.</p>
<p>Apart from the obvious cost and quality related arguments that can be invoked what other could be used so that we can make a case for keeping our code. There is lot of legacy and porting it to a new platform would not bring any business value but a decrease in quality and huge delays.</p>
<p>The arguments used so far are</p>
<ul>
<li>TCO - increased costs due to licensing</li>
<li>ROI - when a break even will be available</li>
<li>talent availability</li>
<li>domain knowledge - this is already captured</li>
<li>deployment independence</li>
<li>testing effort</li>
<li>training effort</li>
</ul>
<p>What am I missing in order to make a compelling case against a single stack? In a microservice architecture shouldn't the architect be focused more on delivering value, having clear interfaces and mechanisms than on concrete implementations? Microservices could evolve independently and choose their own stack for functional and non-functional reasons, forcing everyone into conformity would lead to degeneration and brittleness. Of course that a single stack seems more economical and promises code reuse, but as far as I know the business domains are pretty different in the two worlds so anyways reuse will be hard.</p>
"
411767,"<p>As Jörg W Mittag mentioned there is the legal aspect of what you are talking about, and then the technical.  As long as the app embeds critical logic and database access inside of it, someone with enough patience can reverse engineer and do the bad things you are talking about.  There are different approaches you can take to protect your efforts:</p>
<ul>
<li>Use Digital Rights Management (DRM) to protect the app--still can be defeated but harder to do</li>
<li>Use a code obfuscator which has the ability to make it harder to reverse engineer the code</li>
<li>Encrypt the module that does the critical access (you have to decrypt it when you load it in memory)</li>
<li>Move all critical behavior to services hosted remotely (like in the cloud)</li>
</ul>
<p>None of these solutions are mutually exclusive, but the one that provides the best protection is to move your database access and critical business logic to a service oriented architecture (i.e. web services you control).  That way it is never part of your app to begin with, and then none of the code you are worried about is even available for someone to reverse engineer.</p>
<p>It also means you are free to change how that information is stored and managed without having to release a new version of the app.  Of course, you'll have to provide the appropriate protection to make sure that a user can only see or interact with their own data, but now you don't have to worry about the app being hacked.</p>
<p>Many apps are built this way now.  The app communicates with servers via HTTP with JSON, YAML, Protobuf, BSon, or some other structured exchange format.  The app authenticates to get a session token that is good for a few minutes at a time, and that token is presented to your service so you don't have to worry about server side sessions.</p>
"
409372,"<p>Technically what you are suggesting is very feasible.  It's something that is quite common, in fact.  In the past, there were limited options on sizing a machine and therefore it was necessary to do this in order to use computing resources efficiently.  There are a lot of challenges, however, which largely explain how we got to where we are with virtual machines, containers, and serverless architectures.  AS others have already noted, trying to manage the capacity with regard to load will be complicated and you will end up needing to oversize your VM to meet peak loads.</p>

<p>Since you mention AWS, you should understand that a lot of the options available there are designed to help avoid theses challenges.  The design you mention fits very well into containers e.g. a Kubernetes pod.  There are many options to allow for scaling under heavy loads while avoiding paying for the extra capacity when you don't need it.  You should probably also look into lambdas which cost nothing when not actively in use.</p>

<p>So yes, you can do this but it's a pretty out-dated approach.  Given the options you have available to you, there doesn't seem to be a good reason to go this route.</p>
"
406350,"<p>I think it is important to understand what you get with microservices, why infrastructure is needed, and what the cost of microservices are.  That will help inform you if your approach is reasonable, particularly with the goal of unifying to completely different applications.</p>

<p>First, a summary of the trade-offs:</p>

<ul>
<li>You have the <em>opportunity</em> to <strong>minimize the blast radius</strong> if a service goes offline (i.e. impact to the user)</li>
<li>You have the <em>opportunity</em> to <strong>scale out</strong> dynamically based on load and demand</li>
<li>You have more moving parts so <strong>deployments are more complex</strong></li>
<li>You have the need to centralize how <strong>configuration</strong> works</li>
</ul>

<p>I want to emphasize the word ""opportunity"" because there are restrictions to make those opportunities a reality.  Namely that you don't use server side sessions at all.  If you have to maintain state, either do it in the HTML/JavaScript or do it in the database.  You have to simplify the ability to load balance all the instances of a service, and storing information in memory on the server side works against that.</p>

<p>That said, we start to see <em>why</em> discovery, configuration, and an API Gateway are necessary.  All of these can be managed in different ways.  The core cloud infrastructure pieces are needed precisely because of microservices.</p>

<ul>
<li><strong>Discovery:</strong> enables you to find all instances of a service by name.  This can be accomplished by using Eureka (Spring Cloud infrastructure), Kubernetes Services, or an App Mesh</li>
<li><strong>API Gateway:</strong> a reverse proxy that will map URLs to specific microservices on the backend--providing load balancing across the services.  This can be accomplished by Zuul or SC Gateway (Spring Cloud infrastructure), Kubernetes Ingress service, or an App Mesh</li>
<li><strong>Configuration:</strong> a means of managing configuration and pushing updates to the running services.  This can be accomplished by Spring Config Server (Spring Cloud infrastructure), Kubernetes ConfigMaps and Secrets, or an App Mesh.</li>
<li><strong>Central Log Management:</strong> is required to understand how your application is behaving in the wild.  Whether you use Elastic Stack, Splunk, Data Dog, or a cloud broker provided service, you will be better equipped to diagnose problems that span multiple microservices.</li>
</ul>

<p>Before you say ""I'll use App Mesh"" understand that app mesh is a suite of technologies that integrate with an orchestration layer (like Kubernetes) underneath of it.  You have to understand the concepts of the App Mesh and the underlying technology.  The learning curve very well may be something you don't have the time to take on right now.</p>

<p><strong>How To Migrate to Microservices</strong></p>

<p>The pragmatist in me likes the <a href=""https://dzone.com/articles/monolith-to-microservices-using-the-strangler-patt"" rel=""nofollow noreferrer"">""strangler pattern""</a>.  Essentially, you migrate out specific functionality into its own microservice, and adapt the existing application(s) to leverage that microservice instead of the old way it used to deal with things.  For example, separating out the authentication/authorization functionality into a unified service so that the user's token has all the attributes needed encoded inside of it (see JWT.io) you can minimize service-to-service communication to verify permissions.  That would provide you single-sign-on for your application suite.</p>

<p>But while you are in the design phase, also consider the build vs. buy question.  If you are hosting in the cloud, check your provider if they have services that do everything you need.  For example, single-sign-on is a common need and all the major providers have their version of SSO support.</p>

<p><strong>Key Takeaway</strong></p>

<p>Microservices are a powerful way to manage application deployments.  If you are smart about how you design them, you can minimize your cloud expenditures.  I.e. taking advantage of managed services hides the actual compute instances you are leveraging.  The cloud provider handles that so support the SLAs and minimize the cost to the provider, and you get a lower cost solution.  Having a dynamic deployment allows to you take advantage of spot instances for surge activity.</p>

<p>Putting both of your applications behind a common API Gateway does not magically give you single-sign-on.  Nor does it give you any of the real benefits of a microservices architecture.  However, <strong>it is a place to start</strong>.  You can expand on this initial investment over time.  Even the Amazon store front had to evolve from a monolithic application to microservices.  That took time, and it took time learning how the different architectures helped the application scale and minimize the cost of running it.  That is a journey you will have to take if this is the direction your company wants to go.</p>
"
405176,"<p>If you need high availability where one minute of downtime is not acceptable a single cloud provider is not enough. You need multiple providers to have high availability at that  level, even then it's still a matter of hoping any issues don't affect multiple providers at the same time. You also need internal processes and procedures in place that are far more challenging and demanding than choice of cloud providers. You also need to ensure anything supporting the highly available module is highly available itself in most cases.</p>

<p>When faced with the price tag for true high availability most organizations discover they really don't need high availability. Once a real cost benefit analysis is done, downtime tends to not look so bad. The less downtime acceptable the more your costs to insure that happens increases and that scale is exponential in nature. Accepting an hour of downtime a year only costs $D, but a few minutes of downtime a year is going to cost 10-20 times more, the price paid to prevent losses can quickly eclipse the actual losses from downtime.</p>

<p>To give you an idea of just how extreme avoiding one minute of downtime is, an SLA for 99.9% up-time still allows for a minute of downtime per day. For a 99.99% SLA a minute of downtime on a weekly basis is acceptable, and a 99.999% is five minutes on a yearly basis. It's very easy to have all sorts of SLAs with what look like impressive numbers, but a minute of downtime is an extremely short window. Everything has to be automated to maintain that level of up time, you need to detect and mitigate issues without human interaction. <a href=""https://cloud.google.com/appengine/sla"" rel=""nofollow noreferrer"">App Engine only offers a default SLA of 99.95%</a> which wouldn't meet your needs alone if one minute of downtime is an issue.</p>
"
403459,"<p>The <a href=""https://en.m.wikipedia.org/wiki/Law_of_the_instrument"" rel=""nofollow noreferrer"">law of the instrument</a> tells us that:</p>

<blockquote>
  <p>If you have a hammer in your hand, every problem starts to look like a
  nail.</p>
</blockquote>

<p>So before jumping too quickly to a solution look objectively at the needs and requirements. In particular security
needs, access control, and transactional consistency requirements. </p>

<p><strong>Blob in DB:</strong></p>

<ul>
<li><p>Pros:</p>

<ul>
<li>benefits from the access security of your database, as well as its backup strategy</li>
<li>confidentiality against unauthorized third party access (e.g. if you’re in governement or military affairs)</li>
<li>simplification of architecture due to homogeneous access</li>
<li>reduction in the external bandwidth if db is on premises and if you have lots of internal users. </li>
<li>blob is managed with transactional integrity, as the remaining data</li>
</ul></li>
<li><p>smaller text blobs could fit into text fields (eg up to 1GB in Postgresql), allowing to use content in queries.</p></li>
<li><p>Cons:</p>

<ul>
<li>technical limits of the db (eg 1GB on Postgresql, a little less than 2GB on SqlServer, 4GB on Oracle, support only via GridFS in MongoDb)</li>
<li>significantly increases size of the db </li>
<li>increase operational cost of the db, especially if storage goes significantly beyond initial assumptions.</li>
<li>waste of expensive infrastructure (<a href=""https://www.google.be/amp/s/searchstorage.techtarget.com/definition/Tier-1-storage%3Famp=1"" rel=""nofollow noreferrer"">tier 1 disk space</a> meant for heavily used db data but misused to store objects that are not frequently accessed and could go to tier 2 or 3). </li>
</ul></li>
</ul>

<p><strong>Blob in an object store</strong></p>

<ul>
<li><p>Pros:</p>

<ul>
<li>very effective and specialised solutions on the market</li>
<li>cost effective </li>
<li>easy to set up</li>
<li>resilient (depending on the offer), with backup included </li>
</ul></li>
<li><p>Cons:</p>

<ul>
<li>accessibility risks if mission critical application (e.g. in case of denial of service attack on the provider or on your internet access point)</li>
<li>confidentiality risk (but IMHO this only applies to highly sensitive and classified information in governmental and military affairs)</li>
<li>additional access security may be needed ( eg AWS identity management and acl management)</li>
<li>you need to take care of transactional consistency aspects (e.g. what to do if a blob update fails, regarding the related db changes)</li>
</ul></li>
</ul>

<p>Your current context may influence this evaluation. For example, if you already have some cloud services and identity management active, this will no longer be a cons. Or if your current db has a very small limit for blobs (eg Mongodb) many of the object store cons will not be relevant for you since you’ll have to care for this anyhow.</p>
"
403412,"<p>The big things to address are:</p>

<ul>
<li><strong>non-repudiation:</strong> the user should be unable to deny that the actions are theirs.  That means that the identity cannot be easily stolen by someone, and that the token provided can be validated that it is correct.</li>
<li><strong>auditable:</strong> you need to be able to determine if any users are behaving badly, and terminate access if so.</li>
<li><strong>controllable:</strong> that means that you can impose rate limits, reduce access privileges if necessary, etc.</li>
<li><strong>enforceable:</strong> or the authorization for a user to be able to perform an action is enforced.</li>
</ul>

<p>To that end, there are multiple layers, and each of those technologies have a portion of the whole security posture in place.  The good thing about OAuth is that it is standards based, making it easier for systems to build integrations.  However, you can provide all those guarantees without using it if you really don't want to.</p>

<p><strong>The API Key</strong></p>

<p>By itself, the API key can be easily sniffed out and stolen if you don't use <strong>encrypted</strong> communications.  That said, blindly using an API key also isn't the right solution.  It's easy, but if there is any layer in the communications which exposes the key, it's also easy to impersonate.</p>

<p>At the very least you want to make sure that your API key is <em>verifiable</em>.  OAuth does this by supplying the external system with a <strong>client id</strong>, and a <strong>secret key</strong>.  There are specific rules for providing the client id and secret key, including the rule to encrypt communications.</p>

<p>With OAuth, the keys provided by the external system are used to <strong>negotiate</strong> a <em>session token</em>.  The session token is then provided in the <code>Authentication</code> HTTP header as a bearer token.  Nine times out of ten, the session token is <em>JWT</em>.</p>

<p><strong>OAuth2</strong></p>

<p>OAuth provides a very robust foundation to verify the end user, and provide a limited use session token for all further communications.  If you are the provider generating the session token, then you can use JWT to embed the information your application needs to not only identify the user, but embed their roles and any other attributes you use to decide access.  I do recommend signing the token.</p>

<p><strong>DB for authentication access</strong></p>

<p>This is an implementation detail.  You can use a managed service like AWS Cognito, or Atlassian Crowd to handle all the nitty gritty details, and to keep up with 3rd party identity management solutions.  Cognito has the added benefit of having control over the JWT session token that is generated for your purposes.</p>

<p><strong>AWS API Gateway</strong></p>

<p>The biggest benefit that API Gateway gives you is the ability to rate-limit API calls.  If you are using AWS Cognito, that can associate your AIM roles with the user's token automatically, and that follows through the API Gateway.  The AIM roles can be used to control which buckets you have access to, etc.</p>

<p>However, I don't see the Gateway as an enforcement layer.  I see it more as a traffic control layer to help scale your application.</p>

<p><strong>I'm trying not to be too prescriptive</strong></p>

<p>As long as you have the traits discussed at the beginning of my answer, you will have done a good job securing your application.  There are good reasons why OAuth2 with JWT session tokens are a common solution.  They pretty much tick off the majority of your access control needs.</p>
"
403152,"<p>You should probably look over this: <a href=""https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html"" rel=""nofollow noreferrer"">AWS lambda best practices</a>.  The first bullet is one thing you should definitely consider: </p>

<blockquote>
  <p>Initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves execution time and cost.</p>
</blockquote>

<p>Initializing a DB connection is typically one of the most expensive parts of working with a DB.  The above is good advice but if you are running lots of concurrent instances, I would expect you need a separate connection for each one e.g. connections are typically not threadsafe in Java.  A thousand connections all working on the same tables could cause contention on the DB and actually result in a slower overall execution time i.e. what I call the 'Stooges' problem.  When all 3 Stooges all try to go through the door at the same time, the throughput is less than if they go through one at a time.</p>

<p>A middle path would be to pass in a list of ids up to a certain limit or split the ids across a fixed number of instances.  This allows for control over the amount of concurrency.</p>
"
403071,"<p>The basic concepts are orthogonal, however, they are related.  One has to do with the availability of your application, and the other has to do with the correctness of your application.  Remember, there are differing levels of faults (also known as bugs).</p>

<p><strong>Fault Tolerance</strong></p>

<blockquote>
  <p>""Everything fails, all the time"" -- Werner Vogels, Amazon CTO</p>
</blockquote>

<p>When you design for fault tolerance, you are addressing issues such as:</p>

<ul>
<li>What happens if my service can't access required resources (database, storage, message queues, etc.)</li>
<li>What happens if I receive bad data?</li>
<li>How should I handle another service taking too long to respond?</li>
<li>What performance criteria do I have to meet?</li>
</ul>

<p>There are other related questions you have to ask yourself.  It's important to realize that there are a lot of modes where your code can fail, and the longer it is running the more likely it is to hit a soft failure scenario.</p>

<p>Not all failures cause your code to crash, but avoiding crashes is a big reason that fault tolerant systems are also more available.</p>

<p><strong>High Availability</strong></p>

<p>Ensuring your software is available for use means that users should not inconvenienced when other people are also using the system.  There is a direct relationship to the cost of running a system and its availability.  Quite simply, it costs more to be generally available.</p>

<p>To be available means you need contingencies for when things outside of your code's control are causing problems:</p>

<ul>
<li>How do I scale my application?</li>
<li>How quickly can I scale the application?</li>
<li>What happens when my data center is not available?</li>
<li>What happens when the network gets saturated?</li>
<li>How should I respond if my data center is a victim of disaster (natural disaster and war can cause you to have to change data centers)</li>
<li>How much will this cost me to run?</li>
</ul>

<p>You can start to see why microservices and the cloud are key components of engineering for the internet.  The idea is that you can scale out (more instances) much more quickly and cheaply than scaling up (more CPU and RAM).  Additionally, you can scale back down just as easily when the spike in traffic is over to save you some money when traffic is slower.</p>

<p>Of course you need to design for this as well.  That may require working with data partitioning, multi-region deployments, multi-availability-zone redundancy, etc.  Of course, if this is running on the user's hardware (i.e. a desktop application), then you may need to swap modules in and out of memory depending on whether you are using them.</p>

<p><strong>End Game</strong></p>

<p>You need both of these disciplines to engineer something that can scale smoothly as demand for your application grows.  You have to start making your best guess at what the appropriate problems you are most likely to face now, and then monitor your application while it is running.  That will show you where your current design is having a hard time keeping up with demand, reliably.</p>

<p>I guarantee you that none of the big systems people expect to rely on started where they are now.  You can look at Twitter, Facebook, Airbnb, Netflix, and even this site and find that each of their architectures are different even though they share some commonalities.  That's because the different ways those systems are used required different customization.  What's more telling is the decision process of how they got there.  Most of us are not going to engineer something so large as those big names in the internet, but we can learn from the decisions they made and apply it at a smaller scale.</p>
"
402967,"<p>There's a couple key concepts in here that will limit your ability to scale:</p>

<ul>
<li>Monolithic design</li>
<li>Combinatoric set of artifacts per user</li>
<li>Serialized updates to database</li>
</ul>

<p>Of those, the first two are the most costly.  Tools like Sagemaker allow you to create models.  The most compute intensive part of the process is training those models with your data.  Once you've trained your model, you can deploy it and use that trained model for the live system.  Usually it uses far fewer resources to run a trained model.</p>

<p>There's a few things I recommend:</p>

<ul>
<li>Split the monolithic design if possible so you can create a processing pipeline</li>
<li>Use temporary storage like Redis to store intermediate results until you are ready to store finished products</li>
<li>Use a message queue like Rabbit or Kafka to push processing to the next stage in the pipeline (look at Apache NiFi or Spring Cloud Data Flow to model the pipeline itself)</li>
<li>Leverage GPUs for parallel processing if at all possible.  AWS has EC2 instances with GPU available and some machine learning algorithms work much faster in GPUs</li>
<li>Attempt to batch save your results to your permanent data store (i.e. Mongo in this case)</li>
</ul>

<p>Another thing to consider is using a managed instance of MondoDB.  The Mongo team were really advertising that service at AWS re:Invent 2019.  It allows them to worry about scaling and backing up the Mongo cluster and you to worry about how you use it.</p>

<p>I mentioned Apache NiFi and Spring Cloud Data Flow above.  What they allow you to do is specify your workflow, and they will manage moving messages from one step to the next.  They can also manage your container instances so that only the processing steps that need to be scaled will be.  And when the demand is over, the extra containers will be reclaimed.</p>

<p>I believe AWS has some support for pipelines as well, using Lambda functions to route work.</p>

<p>Last concern here is the terms of service you are trying to support.  If the models you are building are expected to take a long time, you can serialize the processing somewhat.  If processing one user at a time is within acceptable limits, then attempt to interleave requests so that by the end the processing stays within acceptable limits but you get your users satisfied as soon as you can.  You'll still probably need some more capacity, but you might be able to service more users on the same amount of hardware.</p>

<p>As far as automatic scaling, you are looking at a couple technologies:</p>

<ul>
<li>If you are containerized, Kubernetes lets you set policies to automatically scale pods (containers) up and down depending on demand.  There are several advanced features to target certain containers for specific processing nodes</li>
<li>AWS Auto Scaling will perform a similar function at the EC2 instance level.  Combine it with Cloud Formation (infrastructure as code) and you have a very robust autoscaling system in place without containers.</li>
</ul>

<p>What's not clear here is if you are generating learning models for your customers, or running models you already have against customer data.  There are significant details that change in your architecture depending on the answer to that.</p>
"
402623,"<p>A structured approach to improving performance often involves</p>

<ol>
<li>Profiling hot spots, for example high CPU usage.</li>
<li>Profiling wait times. You could include a correlation ID to send through the various systems to keep track on what gets processed when and where.</li>
</ol>

<p>It may be hard to get the full picture running this on the cloud.</p>

<p>Personally, I would say an async event driven model is a mismatch when the user is waiting for a response. Google has an <a href=""https://developers.google.com/web/fundamentals/performance/rail"" rel=""nofollow noreferrer"">excellent article</a> on what the user's expectations potentially may be. The gist of it is, up to 100ms is an immediate response, up to 1000ms is still good when the user perceives there is a task being executed. I can't see from your model what kinds of operations you are performing (and the boundaries may be somewhat fluid, some users understand searching all your e-mail is work and some won't)</p>

<p>Azure Event Hubs is a big data pipeline - likely a microbatching architecture. Microbatching tends to have higher latencies.</p>

<p>Thinking about how you scale up on an event stream - Microsoft is not super specific on how the <a href=""https://docs.microsoft.com/de-de/azure/azure-functions/functions-scale"" rel=""nofollow noreferrer"">auto scale</a> works... and the data will be opaque to you the operator. On the other hand, if you run a request/response pattern fronted by an API gateway, the API gateway's precise function can be to monitor service levels on the various calls.</p>
"
392697,"<p><strong>(Important Preliminary Consideration)</strong> Before attempting to answer the questions you asked, I would highly encourage you to consider renting a high-memory machine for the ML modeling tasks if you're already in the cloud and talking about batch jobs. A 500GB RAM box can be had for $5/hr these days, and it'll be worth every penny (if you have little enough data that such a machine suffices -- anything after 1-6TB RAM is probably pushing the limits of cost-effectiveness, and you'd probably be better off with a distributed system).</p>

<p><strong>(The questions you asked)</strong> I'll focus on your last three bullet points here:</p>

<ol>
<li>It depends. If querying the production DB direction makes sense in your business and if the ML pipeline is closer to a streaming model so that each piece of data is touched few times then you might not need/want any layers between the DB and the ML batch processors. That said, lakes/warehouses/replicas and whatnot are common to isolate customers from the potential performance degradation from analytics. As to the performance question though, to a reasonable order of approximation the difference between ""close postgres"" and ""far postgres"" doesn't really matter unless you have synchronous round trips (and if you do...don't do that). There's a much bigger jump from ""close postgres"" to ""local memory"", and trying to frame the problem in a way so that you can load a blob of data into each ML machine, operate on the blob, and then aggregate the results with little to no back-and-forth will <em>probably</em> perform better than a solution involving a lot of shared state in a nearby database. I wouldn't be inclined to spin such a thing up for performance reasons (though exceptions obviously exist).</li>
<li>It depends. The <em>inferential</em> pipeline absolutely needs to live in production. The <em>model building</em> pipeline can live anywhere. If your model auto-updates in response to new data I would propose it's easiest for the <em>model building</em> pipeline to also live in production. Otherwise it depends on other factors like how often you change models, how good your tests and metrics are, how critical time-sensitivity is, how static the data is, how critical accuracy is, the compute resources needed to train a model, your model version control process, and so on. If your model doesn't auto-update then where you train it doesn't matter all that much in most scenarios.</li>
<li>Yes. People automate re-training all the time. There are even algorithms designed to auto-update in response to new data. Good performance metrics and monitoring are mostly non-negotiable in an auto-updating world.</li>
</ol>

<p><strong>(Desired Architectural Qualities)</strong> Unfortunately there isn't one correct answer to your question. The unique characteristics of the production system, the data you're working on, the ML models you're considering, and the dynamics of the underlying business will push you toward different solutions. Here are a few questions that might help point you to a good solution for your use case. Getting a good handle on requirements will allow you to take an informed and principled approach to choosing between different solutions:</p>

<ol>
<li>Will the production database suffice for your ML pipeline?

<ol>
<li>Is there a high availability assumption for the DB's other clients?</li>
<li>Can the machine handle the additional analytics load?</li>
<li>Does the data already easily map via SQL to a form the ML pipeline can handle?</li>
<li>Is the data especially time-sensitive?</li>
</ol></li>
<li>Do the ML pipelines need to be versioned and stored?

<ol>
<li>Do you need to be able to roll back pipeline deployments?</li>
<li>Do you want to be able to analyze the pipeline's progress over time?</li>
</ol></li>
<li>Should the ML pipeline be pre-trained or automatically trained?

<ol>
<li>Is the data naturally incremental and read-only (e.g. most time-series data), or does the underlying model have a lot of updates and deletes?</li>
<li>How problematic is it if the ML pipeline performs poorly?

<ol>
<li>Consider various facets question of this including false positives, false negatives, implicit bias, worst-case scenarios, median-case scenarios, and so on.</li>
</ol></li>
<li>Can you detect and respond to a poorly performing model?</li>
<li>Is the data sufficiently static that the model can change slowly or never?</li>
<li>Do the questions being asked naturally map to a model designed to adapt to new data?</li>
<li>Is deployment easy enough in your shop that you can easily switch to/from automatic/manual training if one approach doesn't work well?</li>
</ol></li>
<li>If an additional data store is needed, where should it be located?

<ol>
<li>What are the data ingress/egress rates with your cloud provider?</li>
<li>Are there ingress/egress exclusions when you stay in the provider's network?</li>
<li>Does the provider offer everything you need to build the rest of the ML pipeline?</li>
<li>Do you have the means to locally (with a broad definition of ""local"") host any portion of the system?</li>
</ol></li>
<li>Should the ML pipeline even be hand-built in the first place?

<ol>
<li>Can the organization afford the ongoing hardware/software/data cost?</li>
<li>Could the organization afford options like Google AutoML?</li>
<li>Is the data especially secure or sensitive?</li>
</ol></li>
</ol>
"
391902,"<p>My users will be businesses with a small number of accounts each: e.g. Business #1 with 3 users, Business #2 with 5 users, etc.</p>

<p>I am trying to determine the best way to organise the relation (on Postgresql, hosted on AWS) for each business client: for e.g. each business will have their own sets of <code>customers</code>, <code>logs</code>, <code>products</code>, <code>config</code> relations.</p>

<p>I want to create a scaleable database across all client businesses.</p>

<ul>
<li>Do I create a <em>single</em> relation, e.g. of <code>customers</code>, for <em>all</em> my clients? E.g. each business client is given a unique ID, and during onboarding of the business user, the user's account is tied to the specific business UID. Examples in 2 images below.</li>
</ul>

<p><a href=""https://i.sstatic.net/OIPbL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OIPbL.png"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://i.sstatic.net/tzXSc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tzXSc.png"" alt=""enter image description here""></a></p>

<p>This seems like a huge security risk given that all business information is stored in a single table/DB.</p>

<ul>
<li><strong>OR</strong>, do I create individual databases hosted separately in AWS for each business? This seems safe, but also practically not scaleable and expensive!</li>
</ul>

<p>I'm pretty sure there's a middle path where all B2B apps tread which my inexperience simply makes me oblivious to. </p>

<p>I've read <a href=""https://softwareengineering.stackexchange.com/questions/260798/500-databases-or-1-database-with-500-tables-or-just-1-table-with-all-the-records?rq=1"">this</a> and <a href=""https://softwareengineering.stackexchange.com/questions/141261/multi-tenancy-single-database-vs-multiple-database"">this</a>. My question is more directed to the <em>implementation</em> of the table if I were to go with a shared DB and shared schema.</p>
"
388892,"<p>If we consider the cache to be <a href=""http://www.catb.org/~esr/writings/taoup/html/ch04s02.html#orthogonality"" rel=""nofollow noreferrer"">orthogonal</a> to the architecture (and it's), the first pic is ok. </p>

<p>For the same reason that we don't deploy one security service, one API gateway, one message broker or one service locator per POD we don't have to deploy one Cache (in replica-set) per POD.<sup>1,2</sup></p>

<h3>Be aware of premature optimizations</h3>

<p>Caches are meant to solve specific performance issues. Mainly those derived from costly <a href=""https://en.wikipedia.org/wiki/Inter-process_communication"" rel=""nofollow noreferrer"">IPCs</a> or heavy calculations. Without evidence of any of these issues, deploying caches (in replica-set) per POD for the sake of the ""MS' God"", is premature optimization (among other things).</p>

<h3>The <em>Cloud</em> can kill you.</h3>

<p>If our goal is to deploy the architecture in the cloud, the smart move would be to start small. Be conservative. Scaling up|out as the needs come because the contrary is oversizing the architecture. Oversized architectures are potentially dangerous in the cloud because they can, literally, kill the project devouring the ROI in no time. <sup>3</sup></p>

<h3>Size the solution according to the problem</h3>

<p>Perform load tests first, get metrics and shreds of evidence of performance issues, find out whether caches are the solution to these issues. Then, size the solution according to the problem. If and only if services are proven to need a dedicated cache, deploy them. By the time you do it, you do it uppon objective metrics and keeping the bills under control.</p>

<p>I was told once</p>

<blockquote>
  <p>If the solution is more complex or more expensive than the problem it
  solves, then it's not a profitable solution. <strong>It's not even a solution!</strong></p>
</blockquote>

<p><sup>Emphasis mine</sup></p>

<h3>Keep complexity at bay</h3>

<p>MS architectures are complex per se, they don't need us adding more complexity for the sake of dogmas or beliefs we barely understand. Keep the overall complexity of the system as lower as possible but not lower. In other words, keep it simple but not at the cost of defeating the purpose of the architecture (total freedom of deployment and SDLC).</p>

<hr>

<p><sup>
1: I'm assuming that every POD is a replica of the same Service, not different services.
</sup></p>

<p><sup>
2: MS architectures are not about many but small systems, it's about a single system composed by business ""capabilities"", working all together for a greater good.
</sup></p>

<p><sup>
3: Cloud is anything but cheap. Particularly, when it comes to buy RAM
</sup></p>
"
386307,"<p>I like your proposed solution, but I would add that maybe a serverless architecture would suit you well. Did you take a look at Azure Functions? They can be triggered by some events like the upload of file to Blob Storage. <a href=""https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function"" rel=""nofollow noreferrer"">See here</a>.</p>

<p>The thing is that with a serverless architecture for your service, you will have a auto scalable solution, as well as less operation tasks (managing servers).</p>
"
385346,"<p>I currently build applications that are fairly monolithic. I have a one or many code bases that compile into one single binary/package and deployed on a cluster of docker containers. All of the data is stored on a single MySQL database, a redis cluster and possibly a NoSQL database for some of the data.</p>

<p>In this case, the bulk of my data is stored in an MariaDB RDS instance on Amazon Web Services. This works fairly well because RDS handles automated backups, and other benefits.</p>

<p>However, let's say that I want to split a service into its own ""microservice"". Where would it store its data? If I have 5 microservices, spinning up 5 RDS instances, 5 redis clusters doesn't seem to be the most cost effective and seems to be a lot of management overhead.</p>

<p>It seems to me that a cluster of docker containers would be more manageable for a single microservice. For example using something like docker-compose, you can spin up several docker images into as a single unit very easily. AWS has a similar concept called ""Task Definitions"" (to my knowledge) which you can launch on AWS Fargate or ECS. However, Fargate does not allow persistent storage, so you are basically forced to launch your database in something like RDS.</p>

<p>I suppose this is a fairly open ended question, and might pertain to DevOps more than actual Software engineering. How would someone design a microservice to be easily deployed on the cloud whilst being easily maintained as a separate but packaged unit of sub-services (app server, databases, cache, etc)? Using docker compose and amazon Task definitions seems to be the best way to keep consistency between development/staging/production environments, however it does have some limitations such as not having persistent storage on Fargate for example.</p>

<p>Just looking for examples on how someone might achieve this to help my understanding.</p>
"
384682,"<p>According to the <a href=""https://aws.amazon.com/sqs/features/"" rel=""nofollow noreferrer"">AWS feature page for SQS</a>:</p>
<blockquote>
<p>FIFO queues support up to 300 messages per second</p>
<p>Standard queues support a nearly unlimited number of transactions per second (TPS) per API action.</p>
</blockquote>
<p>I'm trying to build a system that will add notifications to a queue that will then be sent to a customers device using push notifications (SMS, APN, webhooks, email, etc).</p>
<p>There will be a Lambda function that will read items off this queue and actually handle sending the message to the user.</p>
<p>Problem is, I'd like this system to be able to scale as efficiently as possible. Being constrained to 300 notifications per second might cause problems in the future. So I want to design this in a way that is much more scalable than that.</p>
<p>I have thought about building some type of system that will use a standard queue then check to see if that notification has already been sent by having a database that stores the ID of notifications that have been sent. Which might work. But at that point I think I'd be opening the door for race conditions. What happens if for the same notification two Lambda functions got triggered at the exact same time? Neither of them have been sent yet. And the user will send up with 2 notifications instead of one.</p>
<hr />
<p>How can I design a system that has the best of both worlds? <code>nearly unlimited number of transactions per second</code> while ensuring that no duplicate notifications are sent.</p>
<p>I don't think I mind quite as much if a Lambda function gets triggered twice for 1 notification, so long as it doesn't get sent multiple times to the user. Of course if I can completely prevent this, that'd be awesome too, so that I can reduce cost.</p>
<hr />
<p>I'd also love to keep using AWS and the more serverless technologies of AWS if possible. I know there is software and ways I could provision EC2 or other types of instances for this. But that takes out the huge advantage of serverless, which is what I'm really aiming for.</p>
"
382785,"<p>My suggestion to you is to start from something very simple and evolve.
Try AWS Lambda + API Gateway for start.
Very simple start, auto scaling up to 1000 concurrent executions per region.
If you will need more concurrent executions you can think about multi region load balancing. Or try to enlarge quota by opening request.
The question is pricing. can be expensive. 
Consider caching results if same documents used.</p>

<p>More complex arch :</p>

<ol>
<li>Upload document to s3. (Only after upload finished call Lambda with doc's name)</li>
<li>Call Lambda to analyze the document in s3. (After, delete Doc, storing hash of the doc with result in cache)</li>
<li>Return result. </li>
</ol>

<p>This is more wise architecture from different perspectives.</p>

<ul>
<li>Scales very good for multiple clients for s3 upload.</li>
<li>Latency Lambda &lt;-> S3 much better</li>
<li>Simple to develop</li>
<li>pricing</li>
</ul>
"
382607,"<p>In the case of displaying and editing the blog entries, the concept of separating the responsibilities of edit and read make perfect sense:</p>

<ul>
<li>Separate microservices allow each to scale appropriately</li>
<li>The only thing that needs to be understood between them is how you are persisting the data</li>
</ul>

<p>From there you can decide what infrastructure pieces you need between them.  From a cost/performance standpoint, you won't get any better than your cloud blob storage.  Of course, blob storage doesn't help with searching and querying, but it serves up resources very fast and you don't have to do anything special to make it scale.</p>

<p>If you offload all the rendering to the client (i.e. a Single Page App), then you only need to pass data back and forth.  That leaves your search/query capability which might be better served by ElasticSearch or Apache Solr.  An asynchronous task can handle updating your search engine, further decoupling things.</p>

<p>That said, with building for internet scale, you want to minimize the points of contention and share nothing if at all possible.  If after all that you feel you still need the Redis cache, you will be better armed to understand where it needs to be included.</p>

<p>Similar Bottom line:</p>

<ul>
<li>Think about the architecture and how you can solve your problems there first.</li>
<li>Eliminate sharing if you can.</li>
<li>There are no simple answers.  I don't know enough about you are doing and why you've chosen the tech stack you have to be any more meaningful in my answer.</li>
</ul>

<hr>

<p>I think that CQRS and Redis are solving 2 different problems and are not necessarily mutually exclusive concepts.  The main issue that limits scalability is when you have to share things, so minimizing the contention around sharing fixes that.  I'm not going to tell you that CQRS is right or wrong for your application, only that you are comparing apples and trucks.  They are very different things.</p>

<p><a href=""https://martinfowler.com/bliki/CQRS.html"" rel=""nofollow noreferrer"">Command Query Responsibility Segregation (CQRS)</a> Has its uses, and works well in focused applications.  However, it's a <strong>design</strong> pattern for your code.</p>

<p>Your other proposal was an <strong>architectural</strong> decision.  While architecture has a big impact on what design patterns are available or relevant to use, the decision is orthogonal to design.</p>

<p><strong>Bottom Line</strong></p>

<ul>
<li>Know your design parameters (do you need to support internet scale?)</li>
<li>Understand your bottlenecks (what's preventing you from meeting your goals?)</li>
<li>Understand the cost of your decisions (how much does it cost to run, and how much does it cost to switch?)</li>
</ul>

<p>Your team needs to agree on the problems that need to be solved, and when you are considering alternatives, make the alternatives of equal type.  For example, should we use Redis, ElasticSearch, or just simple cloud blob storage?  Those are examples of equivalent architectural decisions, which are themselves not mutually exclusive.</p>
"
379926,"<blockquote>
  <p>Why do we even need it?</p>
</blockquote>

<p>The enormous benefit of microservices—and more largely, SOA—is the high level of abstraction of the internals—not only the implementation, but also the technologies being used. For instance, if a system is developed in a form of five microservices by five teams, one team can decide to move to a completely different technological stack (for instance from Microsoft stack to LAMP) without even asking other teams for their opinion.</p>

<p>Look at Amazon AWS or Twilio. Do you know if their services are implemented in Java or Ruby? Do they use Oracle or PostgreSQL or Cassandra or MongoDB? How many machines do they use? Do you even care about that; in other words, are those technological choices affecting the way you use those services?... And more importantly, if they move to a different database, would you have to change your client application accordingly?</p>

<p>Now, what happens if two services use the same database? Here are a tiny part of the issues which may arise:</p>

<ul>
<li><p>The team developing service 1 wants to move from SQL Server 2012 to SQL Server 2016. However, the team 2 relies on a deprecated feature which was removed in SQL Server 2016.</p></li>
<li><p>Service 1 is a huge success. Hosting the database on two machines (master and failover) is not an option any longer. But scaling the cluster to multiple machines requires strategies such as sharding. Meanwhile, team 2 is happy with the current scale, and sees no reason to move to anything else.</p></li>
<li><p>Service 1 should move to UTF-8 as its default encoding. Service 2, however, is happy using Code Page 1252 Windows Latin 1.</p></li>
<li><p>Service 1 decides to add a user with a specific name. However, this user already exists, created a few months ago by the second team.</p></li>
<li><p>Service 1 needs a lot of different features. Service 2 is a highly critical component and needs to keep database features at their minimum to reduce the risk of attacks.</p></li>
<li><p>Service 1 requires 15 TB of disk space; the speed is not important, so ordinary hard disks are perfectly fine. Service 2 requires 50 GB at most, but needs to access it as fast as possible, meaning the data should be stored on an SSD.</p></li>
<li><p>...</p></li>
</ul>

<p>Every little choice affects everyone. Every decision needs to be taken collaboratively, by people from every team. Compromises have to be made. Compare that to a complete freedom to do whatever you want in a context of SOA.</p>

<blockquote>
  <p>it's too [...] unmanageable.</p>
</blockquote>

<p>Then you're doing it wrong. I suppose you're deploying <em>manually</em>.</p>

<p>This is not how things should be done. You need to automate the deployment of virtual machines (or Docker containers) which run the database. Once you automated them, deploying two servers or twenty servers or two thousand servers is not very different.</p>

<p>The magic thing about isolated databases is that it's <em>extremely manageable</em>. Have you tried managing a huge database used by dozens of teams? It's a nightmare. Every team has specific requests, and as soon as you touch something, it affects someone. With a database paired with an app, the scope becomes very narrow, meaning that there are much less things to think about.</p>

<p>If a huge database <em>requires</em> specialized system administrators, databases which are used by only one team can essentially be managed by this team (DevOps is <em>also</em> about that), freeing system administrators' time.</p>

<blockquote>
  <p>it's too costly</p>
</blockquote>

<p>Define cost.</p>

<p>Licensing costs depend on the database. At the era of cloud computing, I'm pretty sure all major players redesigned their licensing to accommodate the context where instead of one huge database, there are lots of small ones. If not, you may consider moving to a different database. There are a lot of open source ones, by the way.</p>

<p>If you're talking about processing power, both virtual machines and containers are CPU-friendly, and I wouldn't be very affirmative that one huge database will consume less CPU than a lot of small ones doing the same job.</p>

<p>If your issue is the memory, then virtual machines are not a good choice for you. Containers are. You'll be able to span as many as you want, knowing that they won't consume more RAM than needed. While the total memory consumption will be higher for lots of small databases compared to a large single one, I suppose that the difference won't be too important. YMMV.</p>
"
376325,"<p>This is my first post on the Software Engineering Stack Exchange so let me know if something is wrong with it.</p>

<p>I'm looking into the serverless offerings of Amazon to try to figure out if that is the way to go for a few new projects I have in mind. I'm particularly interested in an event-sourced, CQRS model, as I find the purported advantages of such a model very attractive in this instance. But I'm having a little bit of trouble understanding all of the services Amazon have to offer, what their pros and cons are, and how it all fits together. I'll give some pretext first and state my questions afterwards.</p>

<p>I'll use an example application to illustrate what I'm after:</p>

<p>It's a simple (static) web application, hosted in S3 and served over cloudflare.</p>

<p>It has two actions: One command and one query (in CQRS terms).</p>

<p>The command posts an event onto the event stream to increment a counter.</p>

<p>The query gets the current state of the counter, i.e. how many times it has been incremented.</p>

<p>That's it, so how do I implement this using serverless AWS technology? Here's what I'm thinking so far:</p>

<p>To send the command to increment the counter, the web application sends AJAX requests to a lambda L1 (through an API gateway). This lambda L1 posts an event to the event stream.</p>

<p>Another lambda L2 listens to the event stream and stores a record of the event/command so that it can be replayed at a later date if need be.</p>

<p>Yet another lambda L3 listens to the event stream and executes the command. In other words, it fetches the current state of the counter, increments it and persists the new state atomically.</p>

<p>To send the query, the web application sends an AJAX request to lambda L4 (through an API gateway), which queries the state and returns the result.</p>

<p>This seems like it should be a fairly straight forward, minimal project. Here are my concerns so far:</p>

<p>First of all, what should my event stream look like? I have seen many suggestions floating around, each one more convoluted and contrived than the last. Various fanning out strategies, mixtures of SNS, SQS, Kinesis, DynamoDB streams, you name it... I fear I will end up with too many moving parts, a cost-ineffective system that's difficult to scale in the sense that the complexity makes it difficult to develop for.</p>

<p>Second, can I achieve atomicity? The event stream services I mentioned above typically have some sort of ""at-least-once delivery"" property, which needs to be handled by the consumer. One suggestion I have seen is to make every event idempotent, but that does not seem feasible in my example application. Two clients could increment the counter at the same time, and one of the increments could get ""lost"" because both of the commands would say ""the counter is now at 17 (for example)"". You could argue that this is correct behavior, both client saw the number as 16 and wanted to increment it to 17, but let's say in this situation we would like both increments to count toward the total. We want our command to represent only a delta between the two states. Is there any way to achieve this?</p>

<p>Third, lambdas L3 and L4 both need to be able to access some sort of persistence layer. Ideally I would like this to be a relational database (SQL) so that I can perform advanced queries on the current application state. It's not necessary for my incrementing counter example, but will be necessary for the projects I have in mind. I think this only leaves me with one option if I want to stay serverless: Serverless Aurora. That's fine by me, but it's my understanding that Aurora needs to run in a VPC, and that lambdas need to run in the same VPC to have access to Aurora. I'm very concerned about performance here, as L3 is the single congestion point in my example (everything else is append-only or read-only). My understanding is that VPCs incur a pretty hefty performance cost (throughput, number of connections, bandwidth), and that lambdas in VPCs can have cold starts of upwards of 10 seconds. How can I tackle these problems? Alarm bells are going off in my head, that this just introduces more problems than it solves. I would probably have to ping L4 continuously so that it never cold starts (10 second load time is unacceptable), and at that point, am I really going serverless? If this is a bad idea, are there any better alternatives? Do I have to persist state in DynamoDB as well, losing querying capabilities?</p>

<p>This post is already pretty lengthy so I'll leave it at these three concerns for now. Aside from answering my questions directly, if you could help me clear up any misunderstandings, offer alternative solutions, etc. I would be grateful!</p>
"
372833,"<p>It all comes down to the cost of fast memory.</p>

<p>When you consider how expensive is the RAM, it starts making sense to use it for a small amount of data you need right now, and keep everything else you don't need that much on a cheaper medium. Often, there are even more levels than that:</p>

<ul>
<li>RAM for a small subset of data you need to read as fast as possible,</li>
<li>Local SSD for a larger subset of data you would like to be able to read fast enough,</li>
<li>Local hard disks, directly attached storage or NAS for the data you may need to read in a reasonable time,</li>
<li>Tapes for data you are expected to read only in exceptional circumstances.</li>
</ul>

<p>This applies equally well to home PCs, data closets, even cloud storage. Compare the price of a AWS server with a hundred gigabytes of RAM with a hundred gigabytes of S3 storage and a hundred gigabytes of Glacier storage. If you want S3 reliability but at a speed of a local RAM, you should expect the price to be accordingly high.</p>

<p>The good thing is that there are specific patterns which are designed to work with this topology (i.e. small but hugely fast non-persistent data medium combined with increasingly large, persistent, slower mediums). Cache is one of them: instead of having to juggle with data, trying to guess which one should be put in RAM, you simply delegate this job to a caching solution, picking the right caching approach. And when it comes to caching approaches, you have <a href=""https://en.wikipedia.org/wiki/Cache_replacement_policies"" rel=""nofollow noreferrer"">a large choice</a>.</p>

<p>Aside the cases handled by proper caching, there are situations where you need high speed access to perform a task (an example which comes to mind is the restoration of a SVN repository from a backup, which is extremely dependent on the speed of storage medium). In most of those situations, persistent aspect is welcome, but not required: for instance, if the server restarts while I was recovering a SVN repository, I can always do the operation again (or do it in parallel on multiple servers if it's worth the money).</p>

<p>Finally, there are situations (scientific analysis, statistical data processing) where it would be great to have huge amounts of very fast data mediums. Persistent aspect is usually irrelevant in those cases (with map reduce, you just redo the job of a machine which terminated unexpectedly), and huge costs of any fast medium usually force to fallback to ordinary SAN solutions.</p>
"
372002,"<p>I think the pattern you outline is the common one. (In that you program your own 'routing worker') But you could condense it down, moving the routing logic (red) into the worker.</p>

<p>For example, say instead of a worker listening to a single queue, I add code that is aware of the users. </p>

<p>I can then fire up a thread per user queue in the same worker service and let the cpu spread its time over each thread.</p>

<p>It might be slightly sub optimal for large numbers of users or CPU bound tasks, but it would simplify your overall solution.</p>

<p>Along a similar line, if you can pass on the costs, a good solution is to spin up a new worker on a new machine in the cloud as well as a queue per user. </p>
"
370490,"<p>I'm evaluating the migration of the following application's architecture: - Nginx + PHP + MySQL - Currently the infrastructure is scalable and redundant in the AWS cloud and It was designed to support one client.</p>

<p>Our next plan is to expand it to support multiple clients, these are my concerns: 1- The nature of the application is focused on collecting granular data by a set of companies with a task force of users based on a list of tasks per item in a given place. In summary: A user can potentially collect around 14.4k records per day (1 account x 2 campaigns x 12 actions x 12 tasks x 50 items) So a big client with 50 employees and 10 places to visit per day would record around 7.2M rows per day, anything can be measured ( a text, file as in a photo or video, etc) Our current way of storing the most granular values in a table is based on the EAV model. This allows the app to be very flexible, but as you can see generates a row vertically. making redundant much of the fk values and exponentially growing that table, giving us some issues when trying to run reports on this database, unless we offload the report somewhere else.</p>

<p>I was wondering if moving to: - Play + Mongodb And making the visit our main collection would reduce the redundant information. In the end a visit would be our most important document and would store 14.4k values, but everything would be contained inside. That means, in terms of documents, I would generate with 50 employes x 10 visit only 500 documents per day, which seems much more mangeable from the operations perspective.</p>

<p>However, the issue here is granular or cross-vertical reporting, like a report of your task force for the month. I guess this could be offloaded.</p>

<p>Would Play + Postgres be a better option and using the table with bjson data with slick?</p>
"
369596,"<p>A Docker image is identified by:</p>

<pre><code>[owner]/[name]:[tag]
</code></pre>

<p>When you get an official image from Docker Hub, the owner is not required. </p>

<p>Example:</p>

<pre><code>alpine:latest
</code></pre>

<p>So, </p>

<h3>I can pull safely the alpine image and indeed get the official image?</h3>

<p>When you pull from the Docker Hub (Official Repository) you will get the official Alpine image. </p>

<h3>My images are only based on the ""official"" images, I do not need a local registry?</h3>

<p>Images are just layers. If all your images are based on official public images you will not need use a repository (Docker Hub or on-premise). But, a good practice is create a <strong>base image</strong>. This image is created from a Official Image plus your enterprise customizations. </p>

<p>Example: </p>

<pre><code>mycompany/alpine-custom:latest
</code></pre>

<p>In this particular case, if you want to keep your image private and on-premise. You can use a internal registry. If you want to keep your image private, but on the cloud, Docker Hub private is also an option. </p>

<h3>What is the advantage of having a (local) registry with ready images in a deployment scenario over building the image on the host off the baseimage (e.g. alpine) and create a new one on the host with few additional git checkouts of code to deploy?</h3>

<p>When a application is ready for deployment, you create a package with all dependencies. You build the application before releasing. Is the same principle. It is better to have a static image prepared to production. It deploys faster and has less chance to download an incompatible dependency version or have a network failure.</p>

<h3>When does it make sense to pull full images from a (local) registry instead of building on the host? And then: why not using dockerhub private repositories for that?</h3>

<p>The chose between  cloud or on-premise repository evolves aspects like: security, performance, reliability, costs, network availability, etc. Each scenario has its own particularities. Usually use DockerHub private is OK for majority of scenarios. </p>

<p>For instance:</p>

<ul>
<li>You have to follow regulations that imposes to not use cloud platforms.</li>
<li>You use Docker for private processing and your network does not have access to the Internet. </li>
</ul>

<p>Is good to thing about the Container Workflow:</p>

<p>Layer one:</p>

<ul>
<li>Pull base</li>
<li>Build image-a</li>
<li>Push image-a</li>
</ul>

<p>Layer two:</p>

<ul>
<li>Pull image-a</li>
<li>Build image-b</li>
<li>Push image-b</li>
</ul>

<p>Deployment:</p>

<ul>
<li>Pull image-b</li>
<li>Run image-b</li>
</ul>
"
365330,"<p>Let me just start by quoting <a href=""https://aws.amazon.com/dynamodb/faqs/#general_anchor"" rel=""noreferrer"">Amazon's DynamoDB FAQ</a></p>

<blockquote>
  <p>Q: When should I use Amazon DynamoDB vs a relational database engine on Amazon RDS or Amazon EC2?</p>
  
  <p>Today’s web-based applications generate and consume massive amounts of
  data. For example, an online game might start out with only a few
  thousand users and a light database workload consisting of 10 writes
  per second and 50 reads per second. However, if the game becomes
  successful, it may rapidly grow to millions of users and generate tens
  (or even hundreds) of thousands of writes and reads per second. It may
  also create terabytes or more of data per day. Developing your
  applications against Amazon DynamoDB enables you to start small and
  simply dial-up your request capacity for a table as your requirements
  scale, without incurring downtime. You pay highly cost-efficient rates
  for the request capacity you provision, and let Amazon DynamoDB do the
  work over partitioning your data and traffic over sufficient server
  capacity to meet your needs. Amazon DynamoDB does the database
  management and administration, and you simply store and request your
  data. Automatic replication and failover provides built-in fault
  tolerance, high availability, and data durability. Amazon DynamoDB
  gives you the peace of mind that your database is fully managed and
  can grow with your application requirements.</p>
  
  <p>While Amazon DynamoDB tackles the core problems of database
  scalability, management, performance, and reliability, it does not
  have all the functionality of a relational database. It does not
  support complex relational queries (e.g. joins) or complex
  transactions. If your workload requires this functionality, or you are
  looking for compatibility with an existing relational engine, you may
  wish to run a relational engine on Amazon RDS or Amazon EC2. While
  relational database engines provide robust features and functionality,
  scaling a workload beyond a single relational database instance is
  highly complex and requires significant time and expertise. As such,
  if you anticipate scaling requirements for your new application and do
  not need relational features, Amazon DynamoDB may be the best choice
  for you.</p>
</blockquote>

<p>Are you even expecting a few thousand users? Do you have any concern with a sudden spike to millions of users? You've already stated that you need ""complex relational queries"" to use the terminology from the quote. Choosing a key-value/document store is not just saying ""I don't need those now"" but also ""and I will never need those"".</p>

<p>The quote also paints an overly rosy picture of NoSQL key-value/document stores. The weakened consistency guarantees lead to a significant amount of extra complexity in the application code to get correctness. My strong impression is that many developers using NoSQL key-value/document stores, just pretend that they have these consistency guarantees (or rather, don't realize that they don't) and write subtly broken code. (Cue the <a href=""http://hackingdistributed.com/2014/04/06/another-one-bites-the-dust-flexcoin/"" rel=""noreferrer"">multiple Bitcoin exchanges</a> that got ""hacked"" because they wrote code against NoSQL databases assuming they provided more consistency than they do.) There are some things that you just can't do with a NoSQL key-value/document store without basically manually reimplementing some of the trickiest parts of a relational database.</p>

<p>My general advice is that a relational database should be the default choice. Realistically, a system using a NoSQL data store will almost certainly have (or benefit from) a relational database as well, so the real question is, ""is there any reason to <em>also</em> have a NoSQL data store?"" The benefits of relational databases is that they are some of the most battle-tested software systems on the planet, are full-featured, and are very unlikely to leave you boxed in a corner where implementing certain functionality is just ""impossible"". Read-heavy loads are not problematic for relational databases, but even if they were the solution to that would be <em>caching</em>. You may even use a NoSQL key-value/document store for that cache! That would be a very good use of a NoSQL solution.</p>

<p>Relational database are likely to be completely adequate for many users needs performance-wise. They simplify and speed up development by presenting a much simpler consistency model and providing more features out-of-the-box. There's likely to be some data where consistency is important and latency isn't important; these are well-served by a relational database. There is also likely to be data where latency is more important and up-to-date consistency less so; caching handles this well and NoSQL solutions often shine here (assuming normal HTTP caching doesn't suffice). If your system does need to scale, you are most likely looking at a hybrid system, not a transition to a different data storage technology altogether.</p>

<p>(There are some application domains where it makes sense to design the system from the get-go for weak consistency to achieve low-latency, e.g. online, multiplayer, first-person shooter games. But this isn't most systems, or at the very least it's a choice between paying for a complicated low-latency design now or paying for it later when you have more information about the requirements and load [and likely more money and expertise].)</p>
"
365193,"<p>Application scalability is an enormous topic and not one that can really be addressed in a single post. However I'll have a crack at a basic explanation.</p>

<p>As you've implied most complex websites are scripts or programs which are executed on a server somewhere and HTML/Javascript (and others) are sent to a browser to interpret and render. Websites like amazon and facebook have millions of unique visitors each day.</p>

<p>To start off with yes, google's servers are no doubt considerably more powerful than your PC at home. However even they are not powerful enough to serve up so many http responses at once.</p>

<p>That's where server farms come in. If ten thousand users want to load the same website then a load balancer splits the traffic onto a number of different servers each of which can run the website.</p>

<p>To make things even more interesting these server farms do not have to all be located in one place, many huge organisations have their physical servers located around the globe allowing them to store and serve content from the one physically closest to the consuming computer (thanks for the suggestion amon).</p>

<p>There are a couple of challenges to overcome, if a user accesses server 462 on their first visit they may not necessarily hit the same server on their next request. Applications can either be designed to accommodate this (by avoiding things like session variables) or the load balancer can be configured to send returning users to the same server for the next page.</p>

<p>If all these servers access the same database under the covers then all the server farm may be doing is shifting the performance bottleneck from the web servers to the database. This is where microservice architecture (building a ""website"" from many tiny websites) can help as can distributed databases like MongoDB.</p>

<p>Many hosting providers nowadays (azure/AWS for example) prefer you deploy web applications to them rather than creating a website on a VM like you would if your company has it's own hosting. This is to allow them to scale up their infrastructure in demand to requirements - however these scale ups often come with cost implications.</p>

<p><strong>TLDR</strong></p>

<p>Many techniques are employed - load balancing, software architecture, and hosting considerations all play a part in building a robust system.</p>
"
363972,"<p>OK, so answering your question literally, You don't need to make any changes.</p>

<p>The volume of data on the database isn't usually a scaling issue for webpages. Any given webpage will only be looking at a small part of the data and databases are designed to retrieve subsets of data from large sets very quickly.</p>

<p>What you need to take account of is how quickly you can generate a page. This is usually limited the number of requests per second for webpages that your server is receiving. Each one takes up a chunk of CPU time and your server has a limited amount. Once you hit 100% every page request will get slower and slower.</p>

<p>So its not the million rows you have to worry about, its the million users.</p>

<p>Website servers are cheap to scale, you and fire up webserver2, copy your website to it and double your capacity with no major technical hurdles.</p>

<p>However, both websites will now be using the same database server. and once that hits capacity you do have some significant techincal problems to solve.</p>

<p>The underlying problem is that you want all the database requests to be looking at the same data. If you simply copy it across to a second db the databases will quickly go out of sync.</p>

<p>If you continually update each with data from the other, the each server is doing twice as much work and you haven't solved your scalablity problem.</p>

<p>MongoDB and other nosql databases are designed to combat this by ignoring it. instead of having a number of tables which must all be consistent, you put all your data into one blob. so its always consistent with itself.</p>

<p>This allows you to spin up multiple instances and just copy the data without too much of an issue. It then solves the problem of the blobs being updated with some clever tricks you don't have to worry about too much.</p>

<p>So in summary: You current php + mongodb website, assuming you haven't made any 'gothcha' errors should scale up to any number of users by just throwing more servers at the problem. If you are cloud hosting that just means clicking a button or two and putting your credit card in.</p>
"
363959,"<p>I'll give a bit of rationale for JacquesB's comment with which I completely agree.</p>

<p>First, for most domains RDBMSs can very likely scale to the load, and by the time this stops being true (if it ever does), you'll have a lot more understanding of your data and the queries you need to support (and presumably a lot more money). In other words, an RDBMS suffices for most use-cases for, at least, quite a while.</p>

<p>However, the real reason to use RDBMSs is flexibility. Using a typical key-value or document store means encoding a preferred way of accessing the data. For your example, this might mean that replies are child elements in a ""post"" document. This is great when you want data in just that format. It becomes far less great when you want, say, to get all the replies by a specific user. Now you have to walk over <em>every</em> post in the system and check <em>all</em> replies. In a relational database, this query would be trivial and, perhaps with the addition of a few indexes, would likely perform reasonably well. To get reasonable performance from the document store would require rearchitecting the data model, or, more realistically, storing a copy of the data organized by user. The latter is essentially manually creating an index. <a href=""http://www.sarahmei.com/blog/2013/11/11/why-you-should-never-use-mongodb/"" rel=""nofollow noreferrer"">Sarah Mei's article</a> outlines exactly how this plays out in detail.</p>

<p>Adding to the flexibility of RDBMSs, most are very featureful. Want to store JSON? Fine. XML? No problem. Need some full-text searches? Well, you should probably use a dedicated solution, but you can at least start meeting the need without changing technologies. Need integration with message queues, ORMs, Excel? No problem. Need more consistency? Use SERIALIZABLE transactions. Don't need that much consistency? Drop to READ COMMITTED, which is often the default! This monolithic, kitchen sink approach is one of the downsides of RDBMSs, but it definitely improves agility when you can use an out-of-the-box, integrated solution to, at least, prototype rather than needing to evaluate and integrate multiple 3rd-party solutions.</p>

<p>One of the things traditional RDBMSs do poorly that NoSQL solution usually do well is distribute. Virtually all of them <em>can</em> be used in a distributed way, but they are <em>much</em> more complicated to set up and maintain than most NoSQL solutions. So-called ""NewSQL"" systems, such as VoltDB, do give an experience much more like NoSQL solutions with regards to setting up distributed clusters. Of course, if you are using a database-as-a-service from a cloud provider, then you don't have to worry about this.</p>

<p>The place where I think most key-value/document stores fit is as a <em>caching layer</em>/secondary index. So, I actually <em>do</em> think you should ""manually create indexes"" using a NoSQL data store. I just think they should be ""indexes"" of data stored in a different system, typically an RDBMS. Another, closely related, way to think about this is the NoSQL data store is a <em>materialized view</em> of data stored elsewhere. This suggests a bit more generality: the ""materialized view"" may be sourced from multiple primary sources each the source of truth for their respective data, and they don't need to be RDBMSs, e.g. some might come from an LDAP server. RDBMSs do, generally, do very well as systems of record. Most NoSQL solutions usually should not be used as systems of record. This splitting of the work alleviates pressure on the system of record, while not asking NoSQL solutions to do things they aren't designed to do. Most of the problems with NoSQL solutions (weak consistency, redundancy, preferred access paths, limited or no integrity checking) don't matter when they are used this way.</p>

<p>Revisiting the earlier scenario: You're a year in to the project and you have 100,000 users. Your sole RDBMS database combined with standard web page caching is performing adequately until a viral article leads to a huge spike in traffic. Your RDBMS struggles to meet the demand. To avoid this in the future, you consider using a NoSQL solution to store AJAX responses in a ready-to-go manner for posts and replies. You realize this may mean that users see different looking reply threads at different times, but you accept this. By using sticky sessions, you can at least avoid the situation that a user fails to see their own reply after posting it. At this time you get the requirement to add a ""Replies by User"" feature. In a day, you write the simple SQL query and, after a bit of initial performance testing, add a few indexes. You don't expect this to be the subject of a spike in demand, so you just go directly against the RDBMS. Since you're using a snapshot-based isolation mode, this neither blocks nor is blocked by any other (read or write) request.</p>

<p>To directly answer your question, I'm pretty confident most comment systems <em>are</em> based on RDBMSs. It is very unlikely your use-case is an outlier where an RDBMS would not suffice. Usually the structure of the data is not a significant factor for choosing a database solution (full-text and graph-based data may be an exception). Usually what's more relevant is how the data will be used: issues such as latency requirements, consistency requirements, availability requirements, the cost of losing data, how independent the data is, and generally how you want to query the data. For most OLTP loads, including comment threads, RDBMSs make reasonable trade-offs for these issues.</p>
"
360022,"<p>The question is one of scale, where it will be hosted, cost and management.  If you know you are going to host in AWS, then you can take advantage of the distributed nature that makes the cloud more scalable.</p>

<p><strong>First Decision: Self Hosted vs the Cloud</strong></p>

<p>The old answers (circa 2014) reflect the mindset when self hosting was still predominant.  However, there are reasons to look outside of an RDBMS for tag related queries.</p>

<p>Filesystem hosting requires that you manage your NAS or SAN yourselves and ensure you have enough provisioning and the expertise to improve performance and capacity as necessary.  It can be very expensive if the costs are not amortized across several applications.</p>

<p>The cloud allows you to use AWS S3 or whatever equivalent blob storage for your cloud provider.  This solution only charges you for the storage you use, and cloud blob storage provides both the scale and performance needed to scale as your application grows.</p>

<p><strong>Second Decision: RDBMS or Search</strong></p>

<p>The way you have to store tags in a relational database vs. a document store makes the queries to get records related to those tags more difficult.  This is even more so when you are looking for intersections between tags (i.e. documents that have 2 or more identical tags).  The queries will slow down the more complicated it gets.</p>

<p>ElasticSearch, SOLR, and similar search servers that can double as a document store provide an ideal middle ground.  Many cloud providers have hosting solutions for these types of problems.  They are designed to scale to very large sizes and perform searches very quickly.  In fact this site (softwareengineering.stackexchange.com) uses ElasticSearch to do queries like this.  NOTE: ElasticSearch is also a NoSQL DB in addition to being a search server.</p>

<p>I will say that you can't think in relational terms when you are doing document searches so there is a learning curve.</p>

<p>Added bonus is that at least with AWS, ElasticSearch costs less than an RDBMS for the same size tier.</p>

<p><strong>Bottom Line</strong></p>

<p>Millions of records is not astronomical for today's RDBMS's.  However, you will reach a saturation point.  Many websites still use an RDBMS for the data storage of record and then synchronize that with a search server for the heavy lifting.  That decision really depends on things outside the scope of this question.</p>

<p>The ElasticSearch/S3 route will scale well beyond that.  However, do your research.  There are tradeoffs that you have to weigh.  In my case this choice was the right one.</p>
"
356334,"<p>It depends.</p>

<p>The behind the scenes implementation of the lambda runner is going to affect this. We can see that in AWS the container might be reused.</p>

<p><a href=""http://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction.html</a></p>

<p>So we could see connection pooling/reuse for some requests at least. Also we should consider the database itself and how it treats incoming connection requests.</p>

<p>This kind of question for me underlines some of the problems with 'serverless' its still very new and immature, so the details haven't been hammered out.</p>

<p>We should always remember that serverless doesn't mean no servers. If the rate at which you call a lambda is high enough, you <em>may</em> effectively have several servers, or 'containers' running.</p>

<p>In practice the start up time and resources such as IP addresses of lambdas can be a real issue. Perhaps as they mature a consensus of how to run them will appear and these problems will get solid answers.</p>
"
350817,"<p>Since this is a static site, I recommend deploying your site in Azure Storage optionally adding CDN.</p>

<p>There is a guide for static sites on Azure Storage <a href=""https://blogs.msdn.microsoft.com/make_it_better/2016/08/09/simple-websites-using-azure-storage-blob-service/"" rel=""noreferrer"">here</a>, and there is a guide for hooking up CDN to your storage <a href=""https://docs.microsoft.com/en-us/azure/cdn/cdn-create-a-storage-account-with-cdn"" rel=""noreferrer"">here</a>.</p>

<p>The benefits are:</p>

<ul>
<li>Very reliable. <a href=""https://azure.microsoft.com/en-us/support/legal/sla/storage/v1_0/"" rel=""noreferrer"">Has the same SLAs as Azure Storage</a> </li>
<li>Very low maintenance. Lower maintenance than the three options you suggested because don't have to concern yourself with all the additional details that app services requires. Don't have to deploy any software.</li>
<li>Performant. Again Azure Storage has pretty good <a href=""https://docs.microsoft.com/en-us/azure/storage/storage-scalability-targets"" rel=""noreferrer"">performance goals</a>, and performance can be improved by adding CDN</li>
<li>Low cost. Azure Storage will have lower cost than the other options because you <a href=""https://azure.microsoft.com/en-us/pricing/details/storage/blobs/"" rel=""noreferrer"">only pay for storage costs and bandwidth used</a> not cost of running machines.</li>
<li>Easy deployment. Azure Storage offers <a href=""https://stackoverflow.com/questions/22829854/how-to-upload-files-to-a-windows-azure-storage"">several methods of uploading content</a>, which should make automating deployments easy.</li>
</ul>
"
347591,"<p>The primary concerns of this design are security and size. But before I get there, I want to clear up a misunderstanding:</p>

<blockquote>
  <p>As wrong as it seems, would it be better to sacrifice normalization and include the franchise ID directly in some or all of these child tables?</p>
</blockquote>

<p>I see why you might think this: if you consider franchise ID as an attribute then including it in every table violates third normal form. </p>

<p>But here's an alternative way to look at it: <em>in the logical design</em> the row key includes franchise ID: <code>(FRANCHISE_ID, TABLE_ID)</code>.</p>

<p>But, you say, <code>TABLE_ID</code> is an identity column! To which I answer: yes, but that's a <em>physical</em> detail, not a <em>logical</em> detail. And logically, tables are allowed to have multiple ""candidate"" keys (I turn to <a href=""https://www.amazon.com/dp/"" rel=""noreferrer"">C. J. Date</a> as my authority for this statement).</p>

<p>And once you accept this logical design, you'll get a lot of physical benefits. First, you don't need joins to access data; while, logically, joins take no time, physically they do. Plus, if your queries tend to retrieve multiple rows for the same franchise, you can also benefit by <a href=""http://blog.kdgregory.com/2017/03/mysql-problem-with-synthetic-primary.html"" rel=""noreferrer"">using a clustered index to collocate rows</a>.</p>

<p>OK, now on to the main topics.</p>

<h2>Security</h2>

<p>From a corporate management perspective, this is probably your most important issue. Clearly, you can't allow one franchisee to see data that belongs to another. But there are many ways to accomplish this, imposing different levels of load on the system <em>and its developers</em>. I'm just going to throw out some ideas here for you to consider.</p>

<p><strong>Predicate applied to individual queries</strong></p>

<p>This is the simplest, but adds the heaviest load to the developers. Every one of your protected queries will have to include a check against franchise ID. Forgetting even one could have economic consequences for your company (ie, lawsuits).</p>

<p>However, I think you can probably overcome this with a combination of code review, static analysis, and integration testing. You need the discipline to ensure that all queries go through a data access layer that's rigorously verified.</p>

<p><strong>Views</strong></p>

<p>To ensure that all queries include a check for franchise ID, you can hide your tables behind views, and ensure that each view includes a franchise check. Each franchisee will have their own set of views, stored in a different schema.</p>

<p>An additional benefit of this approach is that you'll be able to expose data directly to the franchisees. It also allows your physical tables to change without affecting the exposed data.</p>

<p>However, there are several significant drawbacks. First, your developers will have to ensure that they use the correct set of views for each query (maybe not that bad, depending on how you manage connections). Second, you will have a long-term maintenance cost, as changes have to be propagated to all of schemas that hold a particular view (although this should be easily automatable).</p>

<p><strong>Row-level Security</strong></p>

<p>I'm not familiar with SQL-Server, but my understanding of row-level security is that it's based on database users, so you'll need (at least) one user per franchisee. Which means that you'll need (at least) one connection per franchisee, which may cause undue load for your database (or alternatively, constant creation/destruction of connections). I'm also guessing that your developers will have to code queries that include franchise conditions or suffer runtime errors. And you'll have to manage all of those users.</p>

<p>All-in-all, this seems like the most painful route, but it is the one that guarantees security, so I'm guessing it's the way you'll go.</p>

<h2>Size</h2>

<p>From the development perspective this is going to be the bigger pain point -- especially when your users complain about slow response times.</p>

<p>Your overriding goal should be to touch as few data blocks as possible per query. Here are a few techniques that I've used successfully in the past:</p>

<p><strong>Buy as much RAM as you can afford</strong></p>

<p>Your goal should be to keep the entire database in memory. Really. It doesn't matter that SSDs are blindingly fast, they still require time to read and write data blocks.</p>

<p>In a perfect world, you would read the entire database into memory at startup, and the only IO would be writes.</p>

<p><strong>Reduce the ""active"" size of tables</strong></p>

<p>You mentioned one 91MM row table. How much of this table is accessed for a particular query? Can you partition the table so that infrequently accessed data is stored in another table? (I'm assuming that SQL-Server supports declarative partitioning, but if not you can manually move/duplicate rows).</p>

<p>Large tables necessarily mean that queries have to access a lot of rows. Even if you have indexes, because those indexes will also be large.</p>

<p><strong>Collocate data</strong></p>

<p>By default, databases store rows wherever they can find the space. Which means that data that is typically accessed together, such as the transactions for a user, might be spread all over the disk.</p>

<p>However, you generally have some level of control over this, either using clustered indexes (see my link above) or covering indexes. Leverage these to their fullest.</p>

<p><strong>Use read replicas</strong></p>

<p>A typical application executes selects far more frequently than updates, and tends to select multiple rows while updates affect a single row. </p>

<p>By separating these two operations, you get a couple of benefits. First, you can scale capacity independently: if you have a lot of reads you can buy more or bigger machines. And second, you can reduce contention: a long-running select won't block an update (personally, I think this is less of an issue today than, say, 20 years ago, but it's still worth considering).</p>

<p>The downside of read replicas is that there's a lag between the time a row is updated on the server and on the replica. This may or may not be a problem for you (and in my experience, lag is caused by undersized machines; more money solves that problem).</p>

<p><strong>Offload reporting to a data warehouse</strong></p>

<p>True ""reporting"" queries tend to be very different from operational queries. For example, an operational query might retrieve the most recent order for a single user, while a reporting query might find all users that bought a particular product. As a result, attempting to support both operational and reporting queries with the same physical design is a recipe for failure.</p>

<p>At the very least, shift reporting to a dedicated read replica, one that is indexed appropriately. Better is to make use of a completely different DBMS, one whose storage and query characteristics more closely match your reporting needs. Something like <a href=""https://en.wikipedia.org/wiki/Amazon_Redshift"" rel=""noreferrer"">Amazon Redshift</a>, <a href=""https://en.wikipedia.org/wiki/BigQuery"" rel=""noreferrer"">Google BigQuery</a>, or <a href=""https://en.wikipedia.org/wiki/Azure_SQL_Data_Warehouse"" rel=""noreferrer"">Azure SQL Data Warehouse</a>. Or maybe a locally-hosted option like <a href=""https://en.wikipedia.org/wiki/Apache_Cassandra"" rel=""noreferrer"">Apache Cassandra</a>.</p>

<h2>And now for something completely different</h2>

<p>Don't do this.</p>

<p>The time taken to develop a multi-tenant solution is time not available to add features that may be more relevant to your franchisees, or to improve the current code and processes.</p>

<p>If the issue is maintenance, or per-franchise capital expenses, look to alternatives that enable central management. For example, use Azure or another cloud provider with an ops person at the corporate office. You should be able to deploy a cloud-based solution for a per-franchise cost of a few hundred dollars per month (if that), cutting both capital and operational expenses for the franchisees.</p>

<p>If the issue is reporting, focus on more efficient data acquisition and transformation. Again, cloud-based solutions can help with this.</p>

<hr>

<h2>Update</h2>

<p>The idea of moving to a cloud provider -- and Azure is only one option, which I picked because you seem to be a Microsoft shop -- is to eliminate the problems caused by franchisees who aren't trained computer operators.</p>

<p>In the simplest form, you would create a database server for each franchisee in the cloud, and their existing applications would point to that server rather than the local database. The database would always be up, so that you could retrieve data at any time. And, typically, the cloud provider does regular backups and provides other options for fault-tolerance and recovery.</p>

<p>Pricing in the cloud is largely dependent on the features that you want. For example, looking at the <a href=""https://azure.microsoft.com/en-us/pricing/details/sql-database/"" rel=""noreferrer"">Azure Cloud SQL pricing page</a>, the base price for a ""Standard"" database service is $0.0202/hour, or $15/month. I have no idea what this actually provides you in terms of database performance; in my experience, $100/month is more likely.</p>

<p>There is nothing that prevents you from using cloud hosting as a first step, and then moving on to a true multi-tenant solution. And if you have hundreds or thousands of franchisees, that makes sense to manage cost. But it seems like your real problem is one of operations management.</p>
"
342331,"<p>Consider a greenfield development situation where cloud tools are being considered vs. in-house solutions, in the vein of AWS SQS vs. self-hosted Kafka, ECS vs. Mesos-Marathon, Lambda/Azure Functions vs. Whisk vs an array of custom APIs/services.</p>

<p>All else being equal (financial cost, technical expertise, etc.), how can the cost of vendor lock-in be fairly gauged when deciding whether or not to use cloud services beyond basic VM and storage products? I have seen in several cases, where fears of vendor lock-in closed the argument on using higher-level cloud services without even allowing for a technical or financial evaluation of their value to the project.</p>

<p>Of course there is <em>a</em> cost to using vendor-specific services, but that cost can't possibly be so large as to eclipse all other software development costs. Avoiding higher-level cloud services seems, IMO, to be an argument akin to ""let's build a completely abstract ORM in case we need to swap out database products.""... aka <a href=""https://en.wikipedia.org/wiki/You_aren&#39;t_gonna_need_it"" rel=""nofollow noreferrer"">YAGNI</a>.</p>

<p>Self-sufficiency is often the road to poverty, and all software is dependent on many other layers to be successful: Docker, Linux, npm, gcc, and dozens and dozens of others, but these are rarely looked at as ""lock ins"". The costs of doing <em>anything</em> internally, can be significant, including:</p>

<ul>
<li>Lost time to market</li>
<li>Resources devoted to maintaining internal and non-revenue generating services</li>
<li>Higher operational costs</li>
</ul>

<p>So, what is the right way to fairly evaluate cloud services, acknowledging the cost of vendor lock-in as one component in product strategy, without allowing it to dominate other concerns?</p>
"
340772,"<p>If you want to add something to the IDs that would act as a filter from bad data it probably would be better to include eg. last 4 chars of sha1 or md5 of the ID+salt itself so it can't be easily fabricated.</p>

<p>But I think, however that sending this ""checksum"" data as another parameter would be better. So you could send ID + checksum, and discard the checksum in the process, or at least save it outside of the ID so you won't add unnecessary data to a key which is indexed, as all data you put there is costly from DB perspective. Best would be if you provide some data sample.</p>

<p>The other part of your question is interesting... because you say that you're going to redesign your application partly because of pricing model of AWS, while for $90/month you can get a server with 32GB RAM, 8 cores and half TB of SSD storage. It's nice to pay for peace of mind if you have money and want to work on something that ""just works""... but if you're afraid about the costs, then I think it'd be much better from your perspective to invest your time in moving off of AWS, because the price to performance ratio is so laughable there, that the costs will KILL you in the long run. For me these shared cloud enviroments are always limiting factor, slow, expensive, with crappy, old software installed and bandwidtch priced like crazy.</p>

<p>I mean if you can have millions of requests per second on a server that costs nothing, thinking about every request on AWS is a waste of life, which you can waste on eg. designing new features for your app ... and beside it's pointless, it won't become cheap, no matter how much you castrate the code, compress the communication, etc. it'd be still expensive as HELL for anything but an app that has no traffic ;)</p>
"
340747,"<p>If i understood you correctly </p>

<ul>
<li>you already have many different gui-implementations for an exsting backend-api and </li>
<li>you want to modify existing implementation-details of the backend without modifying the api.</li>
<li>your goal: convert backend into a cloud app so the app is more scalable</li>
</ul>

<p>Your question is: In wich order to reimplement ""implementation-details""</p>

<p>The answer is similar to the question: ""which code to change to optimize performance(speed)"" : measure/profile which code slows down the system.</p>

<p>In your case measure which sub-workflow benifts most from a scalable cloud service.</p>

<p>While ""storymaps"" is a great tool for agile projects in my opinion it is not suitable for your type of project.</p>

<p>note: In my opinion a storymap is workflow/feature oriented, your example tasks are component/modul/architecture oriented. </p>
"
338746,"<p>You need to clarify what kind of high availability you're looking for. There are highly available applications that I run that need to be up 95% of the time. There are others that need to run at 99%. I can think of life-or-death scenarios that require 100% uptime. Just those three have drastically different approaches and costs.</p>

<p>Just guessing based on your needs and a 95-99% uptime SLA:</p>

<ul>
<li>Database migrations should be able to happen in real time for most changes. Practice <a href=""http://martinfowler.com/articles/evodb.html"" rel=""noreferrer"">Evolutionary database design</a>. For changes that do require more invasive behavior, you have a few options. One is take the downtime. If possible, running your service in read-only mode might work. For full functionality, I've been wanting to try ScaleArc for a while. It looks like a really slick tool for scaling and resiliency in the SQL Server world.</li>
<li>Putting servers inside your customer's sites is a recipe for an unmanageable disaster unless you've got world-class deployment strategies (which, based on your description of your migrations, you don't have yet). Don't push cloud services on-prem because you have performance problems. Solve the performance problems now and then you won't have to deal with costlier ones done the road.</li>
<li>Your state server should be a database of some sort. Follow their HA guidelines. You can use SQL Server for this, since you already have it available to you.</li>
<li>Speaking of databases, replication does not enable HA. In fact, SQL Replication will cause you headaches around every turn (speaking from experience with multiple node replication scenarios). Mirroring can work, but last I remember, SQL clustering takes 1-5 minutes to fail over to the new server. I've heard good things about AlwaysOn, but I'm still suspicious given Microsoft's track record. Something like ScaleArc might be more help here.</li>
<li>Your web server should be stateless. Spin up three or four and put them behind a load balancer. That solves your uptime worries there. As Frederik mentioned earlier, you can also do rolling deployments this way.</li>
<li>Your web service should probably be stateless. If not, see if you can break it apart into stateless and stateful bits. Putting multiple instances of it behind the same load balancer again solves uptime worries and enables more interested deployment scenarios (e.g. blue/green deployments).</li>
</ul>

<p>Unlike Frederik, I won't call your cloud paranoia unwarranted. It depends on your uptime requirements. It is conceivable that a service would have to run in multiple data centers operated by different providers in different countries for redundancy's sake. Given your current state, however, I'd agree that AWS, Azure, or similar are probably safe bets for your company.</p>
"
338692,"<p>Getting some level of HA on your web &amp; application tier:</p>

<ol>
<li><p>Ideally, factor out any state, including session state into shared-state systems like a database or an in-memory session state server.  Depending on your application design this may cause performance issues due to the added latency getting a large amount of state. </p></li>
<li><p>Your web site &amp; application tier should each have an independent load balancer in front of them. NGINX will do the trick, but IIS can do this too (ARR). </p></li>
<li><p>If a single database can't handle the load, leverage session state partitioning (or sharding or consistent hashing) to route particular request to a particular database box.</p></li>
</ol>

<p>If factoring out state is too hard, you can go with server affinity for load balancing (ie users are consistently routed to the same box, often cookie based). It's not as highly available as a stateless round robin approach, because a box outage will impact all users &amp; state on that that box, but it beats a complete outage (use-case dependent). </p>

<p>On the upgrade side:</p>

<ol>
<li><p>Design your database scripts in such a way that database upgrades can be done while the system is running, in other words, maintain backwards compatibility.  A pattern that works well for that is ""expand, then contract"" -> make only additive, backwards compatible changes but removing dependencies on the fields (etc) that you want to get rid of; then upgrade all clients of the database to v-latest; then do another db-upgrade to get rid of the old fields (etc) in the database.   This can be a slow process if you have a large database and you have to be careful to not scupper the performance of your system. </p></li>
<li><p>Upgrading your app tier: since your not using a cloud environment, I recommend you follow the canary deployment pattern: do a rolling upgrade of your web &amp; middle tier boxes. If the deployment goes wrong, take the box out of the load balancer, just like you would as if it had failed.</p></li>
</ol>

<p>Word of warning: evolving a system that hasn't been designed for HA into one that is, can be a long and costly process. You'll have to make trade-offs along the way (cost vs effort to reach a particular level of availability)</p>

<p>Your cloud paranoia is unwarranted - providers such as AWS in conjunction with good practice on your part can control / mitigate most risks - have a look at their compliance page to get a feel for what regulations they're compliant with: <a href=""https://aws.amazon.com/compliance/"" rel=""nofollow noreferrer"">https://aws.amazon.com/compliance/</a>  </p>
"
336154,"<p>In addition to Robert's answer, you can use any of the mentioned servers in production environments. We do it often. Tomcat is one of the most used.</p>

<p>The point of these servers is that they are OpenSource and free. This fact makes them suitable for development because there're no constraints due to licences. Plus they are lighter than others like JBoss, WebSphere, Weblogic. If you have to run the IDE, the server,  browsers like Chrome, editors, etc, in local, you will appreciate their lightness.</p>

<p>Related to PaaS, Google's and Amazon's PaaS are not cheap. It makes  customers decide to go to a self-hosting or private cloud.</p>

<p>Self-hosting and private clouds imply a lot of things to be taken into account: network configuration, security, maintenance, monitoring, etc. A lot of work apart from the application maintenance. </p>

<p>If you go cloud, PaaS providers already have solved many of these issues. They also provide you with web tools like consoles to manage your environment.</p>

<p>But you pay for it of course. Also, you pay for bandwidth consuming, storage space, etc.</p>

<p>In both scenarios, you are free to use the app server you want. OpenSource and free are not synonyms of ""inadequate for production"". What makes a server suitable for production are its capabilities and features. If they have those you need, they are totally suitable for you.</p>

<p>Once you have the stack of technologies that forms your whole system, you would rather choose the PaaS that gives you better conditions and facilities to deploy it.</p>
"
334800,"<p>We have implemented several push servers and all of them following the <em>old tuple</em> ""socket-certificate"".<em>In Java (1.5 - 1.7)</em>.</p>

<p>To work with certificates has some disadvantages. For instance, we need one for each environment (test, pro, etc). You have to be methodic at managing them or it's quite easy to end up with the wrong cert in pro. Or to forget its renewal (they also expire). </p>

<p>Related to the socket, this approach requires having opened a specific range of ports in the firewall.</p>

<p>Related to the whole protocol of communication. You get so little info after pushing messages. It's hard to figure out what happened with the messages. The only way is to retrieve messages from the queue of responses. A queue which orders is not guaranteed. Neither when APNS is going to put the responses onto it. It might not happen at all.</p>

<p>Compared to GCM (Google cloud message which runs via HTTP), the ""socket-cert"" of APNS is a pain in the ...</p>

<p><strong>My suggestion is to get focus on the HTTP 2 - JWT</strong> protocol. This is a very common implementation of security in client-server communications. You will find many more references about http2 and JWT than looking for APNS sockets and Certs.</p>

<p>Security via JWT is commonly implemented these days. There is full support from the community.</p>

<p>Moreover, If they have planned to drop the support to the current implementation, why even to dare to try it? Why spend time and money twice?</p>

<p>Be preventive. Implementing HTTP2 - JWT approach will save you from further code reviews and refactors. In any case, it's a work to do, so better have it done sooner than later.</p>

<p>Related to the library CleverTap. Well nobody is stopping you from implementing your own client! Suited to your need and requirements.</p>

<p>This has been our case with our current engine. We discarded all the 3rd party implementations and built our own. So far I know it keeps working perfectly... Till Apple drops the service.</p>

<p><em>(If we didn't move yet to HTTP2 - JWT is due to  time and money)</em></p>

<p>There's perhaps alternatives. Google Firebase Cloud Message is multi-platform. So you can push messages to Android and iOs devices from the same service. It works over HTTP and API keys (tokens). I suggest to you to take a look.</p>
"
334294,"<h1>Good ol' Uncle Bob</h1>
<p>I'd encourage you to consider what <a href=""https://www.youtube.com/watch?v=Nsjsiz2A9mg&amp;feature=youtu.be&amp;t=42m17s"" rel=""nofollow noreferrer"">Robert &quot;Uncle Bob&quot; Martin has to say on databases</a>. I find it interesting, with regards to some considerations he discusses regarding SQL and NoSQL.</p>
<h1>TLDR</h1>
<p>SQL popularity was influenced by corporate entities, the open source community, technology stacks like <a href=""http://en.wikipedia.org/wiki/LAMP_%28software_bundle%29"" rel=""nofollow noreferrer"">LAMP</a>, and physical storage limitations of hard drives. As of 2016, your <a href=""https://jaxenter.com/the-top-10-sql-and-nosql-databases-108072.html"" rel=""nofollow noreferrer"">top three</a> database systems are all SQL-based (Oracle 12c, <a href=""http://en.wikipedia.org/wiki/Microsoft_SQL_Server"" rel=""nofollow noreferrer"">Microsoft SQL Server</a>, and MySQL). It was also pointed out in the comments below that SQL has been an approved standard for both <a href=""https://ansi.org/"" rel=""nofollow noreferrer"">ANSI</a>,(American National Standards Institute), and the <a href=""https://www.iso.org/obp/ui/#iso:std:iso-iec:9075:-1:ed-4:v1:en"" rel=""nofollow noreferrer"">ISO International Standards Organization</a> for decades, which has vast impacts on its predictability and quality - a very important business consideration for many enterprises.</p>
<hr />
<h1>Physical Storage Considerations</h1>
<p>Until the wide-spread appearance of SSDs and cloud computing within the last decade, SQL was the predominant leader in database languages; with these new technologies, however, a new evolution of database languages are appearing as well. There have been some articles discussing the <a href=""http://www.computerworld.com/article/3040694/data-storage/ssd-prices-plummet-again-close-in-on-hdds.html#tk.drr_mlt"" rel=""nofollow noreferrer"">closing gap in price and increase in both popularity and accessibility</a> of SSDs over HDDs. There's a great website for <a href=""http://db-engines.com/en/ranking"" rel=""nofollow noreferrer""><strong>ranking of database technologies</strong></a> available, which might interest you. You can see that SQL and relational databases still have a firm foot-hold, but NoSQL alternatives such as MongoDB are <a href=""https://www.marketresearchmedia.com/?p=568"" rel=""nofollow noreferrer"">creeping up as well</a>.</p>
<h1>The Goliaths</h1>
<p><a href=""https://en.wikipedia.org/wiki/SQL"" rel=""nofollow noreferrer"">SQL</a> has gained increasing popularity over the past several decades since it really took off in the mid 1980s. Today, the &quot;concrete&quot; nature of its popularity can be considered with regards to two of the most influential companies on the planet, whose enterprise products rely largely on solutions revolving around the SQL language. Those two companies are: Oracle and Microsoft. Countless small, mid, and large-sized companies use Oracle and Microsoft products, such as <a href=""https://jaxenter.com/the-top-10-sql-and-nosql-databases-108072.html"" rel=""nofollow noreferrer"">Oracle 12c, Microsoft SQL Server and MySQL</a>. It's also important to note the impact of the open-source community as well, which over the past 20 years or so in which it was very common to see many developers using PHP and MySQL together through things like the LAMP, <a href=""https://en.wikipedia.org/wiki/MAMP"" rel=""nofollow noreferrer"">MAMP</a>, <a href=""http://en.wikipedia.org/wiki/XAMPP"" rel=""nofollow noreferrer"">XAMPP</a>, and <a href=""http://en.wikipedia.org/wiki/WAMP"" rel=""nofollow noreferrer"">WAMP</a> stacks.</p>
<p>Both companies, Microsoft and Oracle, are software goliaths, and the OOP programming languages they provide are two of the most popular: Java and C#.</p>
<p>Java, being acquired by Oracle about 6 years ago, is one of the most popular programming languages in the world. Hence, Oracle has a very vested interest in making sure that its programming language is used in tandem with MySQL, which is the source of a very high revenue for the company.</p>
<p>C#, being owned by Microsoft, integrates very well with SQL Server through Microsoft's various database offerings, such as through cloud storage in Azure as well as local installations of Windows Server, which typically runs with an instance of <a href=""https://msdn.microsoft.com/en-us/library/mt238290.aspx"" rel=""nofollow noreferrer"">SQL Server Management Studio</a>.</p>
<p>Other popular languages like PHP also integrate well with MySQL, due to the open-source nature of those languages. The LAMP (Linux, Apache, MySQL, and PHP), as well as MAMP, XAMP, and WAMP, also provided a foray into a mix of easy-to-use out-of-the-box open-source technologies that were all the rave about 20 years ago (and still are very popular to this day).</p>
<p>The open-source community around these languages (in particular, SQL) are huge, so it is quite easy to find support.</p>
<p>The language itself is also very easy to understand and read, given that it is very close to how you would structure a logical English sentence (if there is such a thing!).</p>
<h1>Some Competition</h1>
<p>However, while SQL and other relational databases are holding strong, other alternative document store technologies, like NoSQL, <a href=""http://en.wikipedia.org/wiki/MongoDB"" rel=""nofollow noreferrer"">MongoDB</a>, <a href=""http://en.wikipedia.org/wiki/CouchDB"" rel=""nofollow noreferrer"">CouchDB</a> and Microsoft's NoSQL implementations are becoming <a href=""http://www.techrepublic.com/article/nosql-keeps-rising-but-relational-databases-still-dominate-big-data/"" rel=""nofollow noreferrer"">increasingly popular as well</a>.</p>
<h2>Physical Tech Advances</h2>
<p>Part of this increase in the use of NoSQL-esque platforms may be due to the physical hardware technologies that are settling into the market. As SSDs and HDDs are becoming <a href=""http://www.computerworld.com/article/3040694/data-storage/ssd-prices-plummet-again-close-in-on-hdds.html#tk.drr_mlt"" rel=""nofollow noreferrer"">more closely comparable in price</a>, storing data in the form of large JSON documents and in repository and <a href=""http://searchstorage.techtarget.com/tip/Nine-reasons-object-level-storage-adoption-has-increased"" rel=""nofollow noreferrer"">object storage systems</a>, such as <a href=""http://www.computerweekly.com/feature/Amazon-S3-storage-101-Object-storage-in-the-cloud"" rel=""nofollow noreferrer"">Amazon S3 Storage</a> becomes relatively less expensive, both physically and fiscally, whether you are considering storing it on your own hard drive locally or somewhere in the cloud.</p>
<h2>Cloud Offerings</h2>
<p>Cloud offerings are becoming cheaper by the day, and 1 TB of storage in the cloud is practically <a href=""https://www.cnet.com/how-to/onedrive-dropbox-google-drive-and-box-which-cloud-storage-service-is-right-for-you/"" rel=""nofollow noreferrer"">dirt cheap as well</a>. I personally have 1 TB of Google Drive storage for $1.99 per month (about $25 per year).</p>
<h1>Some Conclusions...</h1>
<p>Storage space (physically) used to be a crucial issue, which is where many SQL databases stood to serve a need for a data language that could be used to keep data storage as efficient as possible. This is where you get the <a href=""http://en.wikipedia.org/wiki/Create,_read,_update_and_delete"" rel=""nofollow noreferrer"">CRUD</a> style applications (Create, Read, Update, Delete). When storing massive amounts of data in a SQL database, you want to avoid duplication as much as possible through a process known as normalization, which has to do with designing the database in such a way as to have as efficient of a relational mapping as possible. This would help the physical lookup speed (also improved through indexing), but all of which was centered around a major concern, which was physical space. You simply couldn't afford to have massive amounts of duplicate data. Not to mention, the need for database replication and backup, to protect against things like natural disasters or power outages, terrorists, etc.</p>
<p>Huge companies with massive amounts of credit card data or health information couldn't afford to go down for even a minute, so they would need to replicate and distribute their data centers geographically to protect their users' data. This was enormously expensive, because every database needed a physical endpoint, a computer and a hard drive to store the mass of information.</p>
<p>Fast forward to today, the need for physical size is less expensive. We'll likely see a shift away from SQL over the coming years, but it has such a large user base, and many large companies are so firmly rooted, that this will likely take a while.</p>
<p>However, new industries, such as infrastructure as a service, provided by companies like Amazon with their <a href=""http://en.wikipedia.org/wiki/Amazon_Web_Services"" rel=""nofollow noreferrer"">Amazon Web Services</a> (AWS) - as well as Microsoft <a href=""https://en.wikipedia.org/wiki/Microsoft_Azure"" rel=""nofollow noreferrer"">Azure</a>, large scalable datacenters filled to the brim with high-capacity, lower-power consuming SSD hard drives are making it increasingly possible for those large companies to port over to newer, more efficient technologies. As a result, we will likely see the change in database language to reflect new physical, logical, and other scientific limitations.</p>
"
333291,"<p>Since you are CPU-limited, you need to get your hands on 150 CPU cores, one for each thread. This rules out a single server, since a server of such proportions would be prohibitively expensive – and you don't really need it.</p>

<p>Your general architecture with a common frontend that distributes work to multiple workers and combines their results appears to be sensible. You'll have to do some calculations to find the most cost-effective solution to get that many CPUs. That tends to point towards AWS Lambda since you only require computations in bursts, but it may come with restrictions. How many Lambdas may execute simultaneously? 150 at once is a lot. Which languages can you use; can you reduce your cost by using optimized native code? Importantly, I don't think Amazon makes specific performance guarantees for that product, whereas you have more control over the physical CPU with more traditional instance types.</p>

<p>And the actual CPU performance is important for you. While you are willing to kill the computation after 5 seconds, the amount of computation performed until then may vary wildly. You could probably manage to get 150 cores rather cheaply by running a Beowulf-cluster of Raspberry Pi boards in your basement, but that is not remotely comparable to the computation power of five high-end Intel Xeon servers.</p>

<p>It is therefore important that you clearly define your performance goals and a SLA and then test a proposed solution. You will also have to think about simultaneous requests. Given the high amount of computations per client request, it may be best to process client requests sequentially if that is acceptable for the clients. But this also puts an upper limit on the clients you can support, since the probability that a client has to wait before their request can be processed grows rather quickly (related to the birthday paradox).</p>

<p>This is a scalability problem. You can either delay it by scheduling client requests as to avoid simultaneous requests, or gain the ability to handle multiple requests in parallel. That in turn can either be managed by throwing more money/servers at the problem, or by performance-tuning of the algorithm. E.g. I've seen a case where a Python program could be made 3× faster by profile-guided optimizations like extracting an instance attribute access out of a very tight loop. The biggest wins always come from algorithmic complexity reduction, if they are possible.</p>
"
333128,"<p>In order to avoid one monolith file containing all the code of your application, you may use two techniques:</p>

<ul>
<li><p>Frameworks. There are a bunch of JavaScript frameworks which will let you declare dependencies between files, and load those files on demand. A popular framework is <a href=""http://requirejs.org/"" rel=""nofollow noreferrer"">RequireJS</a>.</p>

<p>This gives you a straightforward approach where all the dependency work is done for you, and you can focus on coding, like you do in most mainstream languages such as Python, Java or C#. Note that Node.js follows a similar approach.</p></li>
<li><p><a href=""https://stackoverflow.com/q/17776940/240613"">Module pattern</a>. Here, you are not relying on a third party library for the dependencies, but simply split your code into parts which are very isolated one from another. Since you're familiar with OOP, the module would be close to at the same time an object and a namespace: you'll find ways to clearly declare which functions can be called from the outside world, and keep the other functions private.</p>

<p>Once you have your separate modules, you can split your monolith file into smaller files—usually one file per module. From there, you have a choice: either you simply load all the files one by one from the browser every time (which is not the same as “every request”: if you configured your client-side caching properly, the files will be cached for a very long time). Or you bundle all your files into one (or several, whatever makes sense in your case) to reduce the number of requests.</p>

<p>If you want to bundle those files, there are a bunch of server-side libraries which do that. Depending on your environment, you may search for one, or write one yourself.</p>

<p>Minification could also be a good idea, and <a href=""https://developers.google.com/closure/compiler/"" rel=""nofollow noreferrer"">Google Closure Compiler</a> is one of your options. Note that Google Closure Compiler have different levels from a simple “let's strip whitespace” to a very capable and very aggressive transformation of your code. If you try to use the aggressive one on existent code, chances are you won't be pleased with the results, at all. It takes time to become accustomed to how Google Closure Compiler, at this level, wants you to write your code.</p></li>
</ul>

<p>Note that bundling your JavaScript doesn't necessarily lead to better performance. If, for instance, your website has a lot of pages, and each page uses little common code, RequireJS approach would mean two things:</p>

<ul>
<li><p>A user arriving on this page will download only a tiny fragment of all your JavaScript code, i.e. just what is needed for this page. Pushing a huge bundle composed of dozens of files to your user who just needs two or three of them isn't the nicest thing you can do.</p></li>
<li><p>If you change one of the files frequently, RequireJS will just download the new version of this file for the users who already visited your site before. If it's a bundle, a tiny change would invalidate the entire bundle.</p></li>
</ul>

<p>Note that bundling and minification are two different things. You can do one without the other.</p>

<p>Finally, remember that there are some very important aspects which should be your primary concern:</p>

<ul>
<li><p>Client-side caching. JavaScript files can and often should be cached for a very long time, without the need for the browser to even do the check and receive HTTP 304 Not Modified in response. This also means that whenever you change a JavaScript file, all links to it should be changed, i.e. <code>http://cdn.example.com/js/hello.js</code> will become <code>http://cdn.example.com/js/hello.js?r=1</code>, then <code>http://cdn.example.com/js/hello.js?r=2</code>, etc.</p></li>
<li><p>CDN. If you are serving Chinese users from Europe, or Indian users from USA, you're doing it wrong. More importantly, if you are serving static content from your server which is already in charge of dynamic content, you may not only degrade performances, but also pay simply too much for the bandwidth. Services such as <a href=""https://aws.amazon.com/cloudfront/"" rel=""nofollow noreferrer"">Amazon CloudFront</a> ensure excellent delivery performance for your static content.</p></li>
</ul>
"
331073,"<p>My company develops technology for visitor management. We currently have 2 solutions, one that is in the field, running on on-site hardware. The other is cloud, running on the cloud and processing all visitor related thing remotely.</p>

<p>Management wants us now to build a 3rd product to retire the two previous products and roll them into one. A web service to handle all the data either on site (for robustness) or in the cloud (for simplicity).</p>

<p>In my mind, what I need is a web service to handle the data processing, and it just just be spun up on a local machine or in the cloud, depending on need.</p>

<p>Could anyone suggest some architectures to learn or read more about?</p>
"
324564,"<p>Here is my take on this : </p>

<p>The requirement is to find out the sleeping activity of the user ids that are stored in MongoDB which can grow till  1 Million. Later this requirement can be extended to pull other activities of the user. </p>

<p>To build a decoupled scalable system, you can create 2 services. </p>

<p>Service 1 : picks up the userIds from mongoDB or other datasources (if want to change it to something else tomorrow) and keep it in a Queue Service. I would suggest using a cloud-based Queue Service like SQS. </p>

<p>Service 2 : Picks up the data from a Queue and tries to contact 3rd party services like FitBit to get the activity. Let us say if Fitbit service is down/ or your service became quite popular and if there is a surge in the users you can increase the hosts that can consume the messages. You can use Auto-Scale feature of Amazon Webservices to take care of autoscaling. </p>

<p>Let's say tomorrow you want to fetch more activities of users from other systems like Google Fit to summarize all their fitness activities the same architecture works perfectly. The only change you need to do is to use a Simple Notification Service, instead of SQS which pushes the user ids into various SQS queues and each queue will be consumed by a different activity handler to process the data and updates your datastore.</p>
"
315965,"<p>Although, the question may be answered and you decided to accept this answer, I want to highlight the topic from another side.</p>

<h1>1) JSP as a templating system</h1>

<p>As a templating language, I think, JSPs are good like any other. You may find, that <a href=""http://www.thymeleaf.org/"" rel=""noreferrer"">Thymeleaf</a> suits your needs better, but that's subjective.
JSPs are an old - or better mature - technique to get content to the client. In that it is comparable to ASP or even PHP. </p>

<p>The main downside to JSP as a language is, that it is XML-based and as such has a lot of visual overhead or noise. In thus it is comparable to <a href=""https://chameleon.readthedocs.org/en/latest/"" rel=""noreferrer"">Chameleon</a> (a Python template engine). That makes it sometimes hard to understand.</p>

<p>A much more cleaner way of doing this is, e.g. <a href=""http://www.mitchellbosecke.com/pebble/home"" rel=""noreferrer"">Pebble</a>.</p>

<h1>2) Server-side-rendering</h1>

<p>As of 2016 serverside rendering is not dead. On the contrary, if you take <a href=""https://blog.twitter.com/2012/improving-performance-on-twittercom"" rel=""noreferrer"">Twitter</a> as an example:</p>

<blockquote>
  <p>The bottom line is that a client-side architecture leads to slower performance because most of the code is being executed on our users’ machines rather than our own. </p>
</blockquote>

<p>Of course, there are client-side frameworks like: <a href=""https://facebook.github.io/react/"" rel=""noreferrer"">React</a>, <a href=""https://angular.io/"" rel=""noreferrer"">Angular</a> or even <a href=""https://vuejs.org/"" rel=""noreferrer"">Vue.Js</a>. But what they have all in common is, that they are <em>bloated</em>.
And I am saying this not, because I do not like them, but I want to emphasize, that using a Javascript framework like Angular comes at a cost, which might not be realized at first sight - our desktops are fast and our connection is stable and fast too - but if you look at mobile, the whole picture changes. </p>

<p>There are mainly three costs on mobile: </p>

<p><strong>1) Rendering</strong></p>

<p><strong>2) Performing</strong></p>

<p><strong>3) Battery drain</strong></p>

<p>Performing bad in any one of these categories, makes you loose customers. Performing in more of these is even worse.</p>

<p>So there are three ways to solve this problem:</p>

<p><strong>1) Getting started really fast</strong></p>

<p><strong>2) Make as less calls after the page is initially loaded as necessary</strong></p>

<p><strong>3) Reduce fanciness without renouncing an appealing design</strong></p>

<p>This is, where server-side rendering comes (<em>again</em>) into play. 
To get the page initially up and running, it is necessary to deliver as many information (is necessary) at the first load of your page. Who said, that your server needs only to render your HTML? You could render a startup portion of JS on page with a script-tag. Of course we were told, years ago, to separate Javascript into its own files, but in need of performance, you have to blurry the lines a bit.</p>

<p>So what about JSPs? With expression language and some JSTL (e.g. <code>&lt;c:if&gt;</code>) you have all, you'll ever need.  </p>

<h1>3) Regarding your concerns</h1>

<blockquote>
  <p>The JSTL syntax required to make highly-interactive pages via JSP's is getting awfully unwieldy. I'm worried that, when we expand our project and bring on more engineers, there will be a steep-ish initial learning curve, followed by a persistent slow-down in the dev process due to verbosity of JSTL.</p>
</blockquote>

<p>From what was said: Yes, there is a big degree of verbosity if you excessively want to use every feature JSPs offer, but if you reduce your language set, JSPs are not that different from other templating engines.</p>

<blockquote>
  <p>Server-side rendering has been great for my testing, so far -- but what happens when our app is a massive, historic, global success and downright phenomenon? Will the server get bogged down with all the rendering, if done in JSP's? Or are traffic concerns better addressed by load-balancing requests to the server and leaving the server-side rendering in place?</p>
</blockquote>

<p>The question makes wrong assumptions about how a webpage is assembled and deliverd to the client. In terms of <strong>2000</strong> there was the big fat web server, serving all of the hundreds of thousands of users a website had at that point. If you needed more performance <em>scaling up</em> was the way to go. But as we know today, this only works to a limited degree. Instead of <em>scaling up</em> we <em>scale out</em>, having more and more smaller servers taking off the load. </p>

<p>In the context of 2000 your question made sense: If you have one fat server, serving millions of requests and doing all the business logic alone, you had a performance problem- but not due to website templating (remember: JSPs are precompiled and that makes stuff really fast).</p>

<p>Today you are separating functionalities and refactor portions of your application to separate services (some call them <em>microservices</em>, I like the term <em>focussed service</em> or <em>self contained systems</em> better. </p>

<p>So you balance the load by design. And the effect of templating is still marginal.</p>

<blockquote>
  <p>Along the lines of the performance concerns above, if the app gets a lot of traffic and is rendering views server-side, won't our bandwidth usage go way up? We'll be deploying on AWS, which is semi-unfamiliar territory for me, but I'm presuming we'll pay according to how much bandwidth we consume. Pushing view rendering duties onto client browsers seems like a cost-effective strategy for a cloud-hosted app such as ours. Am I mistaken in that thinking?</p>
</blockquote>

<p>If delivering <em>gzipped</em> content eats your <em>bandwith</em> and so your <em>budget</em>, <strong>you are clearly in the wrong business</strong>.</p>

<blockquote>
  <p>Are there enough compatibility solutions in modern JS frameworks to resolve this concern? Or is server-side rendering the best-bet, in light of the likelihood of running on outdated browsers?</p>
</blockquote>

<p>Answering that would lead too much off topic. But let me mention a nice architecture style which embraces the fact, that not all of your users are up to date or have turned JS off; it is a new take on progressive enhancement: <a href=""http://roca-style.org/"" rel=""noreferrer"">ROCA - http://roca-style.org/</a></p>
"
315931,"<blockquote>
  <p>The JSTL syntax required to make highly-interactive pages via JSP's is getting awfully unwieldy. I'm worried that, when we expand our project and bring on more engineers, there will be a steep-ish initial learning curve, followed by a persistent slow-down in the dev process due to verbosity of JSTL.</p>
</blockquote>

<p>That is a legitimate concern.</p>

<blockquote>
  <p>Using JSP's also seems like a less contemporary approach to front-end dev, in light of how many JS frameworks are popular. I'm imagining it'd be tougher to field a team of front-end or full-stack engineers if we build with JSP's.</p>
</blockquote>

<p>If you choose technologies that are suitable for your particular task in areas like ease of use, maintainability, sensible management of complexity, flexibility, and appropriateness for your application's specific functional and non-functional requirements, you will find people who know how to develop and maintain software using them.</p>

<blockquote>
  <p>Server-side rendering has been great for my testing, so far -- but what happens when our app is a massive, historic, global success and downright phenomenon?</p>
</blockquote>

<p>If, and when, that happens, it will be a good problem, because then you'll have the money to fix it.  Every large company has had to do this; they built their product on a platform that got them to market quickly, and re-built it (in some cases, from the ground-up) when the number of users became enormous.</p>

<p>That said, I think you can make some sensible choices early on.  <em>Unless your system requirements demand large, overblown architectures (they don't), avoiding them and choosing more nimble and flexible technologies will generally give you better overall performance and better adaptability.</em></p>

<blockquote>
  <p>Pushing view rendering duties onto client browsers seems like a cost-effective strategy for a cloud-hosted app such as ours. Am I mistaken in that thinking?</p>
</blockquote>

<p>No.  However, have a look at <a href=""https://signalvnoise.com/posts/3112-how-basecamp-next-got-to-be-so-damn-fast-without-using-much-client-side-ui"" rel=""noreferrer"">this Basecamp article</a>.</p>

<blockquote>
  <p>Our app will be marketed to clients who are unfamiliar with technology. I think it's reasonable to assume that such clients will be disproportionately likely to have out-of-date browsers. Old browsers may have compatibility issues with JavaScript rendering -- and ever worse issues with the most modern JS frameworks.</p>
</blockquote>

<p>There's just no excuse for this anymore.  If your clients want to use computers in the 21st century, <em>they need a modern browser,</em> and it's easier than it's ever been to get one and allow it to maintain and update itself automatically and indefinitely.</p>

<blockquote>
  <p>After doing some research on these issues, I've got four strategies in mind...</p>
</blockquote>

<p>The way of the future is so-called microservices and client-side UI such as Angular.  And no, I don't think this just a fad.  Users demand high interactivity with their software applications, and this arrangement can give it to them.  Your back-end JSON web services can be built with anything you like; Node.JS, ASP.NET Web API, whatever.  Maintaining this sort of modularity will make it possible to change out one component without affecting the others.</p>
"
315927,"<p>Firstly, a note on the app I'm about to discuss: It's quite large, on the order of magnitude of a service app like Airbnb -- i.e., it's not just a static web page, it is a full web application. It's in the early stages of development.</p>
<p>Here's the question:</p>
<p>I'm about to begin a rigorous dive into front-end dev and want to make sure I'm setting myself down the right track. The back-end is built with Spring MVC design, and the limited front-end work I've done so far uses Twitter Bootstrap in JSP's and some simple JavaScript. I haven't had any issues per se with that stack, but I have the following concerns:</p>
<h3>Ease and/or Standardization of Development</h3>
<p>The JSTL syntax required to make highly-interactive pages via JSP's is getting awfully unwieldy. I'm worried that, when we expand our project and bring on more engineers, there will be a steep-ish initial learning curve, followed by a persistent slow-down in the dev process due to verbosity of JSTL.</p>
<p>Using JSP's also seems like a less contemporary approach to front-end dev, in light of how many JS frameworks are popular. I'm imagining it'd be tougher to field a team of front-end or full-stack engineers if we build with JSP's.</p>
<p>Should I not worry so much about these issues? Are there pros of server-side rendering with JSP's that outweigh dev concerns? Can you offer a sense of how wide-spread familiarity with JSP dev is?</p>
<h3>Performance</h3>
<p>Server-side rendering has been great for my testing, so far -- but what happens when our app is a massive, historic, global success and downright phenomenon? Will the server get bogged down with all the rendering, if done in JSP's? Or are traffic concerns better addressed by load-balancing requests to the server and leaving the server-side rendering in place?</p>
<h3>Maintenance Costs for Server-Side Rendering from a Cloud-hosted Server</h3>
<p>Along the lines of the performance concerns above, if the app gets a lot of traffic and is rendering views server-side, won't our bandwidth usage go way up? We'll be deploying on AWS, which is semi-unfamiliar territory for me, but I'm presuming we'll pay according to how much bandwidth we consume. Pushing view rendering duties onto client browsers seems like a cost-effective strategy for a cloud-hosted app such as ours. Am I mistaken in that thinking?</p>
<h3>Cross Browser Compatibility</h3>
<p>Our app will be marketed to clients who are unfamiliar with technology. I think it's reasonable to assume that such clients will be disproportionately likely to have out-of-date browsers. Old browsers may have compatibility issues with JavaScript rendering -- and ever worse issues with the most modern JS frameworks.</p>
<p>Are there enough compatibility solutions in modern JS frameworks to resolve this concern? Or is server-side rendering the best-bet, in light of the likelihood of running on outdated browsers?</p>
<hr />
<p>After doing some research on these issues, I've got four strategies in mind, for going forward:</p>
<ol>
<li><p>Avoid client-side rendering. Learn to love the JSTL chunkiness, stay with all JSP's.</p>
</li>
<li><p>Mark up all static HTML elements in the JSP's, then populate dynamically generated values/properties/elements according to user interaction (lots of JS, AJAX).</p>
</li>
<li><p>Make a minimal HTML markup in the JSP's, then populate everything in the view with appends to the DOM when the page loads.</p>
</li>
<li><p>Completely migrate away from JSP's, use something like AngularJs.</p>
</li>
</ol>
<p>Basically, all options represent varying degrees of moving away from server-side rendering and towards client-side rendering. Which of these sounds like the best course of action? Is there an option I've overlooked?</p>
<p>It's still early enough in the front-end dev process to integrate completely novel techs, and I'm new enough to web dev that any of the strategies will involve considerable self-teaching, so nothing should be considered off the table. I'm leaning towards solution strategy #2, so far, as this model would be the least drastic change from the current approach. But, in the interest of building in a way that'll make future builds easier, and be the most accessible for collaboration, #4 is also looking good.</p>
"
315163,"<p>I see 3 very important reasons for using CDNs: </p>

<ol>
<li><p>Reduce network latency for users in distant regions. When you have a web site for global audience, your hosting in North America would not seem fast for users in South Asia, especially, in China. The difference in user experience may be so huge that it will discourage users from using your site. In this case multi-regional CDN will be essential for your business.</p></li>
<li><p>Serve as much as possible from highly available resource. Reliability is one of the primary reasons for using cloud CDNs - the traffic is automatically rerouted to the available nodes and you can add some extra measures to reroute traffic if the whole cloud region is down. </p></li>
<li>CDN is easier to maintain and cheaper than application servers that serve dynamic content. When you have to deal with problems mentioned above, it's natural way to save the money. </li>
</ol>

<p>All this means, that the goal of CDN is to increase availability of your content to end users. If CDN fails or becomes slow for some reason, the client-side fallback will significantly increase page load, because it will first try to get the resource that is not accessible. Better solution is to have the server-side design that will make a substitution of base URL in links like ""{CDN}/js/jquery-version-min.js"". This will allow to re-route the traffic to application server instead of CDN if CDN health check fails - clients will not perform unnecessary requests and will go directly to app server, which would be your fallback with client-side solution. This will also solve problem of local and staging deployments, because you can implement the substitution logic to determine the location of static resources based on environment and configuration.</p>
"
452263,"<p>please note that while I refer to a specific web application framework in the following, the problem also arises with most other web application frameworks I know so please don't be afraid to reply even if you don't have experience with NestJS.</p>
<p>I have a web application with NestJS that runs on Azure Appservices (or AWS ECS).
There are at least 2 instances active at the same time.</p>
<p>The application has a public RESTful endpoint allowing people to POST orders via HTTPS.
These are then stored in a database table.
The new orders shall be exported to a CSV file and the CSV file once a day at a fixed time to to orders processing department.
The CSV must only be emailed at most once per day.</p>
<p>Currently this does not work because the export is initiated by a @Cron (from @nestjs/schedule library) decorated method in a service class within the application.
As there are multiple instances of the docker container, the export is initiated once per running instance instead of only once per day.</p>
<p>My initial idea was to create another REST endpoint that is called once per day by a AWS lambda or Azure Functions function with a cron trigger for the lambda function. This endpoint will then invoke the export functionality.</p>
<p>As the web application instances run behind the load balancer, the HTTP POST request to start the export procedure would only be forwarded to one instance.</p>
<p>The only downside I see is that the lambda could be a single point of failure. If the cron trigger for some reason does activate or the HTTP request is lost somehow, the export would not be done right away.
But if the HTTP request to trigger the export would fail or not be sent at all on one day, it could easily be resent by manually invoking the lambda function during the day.
Another &quot;downside&quot; of this
A very small additional cost for the lamdba function resource (0,000x cents?! if already above free tier limit)</p>
<p>What the other developers suggested was to use the @nestjs/bull module which makes use of redis to synchronize a queue across all connected instances.</p>
<p>This has multiple major disadvantages in my opinion:</p>
<ul>
<li>A redis cache needs to be served with a good enough availability -&gt; bigger additional infrastructure cost per month</li>
<li>More code needed for the wrapping business logic that sets up the queues at app startup (@OnModuleInit in NestJS) compared to a small lambda function (~10 LoC) combinend with an additional HTTP POST endpoint (at most 15 LoC in NestJS)</li>
<li>NestJS specific problems: I need THREE additional dependencies: @nestjs/bull, bull, ioredis (the latter to import Redis.RedisOptions interface in the app.module)</li>
<li>Default BullModule implementation lets app startup fail when redis is not available -&gt; additional single point of failure (could potentially be circumvented by a custom wrapper module but this requires more code again)</li>
<li>Migration to other web application framework would be harder because the new framework would need to provide an implementation to consume the jobs from the redis queue. Quite unlikely that there would be an existing component that can also process the cron jobs stored in @nestjs/bull's data structures within redis. An HTTP POST API on the other hand can be replicated very easily using most web application frameworks (Spring, NestJS, Django, Golang Frameworks, etc.)</li>
</ul>
<p>Downsides both solutions have:</p>
<ul>
<li>Additional terraform resource (lambda function or redis cache)</li>
<li>Increased overall system complexity</li>
</ul>
<p>Question 1:) What am I overlooking?</p>
<p>Question 2:)
What other advantages / disadvantages do you see in both approaches?</p>
<p>Question 3:) what other approaches are there with having even fewer disadvantages?</p>
"
450653,"<blockquote>
<p>where the data will be hosted by the software company</p>
</blockquote>
<p>The problem with that approach is that you end up effectively undoing that arms-length cloud hosting when you want to draw things together for bespoke reporting, because the data ends up back in your possession for you to manage.</p>
<p>The real perceived advantage of most cloud hosting from a client perspective, is avoiding the need to retain staff who understand a particular application enough to keep it in working order, or understand a database enough to develop with it.</p>
<p>For small businesses (i.e. a one- or two-man band) who have one system and can't afford any full-time technical staff, having the provider manage everything is a boon.</p>
<p>But once you start having multiple systems to look after and extracting the data for bespoke reporting, you'll find you need all those technical staff again, not just to perform the practical work of configuration or development, but to retain the knowledge necessary to do the work and to orchestrate things in your particular business.</p>
<p>And as well as undoing <em>part of what you're paying for</em> with a cloud model, the provider also might well have your pants down just for letting you extract your own data.</p>
<p>Indeed it seems many cloud platforms are still in the subsidisation stage, where companies are going easy on the licensing charges to establish the popularity of the model, after which licensing prices will soar.</p>
<p>It's something to think carefully about.</p>
<blockquote>
<p>I'm not sure if anyone is familiar with those applications, but I'm more interested in the prospect of the organization storing data in this way without an in-house DBA to manage everything.</p>
</blockquote>
<p>For a setup of any complexity, you'll likely need internal technical staff to manage and oversee the technical setup, ongoingly.</p>
<p>If your reporting from each core system is simple (and always will be) and there isn't any need for integration of the data, you might be able to do without a data warehouse, and pipe data directly from core systems to Power BI.</p>
<p>But in my experience, you also find that core systems usually already have a simple reporting capability built-in, and people only consider facilities like Power BI when they have serious bespoke reporting needs which are not &quot;simple&quot; in the relevant sense.</p>
<p>Instead the needs are usually &quot;complex&quot; in that you need (at least) a real development environment with programming languages (at least SQL), and proper scheduling facilities. You might also need to think about test environments, source code management, credential management, network and data security, and so on. All this adds up to needing skilled staff.</p>
"
447666,"<p>I recommend going with &quot;<em>Set up a two-hop architecture</em>&quot; option you provided. This approach adds an extra layer of security and control, as the costly machine learning API is not directly exposed to the client. You can implement rate-limiting, logging, or any other security measures at the cloud function level.</p>
<p>This architecture allows you to keep your database and machine learning API separate, enabling you to scale each component independently based on its own requirements. It also mitigates the risk of having to manage two separate authentication systems, as you mentioned with the &quot;<em>JWT and Fast API</em>&quot; option.</p>
<p>To address your concerns about pricing and provider lock-in, you can design your architecture in a way that allows you to easily switch providers. Use environment variables for configuration and keep provider-specific code isolated. Most cloud providers offer cost calculators to help you estimate expenses, and you can set up monitoring and alerts to keep track of usage and any unauthorized access attempts.</p>
<p>I think your approach offers a more secure and scalable solution that aligns well with best practices for API gatekeeping.</p>
"
440285,"<p>AWS Lambdas never made any sense.</p>
<p>Or perhaps I should say &quot;serverless applications&quot; never made any sense. Because it seems to me that's what you are doing, moving application code to lambdas for cost reasons.</p>
<p>As you note, actually the lambdas all run on instances that get billed by the hour, so unless you have long periods of nothing running at all you hardly save any money over always having a tiny instance up.</p>
<p>You also have the various complexities you outline, essentially you are forced into a nano-service++ architecture with complex orchestration problems.</p>
<p>Now you can in theory imagine an application where this architecture is ideal, lots of queues, events and mini functions all working together, but in practice its a bit like functional programming. Nice in theory, but difficult to apply to standard &quot;line of business&quot; applications.</p>
<p>In summary, only use Lambdas where you have a specific need for them. Don't use them because the billing model appears different.</p>
"
439256,"<p>I work in a software company with a single product, a 25 year old monolith application (1.2m lines of code) handling all HR related data (employments, salary, collective agreements etc.).</p>
<p>I've been put on a team that needs to research how we can modernize our application and so I've started reading up on different design patterns and architectural styles. The entire developer department, including me, have had this idea that a cloud native, microservices architecture is a no brainer, but the more I read about the style the more I seem to doubt that this style is actually compatible with our extremely complex and large business domain.</p>
<p>A couple of examples for better clarification:</p>
<p><strong>An employment is terminated</strong></p>
<ul>
<li>All work times and absence items after the termination date must be deleted.</li>
<li>According to the employees collective agreement, a last date of salary must be set.</li>
<li>The employees vacation, flex, time off in lieu account etc. must be paid out or used before this date</li>
<li>The employees logins our platforms must be revoked</li>
<li>Our integration partners must get the information that this employee is no longer with the company</li>
<li>etc...</li>
</ul>
<p><strong>A shift is created</strong></p>
<ul>
<li><p>If the shift is on top of a vacation period, the vacation account must be deducted.</p>
</li>
<li><p>if the shift exceeds the collective agreements max work hours for the given period(month, week etc.). The shift should not be created OR there should a created an overtime addition for said shift</p>
</li>
<li><p>+a million other events triggered in the dependencies</p>
</li>
</ul>
<p>These examples does not even start to cover the complexities of what happens when a collective agreement changes and the impact that can have on salary, accounts etc.</p>
<p>It seems to me that our business boundaries are too large to cover by small microservices without having an absolutely wild amount of redundancy, in addition we are handling peoples salaries. Every part of the CAP principles (consistency, availability, partition tolerance) are all of high importance.</p>
<p>What are your immediate thoughts on the matter?</p>
"
439240,"<blockquote>
<p>at some point the data must exist on some server somewhere in RAM. How is the data protected at that point? From attackers? From the cloud provider? From other customers?</p>
</blockquote>
<p>We are rapidly moving towards a world where data is encrypted in RAM; see for example GCP's <a href=""https://cloud.google.com/blog/products/identity-security/confidential-computing-data-encryption-during-processing"" rel=""nofollow noreferrer"">confidential computing initiative</a> which means that data in RAM is encrypted with a VM-specific key. It is still unencrypted in the CPU itself (including caches) but that removes a number of the attacks you are worried about.</p>
<p>As for your cloud provider, you to a large extent have to trust them. Again, the major vendors are making moves to reduce the amount of trust you have to give them - see e.g. <a href=""https://aws.amazon.com/blogs/security/confidential-computing-an-aws-perspective/"" rel=""nofollow noreferrer"">this post from AWS</a> - but that trust is still to some extent required.</p>
<blockquote>
<p>Is this even a viable attack vector to be concerned about? Why? Why not?</p>
</blockquote>
<p>Without a threat model, this is an impossible question to answer. If you are being attacked by a nation-state level actor, you're probably screwed no matter what you do.</p>
"
436432,"<p>I think you need to start by figuring out how the logging will be consumed.  One of the hallmarks of cloud deployments is scaling out capacity, so the concept of ephemeral instances of a microservice or application is important.  Even if you scale the app manually, you'll likely have multiple instances of the app running simultaneously.</p>
<p>With that in mind, you have the following constraints on how that logging would be useful:</p>
<ul>
<li><strong>Log aggregation</strong>: Having one place to view and filter your logs makes maintenance much easier.  The alternative is having to log in to multiple instances of your app to figure out the general health of the application is tedious and error prone.  Particularly if the instance was automatically replaced by the operations team.</li>
<li><strong>Ephemeral instances</strong>:  If an instance of your app can come and go based on demand, your logs will no longer exist.  That's where log shipping to your log aggregation server is critical</li>
<li><strong>Tracing Failures</strong>:  When you have ephemeral instances, or multiple instances of an app or microservice, you need the ability to reconstruct what happened after the fact.  That means you need some additional context.</li>
</ul>
<h2>Splitting Responsibility</h2>
<p>Systems like the Elastic Stack, Splunk, DataDog, etc. all have the concept of a log shipper which can provide all the necessary context to your log events so you can trace it to the container or server that the application is running on.  They also have the concept of centralized log aggregation.</p>
<p>That means that you can do the following:</p>
<ul>
<li>You can add any set of constants to the shipped log entries.  For example adding a &quot;role&quot; for a log file so you can use it in search terms later.  This seems to fit your intended use.</li>
<li>Your application only needs to worry about logging its behavior.</li>
</ul>
<p>As to whether you log to STDOUT or to a file, that really depends on how the application is run.  If you are running inside a kubernetes cluster, then K8s will make all STDOUT logs accessible through the container.  However if you are running on actual machine instances, those STDOUT logs will be lost.  Most logging libraries allow you to customize the output with configuration, so leverage that to make the decision on where the logs end up an operations concern.</p>
<h3>Logs are an operational concern</h3>
<p>You can easily create a lot of log traffic if you are not careful, and not all of those logs are useful all the time.  When you have control over log levels, it's useful to separate what is needed for local debugging vs. what is needed to understand day to day operations.  Log levels were intended for this purpose.  <strong>Caveat:</strong> if you are using 3rd party frameworks, you can't control how those tool vendors set up log levels.</p>
<p>When you do have control, it's useful to set your logs like this:</p>
<ul>
<li>DEBUG (and lower): should be reserved for software developers, as they are logging low level events to help debug the internal workings while running locally.</li>
<li>INFO: should be reserved for normal operational activity of your application</li>
<li>WARNING: should be used for when you detect conditions that may make your app/microservice unstable (nearing memory limits or disk space limits for exaample)</li>
<li>ERROR: should be used for error reporting that is not expected behavior.  Good example: file permission issues preventing the app from saving data.  Bad example: parse error that is a normal part of processing data and normally reported back to the user through an API</li>
<li>SEVERE: unrecoverable errors leading to shutdown</li>
</ul>
<p>Many logging providers have a variation on a theme here, but you can usually map the concepts above to the library's term for the same concept.</p>
<p>This allows the operations team to lower the volume of log traffic by eliminating the DEBUG and lower messages, and set up alerts on the WARNING and higher messaages.</p>
<h3>OpenTracing</h3>
<p>It's great that you have a session ID to track user behavior.  Including that information in your logs helps track things at a session level.  Going a step further than that, many cloud log systems have support for something called Application Performance Monitoring (APM).  And one of the most commonly supported standards to support that is <a href=""https://opentracing.io/"" rel=""nofollow noreferrer"">OpenTracing</a>.</p>
<p>This allows you to trace a specific request from start to finish, even across asynchronous communications.  There's work to be done to define where each context begins and ends, but OpenTracing provides a way to track child contexts with parent contexts.  That way the logging aggregator you use can visualize that for you in a dashboard, and you can drill in to the specific log messages that apply.</p>
<p>It's particularly useful for the following reasons:</p>
<ul>
<li>You can calculate the overall processing time of every action</li>
<li>You can detect outliers where the performance is way out of normal</li>
<li>You can drill in and find out what happened in the outlier</li>
</ul>
<h2>Conclusion</h2>
<p>Make sure you know how you are going to consume your logging data, and design for that use case.  Know where you can decorate your logs outside of your application so you can divide the responsibilities appropriately.  And think about what is necessary to support the application once it's deployed.</p>
<p>Your first step into the process may not be 100% correct, and that's OK.  If the app is successful, you'll have the opportunity to add in log messages that are needed, and remove or downgrade the level of log messages that are no longer useful.</p>
"
433568,"<p>As <a href=""https://softwareengineering.stackexchange.com/a/433551/327061"">Doc Brown said</a>, there's no generic solution for that!</p>
<p>I would still advise you revisit your limitations, as you need to consider the total cost of ownership, IMHO. A cloud database may end up cheaper in the long run than building and maintaining this system, especially considering fixing the bugs over time due to its complexity.</p>
<p>If your limitations are legitimate (such as an air-gapped computer for security purposes) then be prepared to deal with lots of locking and merging...! A process for a user exporting from system 1 <em>may</em> be:</p>
<ol>
<li>user selects &quot;parent&quot; rows to be exported, and starts the process</li>
<li>the system will mark those rows as &quot;exported&quot; or &quot;locked&quot;. From then on they can't be edited</li>
<li>the system exports the data (this is tricky, if you have a relational system, so you're going to have to look into CSV vs backup scripts vs ETL jobs); including generating checksums</li>
<li>the user loads these files to a flash drive and &quot;imports&quot; them into system 2</li>
<li>system 2 checks checksums and loads data back into a similar relational model</li>
<li>system 2 stores the &quot;source system ID&quot; for each row, which you <em>may</em> decide will be the PK on system 2, or you may decide to use a separate PK.</li>
<li>potentially if system 2 can create data of it's own, then use a nullable source system ID to identify this</li>
</ol>
<p>You would have to ensure that:</p>
<ul>
<li>since system 2 can edit the data from system 1, you can't import the same source ID twice, lest you overwrite your own changes</li>
<li>system 1 can't edit data that is exported</li>
<li>the user can't export rows that have already been marked as exported</li>
</ul>
<p>Now this is getting into the realms of data warehousing, so I would recommend you read up on that (I don't have a link, there's heaps on the web around that). Also read up on ETL (Extract, Transform and Load).</p>
<p>The only free ETL tool I found for MySQL is <a href=""https://www.benetl.net/"" rel=""nofollow noreferrer"">Benetl</a>. I have no idea how good it is, but if you read the documentation you can decide if that's easier to work into your process than doing it manually.</p>
"
433023,"<p>According to the 12-factor-app <a href=""https://12factor.net/dev-prod-parity"" rel=""nofollow noreferrer"">dev/prod parity</a> principle, the developers' local environments and production should be similar.</p>
<p>For developers working on a system with a complex microservice-based architecture, my understanding is that, this would mean that they need to build and deploy dozens of components to their local cluster (minikube for e.g.) -- even if they work only on a small part of the overall application. I see some problems with this approach:</p>
<ul>
<li>Developer machines are obviously not as powerful as the cloud staging/prod environments. Plus they need to be able to run development software (IDE etc) on top of the cluster (including data stores).</li>
<li>Time lag before the developers can start contributing. Though this can be somewhat managed with a sufficiently automated deploy process.</li>
<li>Time spent keeping the local cluster up-to-date constantly. While cloud environments can be kept up-to-date automatically through commit hooks etc, this has to be done manually for local environment.</li>
<li>In effect, doesn't this make it similar to working on a monolithic system where to touch one part of the application, you need to build and run the entire thing.</li>
</ul>
<p>Is there a better way to handle this? Or am I misinterpreting the principle?</p>
"
433012,"<blockquote>
<p>Is correct I say that microservices are split up according to their domains? (Some references about architecture say yes) but looks like it is not a thing we must do.</p>
</blockquote>
<p>There are no absolute right nor wrong ways to define these boundaries, however to get the most benefits, separation is often focused on technical design and coupling.</p>
<p>Ideally the boundaries would leave microservices with one or more entirely self-contained capabilities - i.e. services which give users the ability to 'do something' useful/meaningful with as few dependencies as possible, ideally none.</p>
<p>For example, an authentication capability is a user login which verifies their identity using their credentials.  Many systems rely upon stand-alone services (often from a cloud provider) for authentication because that capability on its own is useful; every other part of a system could fail while that service still allows users to log in.  Similarly, it can be deployed and scaled independently without affecting the rest of the system, even reused for other systems.</p>
<p>The more dependencies a service has on other services the tighter its coupling and therefore the less value its separation has for your architecture; any change or failure in any one of its dependencies can break some or all of that service's capabilities.</p>
<p>When two or more microservices are so tightly coupled and inter-dependent on each other, their value as separate services becomes highly questionable; at this point it might really be a <em>distributed monolith</em> with very little architectural value beyond a traditional monolith, yet suffering disadvantages in gaining extra complexity with deployment, configuration, infrastructure, project structure, communication...</p>
<p>With that in mind, try to focus on capabilities which can be self-contained as far as possible to decide the boundaries and separation between services, grouping closely-related and tightly inter-dependent functionality together.</p>
<p>Some capabilities and functionality may naturally have a lot of far-reaching dependencies; you might also consider putting this functionality in their own service(s) to avoid &quot;polluting&quot; the other services with unwanted dependencies.</p>
<blockquote>
<p>What approach we should use to make the communication between the projects? I've heard about direct API calls, use table storage. Does anyone have a different idea about it?</p>
</blockquote>
<p>This depends upon your communication needs and the manner in which functionality within a service is dependent upon other services.</p>
<p>No pattern can be a one-size-fits-all; obviously the best way to minimise complexity is to minimise the dependencies in the first place, and therefore minimise the number of communication paths.</p>
<p>Web API calls via HTTP are a Request/Response pattern, which is likely to be an easy and sensible default to start with.  It may be possible to use this pattern for most of your communication paths, which might minimise complexity.    Many other patterns exist and technologies which support those patterns.</p>
"
432796,"<p>This <em>would be</em> a broad question - but I'm asking specific to the situation around an app that I've developed.</p>
<p>In short, I've written an app that allows users to persist their photos and videos to the 'cloud'.</p>
<p>In an effort to keep this brief I won't go into too much detail but, essentially, any media captured through the app will be sync'd to Azure blob storage. Should they wish to access this again in the future I have a bit of .Net that will generate a sas token.</p>
<p>That's fine, from what I understand it's a typical implementation for secure storage of media.</p>
<h2>Question</h2>
<p>As the developer - I can jump onto the Azure portal at any time and view <em>any</em> media that I want.</p>
<p>Even if I were to take on other developers in the future, naturally there'd be serious restrictions over the team accessing data in the prod environment, but still - somebody has access.</p>
<p>This doesn't feel quite right, am I missing a security/privacy measure here?</p>
<p>I'm thinking about pushing this app soon, potentially marketing it but, at first, simply handing the APK to family and friends for wider use and testing of the beta - I'll have access to their most personal memories. Is it a matter of trust or is there some way I can implement a certificate or something somehow for this?</p>
<p><em>Note: Obviously only the original authors can download via sas token, I don't just generate the sas token for anybody authenticated/anonymous.</em></p>
"
432288,"<p>Hello we have an async event-driven system (kotlin, spring cloud stream, rabbitmq) where there might be an event <code>FooPayloadArrived</code>, published by an ingress rest-controller.
Processing this <code>FooPayloadArrived</code> and publishing the follow-up result in a subscribe may take some time, let's say 1h.</p>
<p>Being in AWS and using a managed RabbitMQ via the AWS MQ service, <a href=""https://www.rabbitmq.com/consumers.html#acknowledgement-timeout"" rel=""nofollow noreferrer"">we are constrained in the maximum execution of any such message</a>: After 30 minutes (AWS just decided on this new max and you cannot change this setting either....) the message/event currently being processed is basically aborted and requeued, aborting any long-running event-subscriber and restarting it (because message/event is requeued automatically) - so we are in an endless aborted-after-30-minutes loop.</p>
<p>We are currently very much in favour of the event-driven async mode of processing anything, since any failure puts your failed event into a dead-letter-queue (DLQ) which allows you to simply retry the operation after you fixed something. We don't want to abandon this resilience feature for long-running jobs.</p>
<p>Is there some alternative pattern how to design running those jobs vs. the enforced timebox?</p>
"
431205,"<p>There's two issues in your problem statement:</p>
<ol>
<li>A cache is a subset of the larger data to server files that are accessed more frequently.  <strong>You have to invalidate low hit rate cache entries</strong> or you will consume all your resources.</li>
<li>Azure Blob storage is designed to serve up your blob's bytes over HTTPS with high speed, reliability, and scalability.</li>
</ol>
<p>By copying your blobs into your database, you are simply duplicating data.  I highly doubt serving from the database is faster than having Azure blob storage serve the data for you.  While databases can store binary data, that's not really their main use, so it is rarely optimized as storage mediums designed from the ground up for that purpose.</p>
<p>I highly recommend that you examine the ways that you have designed your application to find where it is causing bottlenecks.  If you need to have your application as a proxy to the data, make sure of the following:</p>
<ul>
<li>Do not read the whole blob as an array of bytes--it wastes memory and can cause massive garbage collections</li>
<li>Make sure you stream the bytes from the blob storage stream to the response stream</li>
<li>Design around passing single use URLs (Azure storage can do this), and let the browser pull the data directly</li>
<li>Look at the differences in how you are handling binary data in the database to how you are handling it in blob storage.  Your algorithm may need to be optimized</li>
</ul>
<p>In short, make sure you understand what makes Azure storage &quot;slow&quot;.  If you are testing locally on your machine and your test database is local on your machine, you are artificially penalizing Azure storage.</p>
<p>Granted, I come from an AWS background, and AWS S3 is much faster than serving binaries from an RDS database--particularly at scale.  Microsoft is also a smart company, I'm sure the Azure storage is at least close to S3 in performance and scalability.</p>
"
426618,"<p>I am in the process of carrying out a software architecture for my client, in fact</p>
<p><strong>THE HISTORY OF THE REQUIREMENT :</strong></p>
<p>MY PARTNER SEND cvs files via MFT to MY CLIENT , many times a day .</p>
<p>MY CLIENT has a listener ( cron job in linux and batch ) , when a file is received in a folder , the client consumes it and store it in database ( SQL SERVER)</p>
<p>Regarding data , the estimated trafic :
700 files a day
Millions of lines in cvs file to be consumed
there is no order in data , each time a file is received it is processed by batch .</p>
<p><strong>The new solution is to transfer data via API REST CALL and not OLD MFT :</strong></p>
<p>The need is to retrieve huge data from a partner by calling API REST  (JSON) to finally store the result in our SQL database (not too much processing to do in between)</p>
<p>I proposed two solutions:</p>
<p><em><strong>SOLUTION 1 : Event Driven Architecture and PARTNER EXPOSE API REST TO RETRIEVE DATA :</strong></em></p>
<p>THERE ARE STEPS OF THIS SOLUTION :</p>
<p>OUR PARTNER : create an event in our BROKER QUEUE ( Message indicates that there is new data to get : name API RESSOURCE TO CALL  / parameters  / Bussines Domain )</p>
<p>MY CLIENT : GET the message and read it from the QUEUE</p>
<p>MY CLIENT : Based on this message , my client CALL the REST API ( GET exposed  by our PARTNER ) to GET THE DATA FROM JSON RESPONSE of the call HTTP ( this will be implemented for example by spring batch )</p>
<p>MY CLIENT : STORE The result of this huge response on SQL DATABASE without many modification</p>
<p><strong>SOLUTION 2 :  THE PARTNER DO NOT EXPOSE API , MY CLIENT WILL DO THIS !</strong></p>
<p>THERE ARE STEPS OF THISE SOLUTION :</p>
<p>MY CLIENT  ==&gt; EXPOSE A REST API to our PARTNER : THIS ENDPOINT IS a POST HTTP OPERATION</p>
<p>OUR PARTNER ==&gt; THIS time , our Partner will call THIS POST API REST AND send all data in REQUEST BODY , as a result , the partner update our SQL DATABASE when he calls our API REST ( CLIENT CALL THE POST API WHEN HE WANTS TO UPDATE DATA)</p>
<p><strong>MY Notes :</strong></p>
<p>THE PARTNER IS OPEN to ALL OUR SUGGESTIONS ( SOLUTION 1 or 2 ) and we estimates that it there will be huge data transfers between Partner / Client</p>
<p><strong>My requirements : evaluates the pros and cons of these solutions</strong></p>
<p>For my part, I opt for the SOLUTION 1 because  :</p>
<p>pos1 :  the processing will be asynchronous
pos2 : we will be the master of our data to be added to our database ( is it good as an argument ? )
pos3 : processing in almost real time
cons1 : architecture is complex when in we move to cloud
cons2 : Partner is not familiar perhaps with this solution</p>
<p>The second solution ( we expose API , the partner simply call it )
pos1:  is rather light (less expensive) it is an advantage,
cons1 :  but has the disadvantage that the processing will be done synchronously
cons2 : that we will not be master of the data to add to our REF (unless checks to be made in the POST action to be exposed)
cons3 : call of API will be an additional charge by our partner</p>
<p>Thank you for your feedbacks :  (advantages / disadvantages)
Regards</p>
"
425900,"<p>Some patterns only make sense at a scale. It often makes sense to separate analytical from transaction workloads, but not everything has to be cloud-scale. Many gigabyte-scale problems can be efficiently approached e.g. by loading data into a local database (such as Postgres or SQLite) and performing queries there. In contrast, cloud based approaches make a lot of sense when:</p>
<ul>
<li>the data is too much for a single computer</li>
<li>your data is too much to efficiently transfer, so you'll have to take the compute to the data (instead of bringing the data to the compute)</li>
<li>the data cannot be structured for efficient queries, so reasonable query performance relies on massive parallelization (e.g. with a Map-Reduce architecture, or search products like Amazon Athena).</li>
</ul>
<p>Don't get tricked into paying for an expensive cloud product that you simply don't need.</p>
"
425746,"<p>Change your application from a console app that takes parameters and is started per job to a service that runs constantly listens to a message queue.</p>
<p>Run instances in the cloud so that you are able to dynamically spin up new instances via the cloud providers API.</p>
<p>Your architecture then works as follows</p>
<ul>
<li>Users click button in UI specifying input parameters.</li>
<li>Message with the parameters is sent to the message queue</li>
<li>Worker applications listen to queue and pick up the new message</li>
<li>If worker crashes mid processing the MQ software itself will manage re-queuing the - - message so its picked up by another server.</li>
<li>On successful completion the worker posts a message to a second queue with the - results, which can then be saved to a db, email to the user, whatever</li>
</ul>
<p>To manage scaling, have a second app keep track of the number of messages in the queue and the number of avaialble workers. If jobs arent being done fast enough have it call the clouds API to spin up a new VM with your service installed. As the service runs continually this will immediately pick up a new job from the queue without having to send it parameters, change master directories etc</p>
<p>Essentially here you are using the out of the box Message Queue (RabbitMQ, ZeroMQ, Cloud offering of your choice) functionality to do the fiddly orchestration of your applications.</p>
"
422736,"<p>In AWS you can easily create a RESTful API using API Gateway. With a serverless approach, you would have API Gateway integrate with Lambda to handle your endpoint logic. This seems like the most obvious approach, as its very elastic and cost effective, however, looking into architectures for large companies, most of them use a form of EC2 to handle their API logic.</p>
<p>Here are some examples:</p>
<ul>
<li><a href=""https://www.youtube.com/watch?v=-8FK9p_lLy0"" rel=""nofollow noreferrer"">McDonald's</a> uses ECS instead of Lambda for their API.</li>
<li><a href=""https://www.youtube.com/watch?v=ZDUKRnLfW58"" rel=""nofollow noreferrer"">Under Armor</a>, uses an Application Load Balancer with EC2.</li>
</ul>
<p>What is the benefit of using EC2 for your API instead of Lambda?</p>
"
422123,"<p>Firstly, I want to state that this is my first time trying to build an app. I am doing this for fun, mostly as an exercise to learn system design/architecture. The goal is to build an app that would be scalable to millions of users (not that this app will actually have millions of users-- its just to build my own understanding of large scale distributed systems).</p>
<p><strong>What I am using:</strong>  AWS architecture tools - Lambda, DynamoDB, Amplify</p>
<p><strong>What I would like to build:</strong>
I downloaded and imported Movies from IMDB into DynamoDB. The goal is to allow the user to search for a movie and display its info from the DB. The user should also be able to rate the movie.</p>
<p><strong>What I am asking:</strong></p>
<ol>
<li>Which AWS tool should I use for searching the DynamoDB?</li>
<li>Alternatively, should I be using a different database to search from? If I understand correctly, AWS ElasticSearch is actually a database. Should I just be using this instead?</li>
</ol>
<p>Please also let me know if you have any general advice, or if I am thinking about all of this incorrectly.</p>
"
421082,"<h2>Introduction</h2>
<p>A customer of ours has embedded products with sensors and actuators. Now they would like to connect this device to the cloud so they can remotely monitor and configure it. It should support:</p>
<ol>
<li>Periodic data updates (LwM2M read or notify? Depends on how we implement it)</li>
<li>Alerts (e.g. data above threshold) (LwM2M notifies)</li>
<li>Configuration updates (LwM2M writes)</li>
<li>LwM2M Execute triggers</li>
<li>Optionally Cloud-to-Device data requests (that gets the most recent reading)</li>
</ol>
<p>They want us to provide them with a simple module to accomplish this.</p>
<p>Our company is already using LwM2M quite extensively. More specifically, we have devices running Zephyr RTOS that use the built-in LwM2M engine. This engine requires to know the layout of the OMA/IPSO objects (which resources, what are their properties, etc.). Also, we register data pointers to the resources, and register read callbacks so that a read from the cloud triggers an update of those data before responding. Lastly, we also register write callbacks for e.g. configuration settings, so that they can trigger the required actions.</p>
<p>The LwM2M object .c files use an Observer pattern to observe the sensors/status/data in the appropriate other software modules, and the write callbacks call &quot;SetConfig&quot; type of functions directly at the target modules. This has led to quite tight coupling.</p>
<h2>Now I'm investigating how to best integrate this functionality in an easy-to-use and generic module</h2>
<p>I think the first step is to get rid of the tight coupling I described above, and thus maybe move all the Observers to a single module that handles all communication between the LwM2M objects/engine and the rest of the system? (Mediator / Facade pattern?)</p>
<p>I made this analysis of what that module would need to support the wished functionalities:</p>
<ol>
<li><strong>Periodic data updates</strong> -&gt; An <strong>update interval</strong> must be set per resource. Alternatively, the LwM2M server could set observe attributes <em>pmin</em>, <em>pmax</em> to trigger periodic notifies and thus periodic reads, these reads would then trigger <strong>read callbacks</strong> that update the info before responding to the server.</li>
<li><strong>Data threshold alerts</strong> -&gt; <strong>Alert trigger</strong> that trigger a notify to the server.</li>
<li><strong>Configuration updates</strong> -&gt; <strong>Write callbacks</strong> that take appropriate action (e.g. configure interrupt threshold).</li>
<li><strong>LwM2M Execute triggers</strong> -&gt; <strong>Execute callback</strong> that takes appropriate action (e.g. reboot device).</li>
<li><strong>Optionally Cloud-triggered data updates</strong> -&gt; <strong>Read callbacks</strong> that update the info before responding to the server.</li>
</ol>
<h2>What I find difficult</h2>
<ol>
<li>As you can see, functionality 2 and 5 are sort of opposite (push vs.
pull), but functionality 1 can be implemented in 1 of 2 ways, either push or pull. This is part of what I find
challenging, there seems no generic solution for all 3?</li>
<li>If we would choose server-triggered periodic updates (via the observe attributes), I see two problems: If the customer requires that no periodic sensor-data is lost on network failure, this can't work. Also, I think all resources in an object with different settings would require separate observes?</li>
<li>The other thing I find challenging is the module interface design.
From the above ideas, it would require aliases for each resource to abstract away the LwM2M internals object/resource structure, and the following data per alias:
<ul>
<li>Update interval (if the device is the one with the update schedule)</li>
<li>Alert trigger</li>
<li>Read callback</li>
<li>Write callback</li>
<li>Execute callback</li>
</ul>
</li>
<li>If going with the idea from challenge 3: This is quite an extensive list, and if the embedded firmware
changes at the LwM2M module boundary, or if the LwM2M cloud API
changes, this would require our interference, as we would have to
update the internal resource mapping.</li>
</ol>
<h2>I'm very curious for your opinions, remarks and ideas on this! I would very much appreciate your advice.</h2>
"
420877,"<p>I work for an organization that heavily leverages AWS.  There is a strong push that every team move from containers deployed on ECS to leverage AWS Lambda and step functions for (almost) every project.  I know that there are workflows for which lambdas are the best solution, for example if you are running infrequent, short duration processes or processing S3 uploads for example.  However I feel like my project isn't a great use case for them because:</p>
<ol>
<li><p>We have many calls to a database and I don't want to have to worry about having to re-establish connections because the container a lambda was running in isn't available anymore.</p>
</li>
<li><p>We have many independent flows which would require too many lambdas to manage efficiently.  With each new lambda you create you have to maintain an independent deployment pipeline and all the bureaucratic processes and items that go with owning a deploy-able component.  By limiting the number of these the team can focus on delivering value vs maintenance.</p>
</li>
<li><p>We run a service that needs to be available 24/7 with Transactions Per Second around 10 to 30 around the clock. The runtime for each invocation is generally under 10 seconds with total transactions for a day in the 10's of thousands.</p>
</li>
</ol>
<p>Also generally, I'm not bought into the serverless ecosystem because of a few pain points:</p>
<ol>
<li><p>Local development. I know the tooling for developing AWS Lambdas on a developer machine has gotten much better, but having to start all these different lambdas locally with a step function to test an application locally seems like a huge hassle.  I think it makes much more sense to have a single Java Spring Boot application with a click of a button you can test end to end and debug if necessary.</p>
</li>
<li><p>Reduced Isolation.  If you have two ECS clusters and one is experiencing a huge throughput spike, the other ECS cluster will not be impacted because they are independent.  Not so for lambda.  We've seen that if other lambdas are using all the excess provisioned concurrency and we have to go over our reserved concurrency limit, then we are out of luck and we'll be rate limited heavily leading to errors. I know this should be a niche scenario, but why risk this at all?  I think the fact that lambdas are not independent is one of things I like least about this ecosystem.</p>
</li>
</ol>
<p>Am I thinking about lambdas/ serverless wrong?  I am surrounded by developers who think that Java and Spring are dead and virtually every project must be built as a go/python lambda going forward.</p>
<p>@Mods if there are any ways that I can make this question more appropriate for the software engineering stack exchange community or rephrase it, I'm happy to make changes here as well.</p>
<p>Here's some links to research I've done so far on the topic:</p>
<ol>
<li><a href=""https://stackoverflow.com/questions/52275235/fargate-vs-lambda-when-to-use-which"">https://stackoverflow.com/questions/52275235/fargate-vs-lambda-when-to-use-which</a></li>
<li><a href=""https://clouductivity.com/amazon-web-services/aws-lambda-vs-ecs/"" rel=""nofollow noreferrer"">https://clouductivity.com/amazon-web-services/aws-lambda-vs-ecs/</a></li>
<li><a href=""https://www.youtube.com/watch?v=-L6g9J9_zB8"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=-L6g9J9_zB8</a></li>
</ol>
"
420772,"<p><strong>Yes</strong> - a cloud/lambda function is a microservice.</p>
<p><strong>Independently deployable</strong></p>
<p>see <a href=""https://microservices.io/"" rel=""nofollow noreferrer"">https://microservices.io/</a></p>
<blockquote>
<p>Independently deployable</p>
</blockquote>
<p><a href=""https://martinfowler.com/articles/microservices.html"" rel=""nofollow noreferrer"">https://martinfowler.com/articles/microservices.html</a></p>
<blockquote>
<p>to describe a particular way of designing software applications as suites of independently deployable services</p>
</blockquote>
<p>Microservices are &quot;mostly&quot; about the ability to deploy a service individually (or in a small grouping). This is the only objective aspect about microservices.</p>
<p>In fact, people often contrast a Microservice with a Monolithic &quot;Project&quot; that is deployed with all of the defined services at the same time.</p>
<p>Other subjective attributes:</p>
<blockquote>
<p>Highly maintainable and testable, Loosely coupled, Organized around business capabilities, Owned by a small team</p>
</blockquote>
<p><strong>Yes, but there are more</strong></p>
<p>GCP Cloud Function / AWS Lambda are not the only infrastructure. There are other providers, and you can also use Kubernetes and deploy each of your services as a separate task. Often people are imagining an &quot;elastic&quot; provider. One where they deploy and the infrastructure takes care of the scaling.</p>
<p>However, it is possible to view PHP files as microservices, where they run on PHP-FPM, and served via Nginx. You can create PHP files that are too big, the same that you can deploy a Cloud Function to GCP that is too big. The fact that Microservice Architecture includes good old PHP, should be a good indicator that most people are thinking of Cloud Functions and Lamda when they say &quot;microservice&quot; - they are thinking of that &quot;infrastructure&quot;.</p>
<p><strong>Bonus: Infrastructure Not &quot;Architecture&quot;</strong></p>
<p>Microservice architecture is not really an architecture. The idea of breaking up services into small &quot;loosely coupled&quot; units has a long history in Computer Science. The only unique aspect for Microservices is how they are deployed. In total they are:</p>
<ul>
<li>DevOps to deploy to elastic (per-service) infrastructure</li>
<li>The elastic infrastructure itself.</li>
</ul>
<p>In fact, services can be contained in a single monolithic code project, run together in a programmer's development environment, and then deployed separate in production. The Deployment (DevOps) and Destination (Infrastructure) are the full extent of Microservices.</p>
"
420711,"<p>Consider unwinding the idempotent transaction to the pre-buy state or a &quot;may try again&quot; state on error. This leaves you with two or three terminal states to handle (in total).</p>
<p>By doing this you can also safely reuse the id. Reusing the id preserves the duplicate prevention benefits of the idempotent approach.</p>
<p>You may discover that reliably unwinding the transaction is very hard. If so, it may be time to rethink the design and/or use of idempotency.</p>
<p><em>Addition #1 (based on comments):</em>
Consider starting with a database update of e.g. PAYMENT_STARTING or whatever. include a timestamp. If this fails, you're at &quot;sorry we're offline right now, please try again later.&quot;</p>
<p>Next, call the gateway itself. If this fails you're at &quot;something went wrong, your credit card cannot be charged, please try again later.&quot;</p>
<p>Finally, update the database to e.g. PAYMENT_COMPLETE. If this fails you are responsible for retrying based on e.g. the timestamp from the first update. The ideas in other answers about how to handle the user experience are valid.</p>
<p>However you chose to handle the user experience, the goal should be a solution that converges on one of two states: order completed or fully unwound.</p>
<p><em>Addition #2 (based on comments):</em>
It seems that an underlying question is how to handle unreliable calls e.g. in a public cloud. One common approach is a retry library e.g. <a href=""https://docs.microsoft.com/en-us/dotnet/architecture/microservices/implement-resilient-applications/implement-http-call-retries-exponential-backoff-polly"" rel=""nofollow noreferrer"">Polly</a>.</p>
"
418620,"<p>Git just does not scale to huge projects. Repos should not generally grow beyond 5GB including the entire history if you want a good user experience (or if you want to use commercial hosting services). Thus, binary assets such as images or even videos should typically be managed separately from the Git repository. Git LFS is just a way to automate management of assets through another storage mechanism, but in principle you could also manage assets “by hand”, e.g. by writing a script to fetch them from a network share that can snapshot versions with ZFS¹.</p>
<p><sup> 1: ZFS and Btrfs are advanced file systems that support features such as block-level deduplication and atomic snapshots. This is roughly similar to Git except that they can deal with arbitrarily large amounts of data and that there is no branching, though copies are super cheap due to block-level copy-on-write. </sup></p>
<p>What Microsoft did was deciding that it cost less to pour thousands of engineer-hours into hacking around Git's restrictions instead of getting developers to use a proprietary version control system. This doesn't mean that Git now has good support for huge repositories by default. Instead, Microsoft's solution hinges on Git VFS, a virtual file system that allows Git to work more efficiently. You too could use this VFS, but I doubt it will help much with huge files. Microsoft's use case is more concerned with monorepos where each individual only needs a small subset of files so that a full checkout is not physically necessary.</p>
<p>Since you're somewhat new to version control, it probably doesn't make too much sense to try to bend Git to your will. You <em>should</em> use Git and Git is the most popular version control system, but that doesn't mean it is the best possible version control system that can theoretically exist – many aspects of it are objectively crappy, and large repo support is one of them. To a large degree, these problems are inherent in the design and cannot be directly fixed.</p>
<p>I would suggest that starting to use Git only for source code files will already be a significant step up for your version control setup. In contrast, assets and large files should be managed with more traditional methods, such as LFS if the assets are fairly static. Instead of version control, you might prefer more traditional/linear backup. Even rotating through a few external hard drives is a great approach already, though a NAS or storage server can also be cost-effective compared to more convenient online or cloud backup services when considering the cost of a restore.</p>
"
418163,"<blockquote>
<p>I have a monolithic application which can be divided into multiple steps and different steps have variable scaling requirement</p>
</blockquote>
<p>This has nothing to do with a state machine.  This is a pipeline.</p>
<p>You probably should break it into microservices so you can scale per logical component.  Why do you need a state machine?  Do calls affect subsequent calls?  Generally back-end calls are designed to NOT change state of other backend calls.  If there is only a little state per call, then just pass it along the microservice pipeline as parameters.</p>
<p>If you really need a truly global state, a microservice is probably the wrong answer.</p>
<p><strong>EDIT in response to comments</strong></p>
<p>In the comments, you say step 2 acts on step 1 (and step 3 acts on step 2).  This is a pipeline.  If package A goes into step 1 -&gt; Spits out Package A1, package A1 is fed into step 2, becomes package A2.  If this is the flow, then I don't see why you need a state machine.  You need to dynamically scale &quot;step containers&quot;.</p>
<blockquote>
<p>The only concern which I feel is that if Service1 container instance and service2 container instances are in different servers/hosts, then there would be some network latency which would be multiplied by the no. of requests.</p>
</blockquote>
<p>If you're using Google Cloud all the physical machines will likely be in one very well connected data center.  In my experience, latency inside the data center has never been an issue with either Google Cloud or AWS.</p>
<blockquote>
<p>So based upon my understanding the microservices architecture is not a good candidate for state machines, if we are looking for real time performance.</p>
</blockquote>
<p>I don't understand the fixation on state machines.  Why do you need one.  In a cloud environment, a state machine is a way of sharing global state between instances.  The way the question reads now I don't think you need one.</p>
<blockquote>
<p>resources which can be used by those services i.e. allocating more resources to service which needs them more and then we can auto scale the whole server based upon load.</p>
</blockquote>
<p>So you're essentially redoing Google Cloud's work for them.  Google Cloud offers something called a <a href=""https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline"" rel=""nofollow noreferrer"">pipeline</a>, which sounds like exactly what you need.  The point of both Google Cloud and AWS is to move away from &quot;I bought the box&quot; to &quot;I bought computing power&quot;.  Google Cloud will auto-scale the pipeline, bringing more instances of each microservice online as it's needed, and deleting it when load tapers off.  Don't redo this - you're probably not as good at this as Google or Amazon.  Let them handle it - that's what you're paying for!</p>
"
414686,"<p>Device Management is quite hard to achieve so we've seen the rise of many SaaS, from big providers and small startup, which automate the deploy of edge services onto IoT devices fleet. Some of them rely on containerization (and Docker is pushing towards a top level support on ARM archs) some other else act in a &quot;serverless fashion&quot; which means that let you load some script in some language and then copy it through your fleet</p>
<p>Basically what you can do is</p>
<ol>
<li>Evaluate these tools (eg. Azure IoT Edge)</li>
<li>Work With some configuration management tool (eg. Saltstack)</li>
<li>Roll you own solution</li>
</ol>
<h1>Evaluate Edge Computing Tools</h1>
<p>It's clear that this is the safest choice since you have to do nothing but some benchmark and then integrate your pipeline. But as with all cloud services, they come with their costs and their constraint</p>
<h1>Work With some configuration management tool</h1>
<p>Yes, I'm not crazy. We know config management tools (Ansible, Terraform etc) since we use them to provision hundreds of cloud VMs, but there is not so much difference between a cloud VM and a linux device accessible through SSH and VPN. You just have to make sure that you are using a tool that is scalable enough and has the needed resiliency to work over unreliable channels. Saltstack dose this job quite good, it uses ZeroMQ as event bus and have small footprint agents. You can define your desired state through versioned files and change them accordingly to requirements, or take control of some devices for some specific maintenance tasks. Pay attention on managing all Ops aspects (security, scalability, availability) that are the major burden that this solution carries to you project</p>
<h1>Roll your own solution</h1>
<p>If you have a very simple use case, you wouldn't be eager to pay cloud bills or to manage large scale configuration application for High Avaliability and so on.... You are able to communicate with your devices in a bidirectional way, you could write some platform service able to send events to the edge whenever a config update is available. Then the edge send back some tracking event to understand whether you should retry on some unavaliable device, rollback the deployment or perform some deployment strategy such as canary. But this worth only with the simplest scenario, because building a full fledged management solution takes a huge effort and distract your team for the real valuable activities</p>
"
412144,"<p>I have a front end (WEB GUI) app that I designed (Python for now + JavaScript in the future) that I use to access a controller, it uses REST APIs.</p>
<p>I want to publish this app in the cloud so that others could use it.</p>
<p>The biggest issue I am seeing is the security side as the app needs to authenticate with the remote server (a controller itself) and start sending tasks to the controller that will translate that in internal REST APIs to control for processes on downstream servers</p>
<p>Is there an authentication flow that will guarantee the owners of the controllers that I (the publisher of the front end) do not intercept the authentication flow and I gain unwanted access to their servers ?</p>
<p>My idea is to use a two steps authentication/authorization process like below. Is there a better way?
Please edit <a href=""https://dreampuf.github.io/GraphvizOnline/#digraph%20G%20%7B%0A%20%20node%20%5Bmargin%3D0%20fontcolor%3Dblue%20fontsize%3D8%20width%3D0.5%5D%3B%0A%20%20edge%20%5Bmargin%3D0%20fontcolor%3Dblue%20fontsize%3D8%20%5D%3B%20%20%0A%0A%20%20DesktopClient%20-%3E%20GUICloud%20%5B%20label%20%3D%20%22Step1%3A%20authentication%22%20%5D%3B%0A%20%20DesktopClient%20-%3E%20OnPremController%20%5B%20label%20%3D%20%22Step2%3A%20authorization%5Cn%20the%20user%20manually%5Cn%20allows%20the%20session%22%20%5D%3B%0A%20%20GUICloud%20-%3EOnPremController%20%5B%20label%20%3D%20%22cookie%22%20%5D%3B%0A%20%20OnPremController%20-%3E%20Server1%3B%0A%20%20OnPremController%20-%3E%20Server2%3B%0A%20%20OnPremController%20-%3E%20Server3%3B%0A%20%20GUICloud%20-%3EOAuth%3B%0A%20%20OAuth%20-%3E%20GUICloud%20%5B%20label%20%3D%20%22cookie%22%20%5D%3B%0A%0A%20%20OnPremController%20%5Bshape%3Ddoublecircle%5D%3B%0A%20%20GUICloud%20%5Bshape%3Dsquare%5D%3B%0A%7D"" rel=""nofollow noreferrer"">this diagram</a> if you have suggestions<br />
<a href=""https://i.sstatic.net/yBHYa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yBHYa.png"" alt=""enter image description here"" /></a></p>
<p>After looking closer at the issue I think this is a better architecture</p>
<p><a href=""https://i.sstatic.net/tnCrh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tnCrh.png"" alt=""enter image description here"" /></a></p>
"
411788,"<blockquote>
<p>Is this something to really worry about?</p>
</blockquote>
<p>This is very dependent on the product. A lot of the time, someone doing it will &quot;cost&quot; you $30 a month—who cares if four or five (or most likely zero!) people do it? You can monitor the situation over time and make changes if necessary. It's a bit like profiling code; engineers make notoriously bad estimates of good and bad bits.</p>
<p>Also, think rationally. If you are &quot;angry&quot; at people who do it, put that aside. Etc.</p>
<h1>HOWEVER!!</h1>
<blockquote>
<p>they could just comment out that bit of code and they get access to the entire app again</p>
</blockquote>
<p>If this is a problem, there is a good chance your users could do <strong>more serious things you haven't thought of,</strong> like impersonating other users, messing with their profiles, buying things with their money.</p>
<blockquote>
<p>If so should I be using something like Firebase Cloud Functions?</p>
</blockquote>
<p>Yes, &quot;something like&quot; that. For 95% of people asking this question, the problem is pretty much eliminated if you perform authentication and authorisation and sensitive functionality on the server/cloud rather than the client (and follow best practices correctly.) You don't necessarily need Firebase Functions if you can set up Firebase security rules to do the job. It depends on your application.</p>
<p>However in some cases code really needs to run on the client (e.g. in games or proprietary number-crunching algorithms) or it is too slow. In these cases, obfuscation is where to put your attention. But nobody has mentioned <em>anti-debugging techniques.</em> Malware authors use these to shut down the program if it suspects it is being run in a debugger or VM. This makes reverse-engineering even more time-consuming.</p>
<blockquote>
<p>Should I be creating a proper backend? If so what would its structure be, would my app just be a client for the backend?</p>
</blockquote>
<p>Backends tend to implement <em>behaviour</em> and your client can sometimes access functionality partly through the backend and partly not. If you have complex rules like users managing other users or teams, loyalty points and so on, that goes on the backend. It is madness to try to securely authorise that sort of thing on the client.</p>
<p>Otherwise, it's a matter of taste as to how much functionality to put on the server. On the one hand it creates an extra layer to implement and maintain. On the other hand, you can update backend code &quot;in one go&quot;, so if you want to add new features or fixes, you don't need to worry about rollouts and conflicting versions of your client app everywhere. Doing intensive things on the backend is good for client battery life (at the expense of server $). So on.</p>
"
411735,"<p>The answer is (as usual) that is depends.  If this data is going to be used for reporting and especially ad-hoc reporting, you probably want to pull the variables out into a proper relational model.  Another reason might be if you need to expose that data in multiple formats.  If you don't need to do any of that then the option of keeping the documents as-is becomes more appealing.</p>
<p>As amon notes, you can store JSON in blobs in a relational DB.  The reason you might want to use NoSQL is scalability, speed, and potentially cost.  If you already have a relational DB and you don't have a large amount of documents to store (e.g. millions) it might not be worth adding an additional DB.  A lot depends on the context.  If you are running on a cloud provider, spinning up a NoSQL DB for this might be less effort than getting the DB setup to hold the blobs.</p>
"
404970,"<h1>Problem Background</h1>

<p>Recently, I joined a government agency as a software engineer/scientist/analyst. Previously, worked in software industry - gained 3 years of software engineering experience at previous job (to add to about 7 years in computational science/scientific computing). My current job is to come up with a strategy for modernizing a legacy scientific program.</p>

<p>The scientific program to modernize is a large legacy computational system that basically does mathematical optimization. Development started in the 1990s and has not kept up with best practices, unfortunately. It was/is written by scientists and analysts.</p>

<p>The main component of the system is a Fortran-based (various versions starting from 90, some newer versions incorporated, compiling with 2018 compiler) program that does the optimization. The program consists of 400K lines of Fortran code, 20K lines of shell scripts, and 60K lines of external math solver code. There is no test suite, hence the legacy label. The program can be thought of as a dozen modules that describe a particular physical component's behavior in the optimization. The general flow of the Fortran program is described in a <code>main</code> routine, where these dozen modules are called sequentially. The <code>main</code> routine does some other data orchestration and I/O as well. There is some interface to commercial products and optimization solvers, probably through a home grown Fortran wrapper. One of the biggest issues IMO is the use of global variables - both <code>main</code> and the modules have access to these globals, so change to the state can be made from anywhere (see <a href=""https://softwareengineering.stackexchange.com/questions/405059/global-variables-and-common-block-management-in-fortran"">my specific question</a>).</p>

<p>There is a lot of home grown code for sub-systems or utilities that manage the main Fortran program, written mainly as shell scripts. These sub-systems include:</p>

<ul>
<li>a queuing system that manages the executions of the main Fortran program on internal prem Windows servers, </li>
<li>post-processor that converts the Fortran UNF files to CSV and Excel format,</li>
<li>custom visualization package written in Visual Basic that plots the results of the Fortran program,</li>
<li>version control utilities as wrappers around RCS VCS, </li>
<li>compiler utility that wraps the Fortran compilation.</li>
</ul>

<p>Those are the main sub-systems or utilities necessary to work with the Fortran
program and its input/output, but there are loads of other Fortran programs and
shell scripts that do longer-term things like server space management and license management.</p>

<p>My immediate team is responsible for the Fortran code execution and integration
with other modules (so not all 400K lines of Fortran is in our scope, just maybe 10-20%, the rest is with other groups responsible for the dozen modules, which introduces some organizational pains since we have no control over their code). My team consists of me and another software developer, both mid-level software developers converted from scientific computing. A junior software developer with a traditional background in software and CS is joining shortly. Our senior software developer (one of the original developers of the entire system) is retiring in 1 month, and we are in the process of trying to find a replacement.</p>

<h1>Problem</h1>

<p>My question is: <strong>What are the components and sequence of the modernization plan/strategy that I should consider?</strong> The modernization is basically the process of moving from legacy to a more modern process, both technically (e.g., architecture, frameworks) and organizationally (e.g., agile process management for development).</p>

<h1>Proposed strategy</h1>

<p>Currently, at a high level, my plan is to:</p>

<ol>
<li>assess extent of home grown code for systems that are not part of the main Fortran program;</li>
<li>replace each of these home grown solutions with best practice open source
solution, so we maintain as little code as possible;

<ul>
<li>current order is modern VCS (Git/Gitlab), then queuing system, then viz package, but order will be determined by how much code there is per sub-system.</li>
</ul></li>
<li>with the remainder of the code - hopefully just the main Fortran program and not some vital sub-system that we cannot find an open source solution for - capture current behavior with characterization tests;</li>
<li>refactor (update Fortran, port all functionality that doesn't do number crunching from Fortran to Python, etc.), make sure tests pass, repeat;</li>
<li>""futurize"" code by updating architecture to enable cloud compute (to avoid vendor lock in), using Docker for containerization.</li>
</ol>

<h1>Research</h1>

<p>I've looked at some great discussion of similar topics:</p>

<ul>
<li><a href=""https://softwareengineering.stackexchange.com/questions/155488/ive-inherited-200k-lines-of-spaghetti-code-what-now"">I&#39;ve inherited 200K lines of spaghetti code -- what now?</a></li>
<li><a href=""https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/"" rel=""nofollow noreferrer"">https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/</a></li>
<li><a href=""https://softwareengineering.stackexchange.com/questions/362848/how-to-deal-with-a-large-codebase-with-no-requirements-and-the-responsible-perso?noredirect=1&amp;lq=1"">How to deal with a large codebase with no requirements and the responsible person leaving the company soon</a></li>
<li><a href=""https://softwareengineering.stackexchange.com/questions/114542/how-can-a-large-fortran-based-number-crunching-codebase-be-modernized"">How can a large, Fortran-based number crunching codebase be modernized?</a></li>
<li><a href=""https://softwareengineering.stackexchange.com/questions/122014/what-are-the-key-points-of-working-effectively-with-legacy-code"">What are the key points of Working Effectively with Legacy Code?</a></li>
</ul>

<p>But notice that some of these questions and answers are almost 10 years old,
so I wonder if there are better approaches available. Also, I am dealing with
a procedural scientific computing environment, rather than a heavy OOP business app, so perhaps the principles mentioned in the above Stackexchange links don't carry over as well. I am also not a senior software engineer, so not sure if I am even using the right terms in search and question formulation. There is the complication of scripts and utilities in the system that makes this effort not just about porting or refactoring Fortran, that makes this situation and problem unique.</p>

<p>Thanks!</p>
"
401720,"<p>In AWS I've got lots (dozens) of lambda functions. As my organization has gained experience with AWS, we've gone through various generations of infrastrucure around building and maintaining our functions. Currently we're making use of the SAM template and Code Pipline created from a CodeStar project to keep track of things. Most of our lambda functions are small and have their own git repos.</p>

<p>When we have configuration that's different between our stage and prod environments, we make use of environment variables pulled in from the <code>template-configuration.json</code> and <code>template.yml</code> provided by CodeStar. For sensitive configuration, we make use of Secrets Manager.</p>

<p>This has been working well for us. If I need to update a piece of sensitive configuration that's being used by ten different lambda functions, I can just update it once in Secrets Manager. What I'm noticing though, is that for configuration that's not sensitive, if it's something that's used by ten different functions, I've got to go update the value in ten different <code>template-configuration.json</code> files, and I have to try and make sure not to miss one!</p>

<p>Surely there's a better way.</p>

<p>I'm aware of Parameter Store in AWS Systems Manager, but I haven't used it before. Is this my best option? Store non-sensitive environment configuration in Parameter Store, and store sensitive environment configuration in Secrets Manager? Is the performance of Parameter Store similar or better to Secrets Manager?</p>

<p>Is there another pattern or AWS Service or something else I should be considering?</p>
"
399960,"<p><strong><em>No, however there are frameworks available to enable easily migrating between cloud providers.</em></strong></p>

<p>The basis for my answer is found within the definitions provided below.</p>

<p>Per <a href=""https://en.m.wikipedia.org/wiki/Function_as_a_service"" rel=""nofollow noreferrer"">Wikipedia</a>,</p>

<blockquote>
  <p>a category of cloud computing services that provides a platform allowing customers to develop, run, and manage application functionalities without the complexity of building and maintaining the infrastructure typically associated with developing and launching an app.</p>
</blockquote>

<p>Likewise, <a href=""https://en.m.wikipedia.org/wiki/Least-cost_routing"" rel=""nofollow noreferrer"">Least-cost routing (LCR)</a> is defined as follows:</p>

<blockquote>
  <p>the process of selecting the path of outbound communications traffic based on cost.</p>
</blockquote>

<p>Public cloud companies provide integrations and functionality that is not easily portable to other providers. In practice, most companies end up being locked in to certain providers due to these architectural differences. Therefore, unless developing separate codebases that perform the same task, (which would be redundant), there is no way to leverage the cost savings found on another provider without accouting for these changes.</p>

<p>While recommendations are off-topic, I can provide two arbitrary examples of frameworks that provide the portability required to enable rapid transitions between various cloud services.</p>

<ul>
<li><p><a href=""https://knative.dev"" rel=""nofollow noreferrer"">Knative</a> is a kubernetes-based solutions for running ""serverless"" code.</p></li>
<li><p><a href=""https://serverless.com"" rel=""nofollow noreferrer"">Serverless</a> is a framework for deploying functions as a service.</p></li>
</ul>
"
394623,"<p>I'm planning to construct a workflow/environment for training and serving NLP classifiers, that follows something like:</p>

<ol>
<li>The model training system takes in annotated documents from a subset of a variety of preconfigured sources, along with a set of user-defined parameters on how to run the model (e.g. which n-gram features to generate, whether to apply negation/lemmatization, etc)</li>
<li>Model training system outputs a model file to an S3 bucket</li>
<li>A Flask-based API service loads the model from S3 on startup, and uses it to provide real-time prediction</li>
</ol>

<p>However, there are a few caveats:</p>

<ul>
<li>The training workflow will feed into multiple stand-alone services, not just one</li>
<li>Each service might have multiple models attached to it (so an incoming POST of a document would receive a response with multiple classifications, based on multiple different model predictions)</li>
<li>The calls per minute per service would be relatively low (maybe one call to a service every few minutes)</li>
</ul>

<hr>

<p>I've looked into existing offerings like SageMaker, but that's limited to one API service per model. It's also seemingly designed for API services that would <a href=""https://stackoverflow.com/questions/49422065/aws-sagemaker-hosting-multiple-models-on-the-same-machine-ml-compute-instance"">receive thousands of calls per second</a>, which is not at all cost-effective for my needs.</p>

<hr>

<p>As such, here's my plan:</p>

<p><strong>Pre-/Post-processing Package</strong>. Have a code repo containing all the pre- or post-processing methods that might be called by the classifier pipeline (both in training and in prediction). These methods all include a high amount of logic variance, dictated by input params. This code is, on its own, not deployed anywhere.</p>

<p><strong>Training service</strong>. A high-resource EC2 instance which imports the above pre-/post-processing package, has input connectors to all possible data sources, and outputs to an S3 bucket. Data scientists would input a set of params and data source(s) and run the training on this instance.</p>

<p><strong>Model storage</strong>. Outputted models are stored in various S3 buckets, based on some organizational structure related to the data sources and classifier type.</p>

<p><strong>API services</strong>. A series of low-resource Flask-based API services which use config files to dictate which models to load from the S3 bucket. This <em>also</em> would need to import the pre-/post-processing package, so it could apply said methods during prediction of incoming docs.</p>

<hr>

<p>So, my questions:</p>

<p><strong>Does this general architecture make sense? Or are there sections I should rethink?</strong></p>

<p><strong>Are there existing cost-effective systems I should be looking into which would handle this better than constructing the entire ecosystem myself?</strong></p>
"
391040,"<p>Microservices have technical and social aspects:</p>

<ul>
<li>each microservice can be <em>developed</em> and <em>deployed</em> independently</li>
<li>each microservice can <em>scale</em> independently</li>
</ul>

<p>For each of these aspects – development, deployment, scaling – it took some time until microservices became a sensible option.</p>

<p>&nbsp;</p>

<p>Most problems then and now are simply not so big that scaling would be a concern. And long ago, the cheapest way to scale was not to build distributed systems but to build bigger machines. What has changed since then: now we are solving many more problems with computers, in particular internet-related stuff, and quite a few problems have performance requirements that cannot be satisfied by a single blade server. Bigger machines are not an option beyond some point due to physical limits. So distributed systems it is.</p>

<p>As a sub-point of this, distributed systems can be attractive because they are potentially more highly-available. This doesn't imply microservices as a monolithic architecture can be made HA as well, but microservices are a popular way to build highly available systems.</p>

<p>Microservices aren't necessary for a system to scale, but it allows different parts of a system to scale <em>independently</em>. For example, some web app might need a different number of frontend servers, database replicas, and worker nodes for optimally cost-effective performance.</p>

<p>&nbsp;</p>

<p>Software development used to be release-oriented: requirements go in on one side of the process, developers do their thing, and some time later a release emerges on the other side. Such a view causes difficulties if requirements are uncertain and need to be tested by putting them into practice, if there is ongoing maintenance without clear releases, or if the project is simply very big so that it's ineffective and too risky to release all components together.</p>

<p>The dot-com bubble shifted priorities towards minimizing time to market. “Rapid Application Development” was a big thing. Agile techniques emphasized iteration and early deployment. One of the most influential agile techniques were things like automated unit testing, continuous integration, and (much later) continuous deployment. DevOps builds upon these agile ideas and suggests a strong focus on making deployments automated.</p>

<p>All of this is relevant for a service architecture because services can be modified and deployed independently so that changes reach production faster. That involves a lot more total deployments than for a monolithic architecture, so that automated build and deployment systems are a prerequisite. Independent development also means that different services can be built with different technology stacks, as long as the services can communicate via some common interface.</p>

<p>Containers are not necessary for microservices, but containers simplify system administration and container images make automated deployment a lot easier. Similarly, “the cloud” is not necessary for microservices. But a microservice system does need a cluster of machines to run on, and cloud platforms are an easy low-capex way to get that cluster.</p>

<p>&nbsp;</p>

<p>Microservices represent the current best practice for developing large systems that need independent development, deployment, and scaling. This is based on the currently known technologies and development techniques.</p>

<p>However, microservices are not the first occurrence of service-based architectures. Before that, service-oriented architectures (SOA) were already popular, especially in an enterprise context. SOA and microservices have overlapping goals and techniques, but SOA tends to be about integration of different services over a common message bus/queue. Microservices are an evolution of SOA, but may favour more fine-grained services and don't necessarily have a centralized message queue.</p>

<p>Microservices will also not be the last occurrence of these ideas. Now that the microservice hype is dying down they are becoming an ordinary tool in the system architect's toolbox, but our technologies and techniques continue to evolve. Interesting directions include scaling <em>down</em> with FaaS, and moving computations to the data with edge computing.</p>
"
390552,"<p>I am considering what it takes to implement an email server. <a href=""https://cloud.google.com/compute/docs/tutorials/sending-mail/"" rel=""nofollow noreferrer"">Google Cloud</a> basically doesn't allow you to send emails at scale (they block the email ports pretty much), though it sounds like you could receive email. <a href=""https://aws.amazon.com/blogs/messaging-and-targeting/amazon-ses-now-offers-dedicated-ip-addresses/"" rel=""nofollow noreferrer"">AWS</a> on the other hand allows you to send emails for about $1 per 10k. <a href=""https://www.codeinwp.com/blog/best-smtp-providers/"" rel=""nofollow noreferrer"">This</a> sums up some other SMTP services like SendGrid, and the costs involved.</p>

<p>I am aware (vaguely) that there are lots of problems Internet Service Providers (ISPs) want to prevent, like email <a href=""https://help.campaignmonitor.com/how-why-isps-block-emails"" rel=""nofollow noreferrer"">spam</a>. It sounds like they have IP blacklists, and somehow intercept the emails and can figure out if they are spam by checking their content. Somehow also they get access to abandoned email accounts and check who is emailing there (I have no idea how this works, but if there are some helpful links I'd love to know, though not relevant for the question). Basically, the ISP uses all kinds of techniques to figure out if your email service is spammy, so they can block your IP and shut it off. I don't see why this needs to happen at the ISP level, but that's beside the point.</p>

<p>What I'm wondering about is how to architect an email server so as to not get black listed, and to have it ""work"" 24/7, for years and years without interruption. Say I want to implement a service like Gmail or SendGrid. I'm wondering what measures you should take to architect an email server. That is, what the best practices are architecture-wise to create a successful email server.</p>

<p>Specifically where I'm at currently is, it seems using Amazon SES is the best option. It is the cheapest by far and doesn't have any bells and whistles. Otherwise you would have to buy your own hardware and build your own cloud if you wanted to get any cheaper or lower level I'd imagine, and buy your own IP addresses. But short of that, using AWS SES sounds like a good option.</p>

<p>They give you the ability to use <a href=""https://aws.amazon.com/blogs/messaging-and-targeting/amazon-ses-now-offers-dedicated-ip-addresses/"" rel=""nofollow noreferrer"">dedicated IP addresses</a>, and as they state:</p>

<blockquote>
  <p>most email certification programs require you to have dedicated IPs because they demonstrate your commitment to managing your email reputation.</p>
</blockquote>

<p>So email server architecture principle 1, have dedicated IP addresses. But I don't want to do this just yet and then get blacklisted for an unknown reason, which brings me to the crux of the question. <em>How not to get blacklisted</em>. Given this is a service like Gmail or SendGrid, which could be sending millions of marketing emails and millions of personal emails, from millions of different email accounts, <em>every day</em>. I don't see how to tell if I am putting the right things in place for the email server to be top quality and to potentially be ""certified"" (not sure what email server certification really is or if it's a thing, Google search doesn't reveal anything, but AWS mentions it). That is, what the high level <em>things</em> are that you should put in place to <strong>guarantee</strong> that all emails will always get delivered (or all emails from all ""good"" email accounts on your system get delivered). If it's not possible to <em>guarantee</em> this, then I'd like to know why not, and the answer could just be tailored to whatever is closest to a guarantee that we can get.</p>

<p>Basically, the architectural measures to put in place for an email server to consistently deliver email without being blocked.</p>

<p>I am not (for this question) considering anything about scaling the email server or building the email server itself, just the architectural best practices to prevent being blacklisted.</p>

<p>From my understanding so far, some of the initial principles are:</p>

<ol>
<li>Have a dedicated IP address. (Not sure if you should just have one, or if you can have 2 or 3, or 100).</li>
<li>Don't send spam.</li>
</ol>

<p>That's all I can think of. For (2), this means you have to have good spam filters in place, and other security measures such as verifying that there is a person behind the email account, etc. But for (2) as well, I am unsure how to handle the problem of false negatives. That is, some users might send 100+ individuals a day, maybe even a few mass marketing emails like on those ""get rich with adwords"" marketing sites with email lists in the 10's of thousands. I would like to know if purely the <em>volume</em> of emails causes a red flag, and how to get around that. Then the content, just want to make sure this is purely based on in-house spam filters, and that the ISP wouldn't block that kind of stuff.</p>

<p>If this is a broad topic, I would like to keep it narrowly focused. I imagine one part of this is to learn more about <a href=""https://rads.stackoverflow.com/amzn/click/com/1908422009"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">email spam prevention</a>, which I will do. So this question doesn't need to cover the spam stuff in any detail. To keep it narrowly focused, I'm wondering what architectural measures should be put in place not to be blacklisted. This might include (just making this up):</p>

<ul>
<li>Have a fixed number of dedicated IP addresses, less than x number.</li>
<li>Contact some ISP providers and tell them manually (on the phone even) about your business goals.</li>
<li>Implement spam filters to prevent spam going out in the first place.</li>
<li>If you have geographically distributed email servers, perhaps something there as well.</li>
<li>Programmatically send the abandoned accounts or closed accounts to the ISP for checking.</li>
<li>Give access of ISP to some other stuff perhaps, by manually creating an API integration and partnership or something.</li>
<li>Associate phone numbers with the accounts.</li>
<li>etc.</li>
</ul>

<p>I can understand how to <em>implement</em> an email/SMTP server, and send/receive messages at scale. So architecturally that makes sense. What's missing in the picture is the architectural components to prevent being blacklisted at this sort of scale.</p>

<p>To put it succinctly, I'd be interested to know how Gmail and SendGrid avoid getting blacklisted, but that's probably proprietary :)</p>
"
384565,"<p>No. This is definitely not agile. Nor is it a good idea.</p>

<p>Cross-functional teams, i.e. teams that include every role (analyst, server admin, database admin, UX designer, QA tester, technical writer, graphic designer) that is needed to successfully deliver working software, are a staple of many agile methodologies. In fact, in many methodologies, the customer itself is also considered to be part of the team.</p>

<p>Actually, this is orthogonal to agile, though. Cross-functional teams are simply a good idea, <em>regardless</em> of whether you are doing agile or not.</p>

<p>What <em>is</em> true, however, is that with the constant rise of automated testing, developer testing, test-driven and behavior-driven development on the one hand, and software-defined infrastructure, highly automated commissioning, configuration, and deployment, DevOps, and cloud hosting, some of the workloads may have shifted from admins to DevOps engineers, and from QA to development. That does <em>not</em>, however, mean that those roles are extinct. It just means that QA has more interesting bugs to chase because all the trivial ones have been found by developer testing, and admins focus more on enabling DevOps to manage the infrastructure with automated tools than managing it themselves.</p>

<p>There is an easy test to check whether something is agile: when someone says ""you do this because it is agile"", then it is not agile. Agile is all about self-managing teams that constantly reflect on their processes and adapt them. Whenever someone says ""you do this"", then it is not agile. It is only agile when <em>the team</em> says ""<em>we</em> do this, because after reflecting on our past experiences, we have determined that it works, and we will keep reflecting on it and stop doing it as soon as we determine it doesn't.""</p>
"
378569,"<p>Goal: Prevent unauthorized 'clone' apps from using REST API-based solutions <strong>where the customers manage their own servers and services instead of the vendor doing so</strong> (database, resource, and identity). In other words: on-premise servers instead of a cloud-based solution. </p>

<p>My company wants to be able to authorize only some third-parties to be able to create apps that can use our APIs, but not just anybody, in addition to our own apps. So we will likely have some process where third parties will submit their apps to us for signing and adding to some kind of 'white-list' (whatever form that ends up taking.. hence the question.)</p>

<p>(Why no cloud? Some countries and industries are subject to stricter regulations where cloud or off-site servers is not an option for some classes of sensitive operational data. If that is not good enough of a reason, then please consider this a thought experiment). </p>

<p>This question is a variation on <a href=""https://softwareengineering.stackexchange.com/questions/219028/how-to-safeguard-a-rest-api-for-only-trusted-mobile-applications"">this near identical question</a>, but with the additional constraint that the software vendor doesn't own or manage the servers.</p>

<p>I have zero affiliation with CodeBlue, the makers of Approov, but they did publish a decent explanation of the typical process for securing REST APIs for both web apps and mobile apps. Here is a link to part 2 of 3 which covers some of the critical points such as the mediation server and the attestation process for authorizing the client and techniques for keeping shared secrets out of public clients (i.e.: mobile apps).</p>

<p><a href=""https://www.approov.io/blog/mobile-api-security-techniques-part-2.html"" rel=""nofollow noreferrer"">https://www.approov.io/blog/mobile-api-security-techniques-part-2.html</a></p>

<p>The solution described by the above article from CodeBlue sounds dandy for a cloud-based or vendor-controlled environment. However, I have been struggling to find a way to protect against clone apps when the customer has physical access to not only the clients, but to the servers. Configuration alone could probably enable unauthorized apps, and if that doesn't work the app itself could be modified.</p>

<p>Of course there is a legal safety net here (license agreement, terms of service), but is there anything code-wise that can be done to harden against this, if not prevent it outright?</p>

<p>I am aware of - and pursuing - various techniques for obfuscating solutions (like spreading the secrets around, and protecting against reverse engineering) and for client attestation, but in all of my models so far there exists the possibility that a malicious customer (or agent) can trick the REST API into believing that requests are made by an authorized client. I know I'm in some kind of arms race here - so I'm reaching out to the community for thoughts on this because all of the methods I have researched over the past week rely on the vendor owning the server.</p>
"
378239,"<p>A goal of a service in SOA is to perform some logic which, otherwise, would have to be duplicated among other services. That logic is, in turn, the reason for the service to exist.</p>

<p>In your case, you add no value through the service. Any other service could easily access S3 directly, given that S3 <em>is</em> itself a service (it is not <em>your</em> service, but that is irrelevant here).</p>

<p>Not only there is no added value to the intermediary service, but its cost is rather prohibitive. You may imagine that you can draft one in a few hours, which is true. However, in order to have something that works reliably and handles all the edge cases, you have to deal with caching, redundancy, interrupted uploads and recovery, large files, timeouts, brute force and DDOS, special characters in parameters, permissions and IAM, and dozens of other pretty things.</p>

<p>Therefore, you're about to spend two or three weeks of development and possibly one week of debugging (YMMV; I don't know your experience in HTTP, SOA and the technology you want to use to create your service, but even if you're highly experienced, you shouldn't expect spending less than two weeks on that, which is <em>a lot</em>).</p>

<blockquote>
  <p>I just wanted to know how it's done in commercial/big projects.</p>
</blockquote>

<p>Companies pay on average $700/day for a highly experienced developer, so given my estimate of two weeks, you're at $7,000 for a task. Commercial projects are about money; when there is an opportunity to save an amount even as small as $7,000, they'll do it.</p>

<p>Additional to this direct cost, there is a cost in terms of hosting and code maintenance. This service will have to be maintained for years, adding to the bill much more than the original price. Again, all this money wasted without the clear understanding of what could such service <em>save</em> to the company. It doesn't save bandwidth, nor other resources. It doesn't reduce the amount one will pay to Amazon, so...</p>

<p>This is not all. The cost of maintenance of all the projects which depend on the intermediary service will also increase. If a service:</p>

<ul>
<li>Has to be patched and the patch requires an interface change,</li>
<li>Has to be moved to another location, with a change in its URL,</li>
<li>Is down, requiring to have a circuit breaker to ensure the client service doesn't go down in turn,</li>
<li>Is deprecated, requiring to migrate the client to something else,</li>
</ul>

<p>the immediate and unforeseen maintenance is required and is usually costly as well. Now, it is much more likely that those four things happen to <em>your</em> service than to Amazon's S3. Not because <em>you</em> are a bad developer, no. But because Amazon's scale is slightly different than the scale of your service, which means that they have much more workforce to pay to ensure the clients can rely on S3.</p>

<p>Finally, many developers have prior experience with Amazon AWS (and possibly S3). This means that when you hire a new guy, he can easily understand how a service is storing files if it uses S3 directly. If you add a level of indirection, this benefit is lost. And, more importantly, every time someone has a problem with the storage, he would need to ask himself if the problem comes from the client service, from your intermediary or from S3. This adds to the debugging time.</p>

<p>So:</p>

<blockquote>
  <p>Is it a clean/good-design-patterns-compliant approach ?</p>
</blockquote>

<p>No. Add services when they add value. Don't make things more complex than they need to be (KISS) and, specifically, don't add layers which bring no benefits.</p>

<blockquote>
  <p>What is good/bad about that approach?</p>
</blockquote>

<p>What is good: the fact that you provide an interface which is much simpler compared to S3. For developers unfamiliar with AWS, S3 can be quite complex.</p>

<p>What is bad: already told it above.</p>

<blockquote>
  <p>How could I do that in a better way ? (don't want to store files (content) on the machine where application is running [locally] )</p>
</blockquote>

<p>By calling directly S3.</p>
"
376805,"<p>Never ever commit secrets to source control. And by extension, never build a container image that includes secrets. Instead, secrets should be provided during deployment, e.g. as environment variables. If you are using some cloud provider, they probably have special tooling to manage keys and other secrets.</p>

<p>If you need to store secrets during development, perhaps use a config file but never commit these files. Instead, explicitly .gitignore them.</p>

<p>Now that you aren't version-controlling any secrets, your repository is already in a publishable state, and complying with the AGPL is easy. The (A)GPL does not force you to make any passwords, API keys, or other secrets publicly available, except under the very limited circumstances that you are embedding (A)GPLv3 software into an embedded device, and such secrets are necessary for installing modified versions on that device.</p>

<p>Recommended reading: <a href=""https://12factor.net/"" rel=""noreferrer"">The twelve-factor app</a>, a short guide to web app architecture and deployment. Especially consider their definition of <a href=""https://12factor.net/config"" rel=""noreferrer"">configuration</a> and their <a href=""https://12factor.net/build-release-run"" rel=""noreferrer"">distinction between build, release, and run phases</a>.</p>
"
373055,"<p>This might be a partial answer but I think there's some info that might be useful in it, so...</p>

<p>First, I'll try to answer the questions:</p>

<ol>
<li>""<strong>Would you all recommend sticking with the above plan or if you suggest doing something different, what would it be?</strong>""</li>
</ol>

<p>I would actually try to convince the stakeholders (money payers) that using an enterprise software would be in their best interest 'cause you're describing a very hard problem and you'll likely spend more money developing it than something like New Relic costs.</p>

<p>That said, the implementation totally changes based on what you're currently doing for releases and if you control the application code you're trying to monitor or if it's some 3rd party plugin you want to get data about too.  I'm going to guess that the app is your company's and that your might have <em>some</em> CICD but it's not 100%.</p>

<p>If that's close enough, I think it would be easy enough to stand up a Kafka cluster or three and log everything you can get your hands on into them.  Maybe one is for RESTFUL calls and one is for app-level errors or whatever you decide.  This could stand as your centralized logging system.</p>

<p>Since you control the application code, you can log pretty much anything you like by asking the developers to add more logging, where you want.  That's not usually a favorite request but if it makes sense and especially if higher ups back the idea, you can probably get all the logging you want to get.  That will tell you about RESTful calls, app errors and things like that but you'll have to go deeper to get latency reporting and info on slow SQL queries and all that.  I believe you can get some good info out of some free Nagios logging too.  At a minimum, you should be able to get stats about how the hosts are doing; e.g. Memory available, CPU consumed, Disk space, etc.</p>

<p>You can then have something consuming all these messages and tie in some pretty useful automations, based on the state of your system and the actions coming in.  ""Oh look, everyone is buying all the X-product!  Let's make a promotion to capitolize on it!"" or ""Oh look, all of our apps are throwing 500s!  Let's go hide somewhere until this all blows over!""</p>

<p>I'd say you should use something like AWS's SQS because you don't have to worry about managing the Kafka/Zookeeper cluster <em>on top of</em> managing your application(s) code.  SQS is pennies for millions of messages but if you're going 100% open source, Kafka is a solid choice, in my humble opinion.</p>

<p>Choosing NoSQL...that one's tough here.  I think it's probably the right choice but maybe, if you can, hold off on deciding about that level in your system until you find out how you want to use that data, in other words, make sure the use case fits NoSQL before you commit.  Switching and keeping any history would be tricky.</p>

<ol start=""2"">
<li>""<strong>What I lack knowledge in is ""BigData"". What exactly is BigData in a nutshell? Can we apply it for our needs? Are there open source solutions for BigData?</strong>""</li>
</ol>

<p>I'm no PHD but I dabble and I work with those teams, daily.  With that history, I'll try to explain in my words.  Big Data is a blanket term you can use to talk about problems and solutions with, when the problem you're trying to solve has a grip of data tied to it by nature.  It's usually not helpful to work from this mentality when your data is still in the GB range but I'm sure there's plenty of use cases where it makes sense.</p>

<p>Anyways, it works like this; Say there's this thing that we all do, like use the internet.  Pretend, if you will, that there was some way to collect data about our internet usage, like what sites we visit and what search term we used to get to that site.  That's a looooot of data and when you think of it and use it as kind of a resource, you get a lot of insights and benefits out of it but it's also really hard to sift through all that data and reason about it.  In this example, the data is all our internet usage.  The problem is how to sale crap to us, using google adwords or whatever, and the ""Big Data"" in this is that we're considering all this data and how we can use it.  A big part of ""Big Data"" is also the infrastructure used to run it.  It's not trivial so it usually gets a lot of attention.</p>

<p>There's lots of open source tools you can use for using your data.
If your apps were to use any tools, Spark comes to mind.  You could set up streaming from your logging message system to modify all the incoming data so that it's easier to use for humans and machines.  I guess if you get really ambitious, you could try to implement some machine learning based on all your data.  You'll have to come up with the problems you want to solve first, though.</p>

<ol start=""3"">
<li>""<strong>Would using BigData or any other solution help with faster analysis/querying of data for our analytics dashboard?</strong>""</li>
</ol>

<p>Yes.  That's the whole point of it all.  The problem is, you have to hire a bunch of expensive people to implement it.  Your best bet is to tackle that hurdle when you get to it.  If you can create your logging system and the dashboard, you'll be worlds better than you are now.  No need to build an F1 car when you just need a tractor, ya know?  And if I were to continue the F1 car and tractor metaphor, you can think of it like this -> The tractor that you're going to build now is creating the paved surface for your BigData F1 car to do impressive things with.  You'll need all that data your logging system is going to collect before you can use the data to reason about, if that makes sense.</p>

<ol start=""4"">
<li>""<strong>Is there an out of the box open source solution for dashboards(UI) so that we don't have to build it from scratch?</strong>""</li>
</ol>

<p>I don't know of one but that doesn't mean it doesn't exist.  Building the dashboard can be as big or small a project as you make it.  Honestly though, I think this is the easiest part of the work ahead of you.  Enjoy, if you build it yourself!  It should be fun.</p>

<ol start=""5"">
<li>""<strong>If sticking with NoSQL is a good plan, what NoSQL DB would you recommend? MongoDB? Cassandra?</strong>""</li>
</ol>

<p>My opinion here is just to use a cloud tool and let them decide the engine.  I doubt, from what I'm hearing, that you'll be taxing any graph database on tough queries.  Your issue is most likely going to be from reads and writes, so maybe figure out how much you think you'll be pushing into the DB and how you'll be querying that data.  It should lead you to whether or not NoSQL is a better fit and during your reading, I bet you'll stumble on some article that makes up your mind on which engine you chose.</p>

<p>It's an interesting problem and it sounds really fun to work on.  </p>

<p>Good luck and I hope that helps, at least a little!</p>
"
363962,"<p>My question is in context with the Serverless Architecture (e.g. AWS Lambda) and how does one interact with the Databases in this system.</p>

<p>Typically in a 3 Tier architecture, we have a web service which interacts with the Database. The idea here is to ensure that one database table is owned by one component. So changes in there, does not require changes in multiple places and there is also a clear sense of ownership so scaling and security are easier to manage.</p>

<p>However, moving to serverless architecture, this ownership is no more clear and exposing a web service to access a database and having a Lambda use this web service does not make sense to me.</p>

<p>I would like to know a bit on the common patterns and practices around this.</p>
"
357689,"<p>I'm building a web application, which involves (currently) a REST backend and a frontend SPA. The backend is hosting massive geo-enabled data, and the frontend is displaying it on a mapbox map. I'm hitting performance issues, because of the stack I set up. Here's the stack, when it comes to displaying a map.</p>

<pre><code>    MapboxGl.js player
        ^          ^
        | (HTTP)    \
 Node.js backend     Mapbox APIs vectors
        ^
        |
     MongoDB
</code></pre>

<p>The HTTP endpoints serve both <em>GeoJSON</em> and REST data. The GeoJSON part is dynamically generated from a set of MongoDB collections (say, <code>gpspoints</code>, <code>images</code>, <code>lines</code> etc.). The Mapbox API is my Mapbox Studio style.</p>

<p>I'm looking for the best alternative to this design. And reading about maps, vectors, tiles, rendering and stuff, I came up to the idea that I should trade my GeoJSON server for a Vector tiles server (only for the map part). I'd like to end up with something like.</p>

<pre><code>         MapboxGl.js player
       ^         ^           ^
      /          | (HTTP)     \
Vector tiles  Node.js backend   Mapbox APIs vectors
   Server            ^
     ^               |
     |               |
Other backend ?   MongoDB
</code></pre>

<hr>

<p>I'm struggling at the point of making design choices.
The questions I have in mind are :</p>

<p><strong>About generating :</strong></p>

<ul>
<li>I came to the idea that I must <em>generate</em> my vector tiles from a GeoJSON. Is that correct ? Would it be possible to generate vector tiles from scratch, I mean, say, from a node.js routine script, parsing datas from various sources (.csv, other databases, etc.) and make operations on it (using turf.js), then generating vector stuff, like pushing, maybe updating, deleting, etc ?</li>
<li>I found <a href=""https://github.com/mapbox/geojson-vt"" rel=""nofollow noreferrer"">geojson-vt</a> which seems to be able to translate GeoJSON to vector tiles, but only in json format. I found <a href=""https://github.com/mapbox/vt-pbf"" rel=""nofollow noreferrer"">vt-pbf</a>, am I on the right direction ?</li>
</ul>

<p><strong>About serving</strong></p>

<ul>
<li>Once I would have been able to generate whatever-format-vector-tile, I am going to be willing to serve it. I found <a href=""https://github.com/mapbox/tilelive"" rel=""nofollow noreferrer"">Tilelive.js</a> which seems very good, and has plenty of modules, including backend ones. Is that a good option ?</li>
<li>Speaking generaly, would it be possible not to generate any static file stuff, and maybe dynamically serve any-vector-format from a database, <strong>through</strong> something like tilelive ?</li>
</ul>

<hr>

<p>Additional info :</p>

<ul>
<li>My datas are ""mutable"", but I have routine scripts that run at night and parse new datas. Thus, live-map-displayed-data mutability is not a requirement : I can update / re-parse a whole dataset at night to serve it on the morning</li>
<li>Both generating and hosting on Mapbox studio is not an option, since my data is critical and must be hosted in specific coutries. A self-hosted option is mandatory, any cloud-based stuff is not ok :/</li>
</ul>

<hr>

<p>This question is redundated in <a href=""https://gis.stackexchange.com/questions/255926/self-hosting-vector-tiles"">gis.stackexchange</a></p>
"
351970,"<blockquote>
  <p>Is this approach secure? Specifically:</p>
  
  <ul>
  <li>Is sending the username and password through JSON safe if done over HTTPS?</li>
  </ul>
</blockquote>

<p>Yes. Headers, request params and request body are encrypted during the communication. </p>

<p>Once on the server-side, do not log the request body :-)</p>

<blockquote>
  <ul>
  <li>How would I prevent unauthorised domains from making calls to this endpoint?</li>
  </ul>
</blockquote>

<p>You can not. Basically, once the API is on the WWW, it's automatically exposed to all sort of malice. The best you can do is to be prepared and to be aware of the threats. At least about those that concern you. Take a look <a href=""https://www.owasp.org/index.php/OWASP_API_Security_Project"" rel=""nofollow noreferrer"">here</a>.</p>

<p>A possible approach to the problem could be implementing (or contracting) an <a href=""https://en.m.wikipedia.org/wiki/API_management"" rel=""nofollow noreferrer"">API Manager</a>.</p>

<p>On-premise API Managers can reduce the attack surface because all the endpoints behind the AM are not necessarily public.</p>

<p>You could achieve the same result with some products in the cloud, but they are absurdly expensive for the mainstream.</p>

<p>Anyways, the API Management endpoints will remain exposed to attacks.</p>

<blockquote>
  <ul>
  <li>Furthermore, how would I prevent programmatic logins?</li>
  </ul>
</blockquote>

<p>If by programmatic logins you mean attacks by brute force, a <em>threshold</em> (max number of allowed requests per second) and a black list should be enough to deter the attacker's insistence. For further information, take a look <a href=""https://www.owasp.org/index.php/REST_Security_Cheat_Sheet#Protocol_for_Authentication_and_Authorization"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Many of the API Managers provide out of the box API Rate Limit configurations and <a href=""https://en.m.wikipedia.org/wiki/Whitelist"" rel=""nofollow noreferrer"">Whitelists</a>.</p>

<p>If you are familiar with the Google API Console, then you can guess what an API Manager can do.</p>

<blockquote>
  <ul>
  <li>Should the refresh tokens be hashed before storing them in the
  database, or am I just being paranoid?</li>
  </ul>
</blockquote>

<p>Whether the refresh token is a plain UUID or anything else, I don't like to expose this sort of implementation detail. So I would suggest to hash it. To me, the more opaque are the implementation details of the security layer, the better.</p>

<p>Regarding the JWT security, take a look <a href=""https://www.owasp.org/index.php/JSON_Web_Token_(JWT)_Cheat_Sheet_for_Java#NONE_hashing_algorithm"" rel=""nofollow noreferrer"">here</a>.</p>

<blockquote>
  <ul>
  <li>If the client were a web browser, how would I securely store the
  refresh token on the client?</li>
  </ul>
</blockquote>

<p>You might be interested in <a href=""https://www.owasp.org/index.php/JSON_Web_Token_(JWT)_Cheat_Sheet_for_Java#Token_storage_on_client_side"" rel=""nofollow noreferrer"">JSON Web Token (JWT) - Storage on client side</a>.</p>
"
351507,"<p>You (almost) already said the answer:</p>

<blockquote>
  <p>a [not so] quick solution would include setting up an entire testing environment (outside Travis, of course), literally replicating the production environment, and using a dedicated system to spawn 100k separate processes, hitting the application running on the testing environment, and collecting reports.</p>
</blockquote>

<p>Yes, this is a good solution. And, even better, <em>it will force you to automate your production set-up</em>. There is a way to do it that is <em>not</em> to use a ""dedicated"" system, instead to spin up and tear down the entire system so that it only runs when you need it.</p>

<p>Use a cloud service like AWS to spin up a production-capable cluster in seconds or minutes (and also able to tear it down in seconds or minutes), and then use a cloud service like AWS to spin up hundreds or thousands of instances to test your app.</p>

<p>This is definitely possible and will force you and your team to adopt a whole lot of ""best practices"". You don't have to do this in AWS, I'm sure that Azure or Google Cloud should also work just fine.</p>

<p>Disclaimer: I attended AWS reInvent a few years ago and saw a talk by a company that did exactly this. They had CloudFormation scripts that could spin up an entire production cluster (in a VPC no less) in less than ten minutes, and then used <em>thousands</em> of spot instances to hammer on the production cluster. It ran daily in a matter of minutes and the cost was very cheap.</p>
"
349301,"<p><em>Q: <strong>But that also means I would have to put the business logic in the
front-end, in the Angular2 web app, right</strong>?</em></p>
<p>Yes. If it's not backed by a server, the business should be implemented somewhere.</p>
<p>After Google's acquisition, Firebase evolved and became a platform for developers of apps that could not afford to build and deploy their own backend.</p>
<p>Most of the Firebase featured services are transversals (storage, login, analytics, and messages service) but few address the problem in question.</p>
<p>It's true that Firebase provides with <a href=""https://firebase.google.com/products/functions/?hl=es-419"" rel=""nofollow noreferrer"">Cloud Functions</a> (sort of lambdas) which can be used to perform business-specific rules. However, for enterprise applications or large applications with a complex domain, this kind of support falls short. Even if we manage to implement the business with cloud functions, the overall could make the solution very hard to maintain. Mainly for the complexity.</p>
<p><em>Q: <strong>So if I someday in the future I would like to make a mobile app front-end, I would have to duplicate the business logic code?</strong></em></p>
<p>Not necessarily. If the web app is built on Angular, cross platforms like <a href=""http://www.nativescript.org"" rel=""nofollow noreferrer"">NativeScript</a> may allow you to reuse the web components, libs, utilities, models, etc. I haven't delved into the subject so I can't assure you full compatibility. The key is <a href=""https://www.typescriptlang.org"" rel=""nofollow noreferrer"">TypeScript</a>, both Angular and NativeScript requires us to code on TS.</p>
<p>The matter then is <em>where we host the Javascript for its distribution and versioning?</em>. A word <a href=""https://en.wikipedia.org/wiki/Content_delivery_network"" rel=""nofollow noreferrer"">CDN</a>.</p>
<p>You could also hire a hosting service with Web support and host the JavaScript yourself.</p>
<p><em>Q: <strong>I guess the alternative would be to create a backend that contains the business logic and uses Firebase for the data storage, but that seems a bit weird (couldn't I just use an ORM or something directly in my backend to achieve the same result without a lot more work?)</strong></em></p>
<p>Some considerations.</p>
<p>On one hand, hosting, rolling out, managing and maintaining a database is no little thing. Not to mention handling security, scalability, availability, etc. So, having a DB provider looking after these things is interesting. It's not a crazy idea these days to roll out our database somewhere on the cloud. Of course, I would not suggest this if we were implementing the middleware or the back-end of a bank. But it could make sense for the client's session, user's profiles, preferences and this sort of data that usually lives on the client-side temporarily.</p>
<p>On the other hand, deploying our back-end is useful for a simple reason, <em>decoupling</em>.</p>
<p>Instead of <a href=""http://en.wikipedia.org/wiki/Mashup_(web_application_hybrid)"" rel=""nofollow noreferrer"">coupling our clients to all sorts of services</a> we don't manage and control, we deploy a server-side application from where we look after these things so that our clients don't have to worry about issues like services shutdowns or breaking changes. Additionally, we gain on simplicity because our back-end acts like a facade.</p>
<p><em>Q: <strong>How do people usually structure these kinds of apps, if they want to make use of Firebase for example?</strong></em></p>
<p>It varies widely from project to project. For instance, we use Firebase + back-end.</p>
<ul>
<li><p><strong>Firebase DB</strong> to share data between <em>devices-accounts-sessions</em>. Also as a changelog, when our backend is temporarily unavailable clients send the write operations to the log, which is synchronized later.</p>
</li>
<li><p><strong>Firebase Cloud Messages</strong> provides us with upstream/downstream push notifications and topics. We use the service for pub/sub message exchange.</p>
</li>
<li><p><strong>Firebase analytics</strong> Mostly for metrics.</p>
</li>
<li><p><strong>Back-end</strong> for everything strictly related to the business</p>
</li>
</ul>
"
341113,"<p>There are a number of things that go into the term ""cloud development"". I'm focusing on AWS because it's the platform I know, but this should be generalizable. I include the AWS terminology to give you somewhere to start searching. There are three things that AWS or other cloud providers can provide:</p>

<h2>Infrastructure as a Service (IaaS):</h2>

<p>Here the cloud provider is selling roughly-speaking plain old machines (almost always virtual machines). Essentially this is a replacement for on premise server: instead of buying a physical machine with so much memory / CPU / etc. and install an OS, the provider sells you a ""VM"" (in AWS: an EC2 instance) with so much capacity (in AWS: an instance type) that comes pre-loaded with an operating system and maybe some software (in AWS: an AMI). I put ""VM"" in scare quotes because the relationship between an instance and any physical machine can be quite slippery, but from a user's perspective instances are basically VM's running in some datacenter you can access over the internet. Leveraging these offerings of a cloud provider can reduce the headache that goes into making sure the application has somewhere to run.</p>

<p>At the most basic level of cloud development, you are doing exactly what you did with traditional on-premise applications except your servers are not physical machines but instances running in some data centers. At a more advanced level, you have started to leverage the unique advantages of these instances (namely: they are easy to spin up and creating a new instance can be automated, see AWS Cloud Scaling) to run your applications distributedly across instances. This can buy you availability (if one of your instances dies, it can be replaced) and scalability (if you need better performance, you can always add more instances).</p>

<h2>Platform as a Service (PaaS):</h2>

<p>At this level, the cloud provider is selling you not only ""machines"" but also important parts of your architecture like databases and storage in a way that is fully managed and abstracted even further from the notion of VMs running on machines. For example, AWS' RDS is a managed RDMS that abstracts away all of the details of maintaining, say, SQL Server on an instance(s) (OS patches, software updates, backups, replication, sharding). Or S3 will encapsulate an instance(s) acting as a blob storage server. At this point the cloud provider is not only giving your application somewhere to run but also providing basic parts of the application in a way that saves time and provides more easily for the big watch words of cloud development: availability and scalability.</p>

<p>At the deepest end of this pool, you are writing serverless architectures using technology like AWS Lambda and have more or less fully abstracted away the fact that there are real machines running your application.</p>

<h2>Software as a Service (SaaS):</h2>

<p>This is usually what cloud developers are working towards: they are developing software (image editing, email, whatever) that from the user's perspective is abstracted from any machine (and in particular requires nothing more than a thin client to use, which in turn is usually a blob of javascript downloaded when visiting a website). It's worth noting that cloud providers can also mosy in on this space: AWS has WorkMail or Elastic Transcoder that are firmly SaaS.</p>

<h1>When are you doing Cloud Development</h1>

<p>To state almost a tautology: doing cloud development is developing with cloud technology, which means leveraging at least one of those *aaS technology. In my mind cloud development is much more about what you use than what you provide. So if you have a SaaS web application, but you own and manage all the data centers and the platform on which the application runs, you are more of a cloud provider than an actual cloud developer (of course at this point you are both). So in my mind 3 is kind of the hallmark of cloud development and offers the most promise: to be able to write applications with nice features (availability, scalability, etc.) at lower operational cost. At 2 you probably are doing cloud development and 1 is so nebulous that it could be anything.</p>

<p>To wrap this up, we can have a quick and dirty questionairre for what sort of development you are doing:</p>

<p>Q: Can you tell me exactly where the machines running your application are?</p>

<ul>
<li>No idea -> on premises</li>
<li>Some of them -> hybrid</li>
<li>All of them -> on premise</li>
</ul>
"
338755,"<p>TL;DR:  Build redundant, modular; test for availability; monitor closely.</p>

<p>After realizing that trying to squeeze in any explanation might go very long so I will write down all the observations I have made.</p>

<h1>Questioning the premise</h1>

<h3>Cloud system is panacea</h3>

<p>Even if you were to go fully on cloud, with a top cloud provider, you will still need to design your application for resilience, grounds up. AWS might replace your VM, but your application should be capable of restarting if left in the middle of computation.</p>

<h3>We don't want to use cloud system, because of x/y/z</h3>

<p>Unless you are an ultra large organization, you are better-off using cloud systems. Top-3 cloud systems (AWS, MSFT, Google), employ thousands of engineers to give you promised SLAs and the easy to manage dashboard. Its actually a good bargain to use them in lieu of spending a dime on this in-house.</p>

<h1>Problems in scoping and design</h1>

<p>Defining,  quantifying and then continuously measuring the availability of a service is a bigger challenge than writing solution for availability issues.</p>

<h2>Defining and measuring 'availability' is harder than expected</h2>

<p>Multiple stakeholders have a different view of availability, and what may happen is the definition preferred by a person with highest salary trumps other definition. This is sometimes correct definition, but often the eco-system is not built around measuring the same thing because that ideal definition is much tricky to measure, let alone monitor in real time.  If you have a definition of availability that can't be monitored in real time, you will find your self-doing similar project again and again with eerie similarities.
Stick with something that makes sense and something that can be easily monitored.</p>

<h2>People underestimate the complexities of the always available system.</h2>

<p>To address the elephant in the room, let me say this: ""No multi-computer system is 100% available, it may in future but not with current technology.""
Here by current technology, I am referring to our inability send signals faster than the speed of light and such things.
All comp-sci engineers worth their salt know <a href=""https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing"" rel=""nofollow noreferrer"">distributed computing limitations</a>, and most of them will not mention it in meetings, being afraid they will look like noobs.
To make up for all those who don't mention <a href=""https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing"" rel=""nofollow noreferrer"">distributed computing limitations</a> I will say, its complicated but <strong>don't always trust computers</strong>.</p>

<h2>People overestimate their/their engineer's capabilities</h2>

<p>Unfortunately, availability falls in the category, where you don't know what you want but you know what you don't want. It is a bit trickier that 'know the wants' category such as UI. 
It requires a little bit of experience and lot of reading to learn from other's experience and some more. </p>

<h1>Building an available system from grounds-up</h1>

<p>Make sure you will evangelize to every architecture and design team about the right priority of the availability as a system requirement.</p>

<h2>Attributes of system helping availability</h2>

<p>Following system characteristics have shown to have contributed to system availability:</p>

<h3>Redundancy</h3>

<p>Some examples of this are to never have only a single VM behind a VIP or never store only a single copy of your data. These are the questions that a good IAAS will make it easier for you to solve but you will still have to make these decisions.</p>

<h3>Modularity</h3>

<p>A modular <a href=""https://en.wikipedia.org/wiki/Representational_state_transfer"" rel=""nofollow noreferrer"">REST</a> is better than monolithic SOA. An even modular microservice is actually more available than the usual <a href=""https://en.wikipedia.org/wiki/HATEOAS"" rel=""nofollow noreferrer"">HATEOS</a><a href=""https://en.wikipedia.org/wiki/Representational_state_transfer"" rel=""nofollow noreferrer"">REST</a>. The reasoning could be found in Yield related discussion in next section.
If you are doing batch processing then better to batch processing in a reasonable batch of 10s compared to dealing with a batch of 1,000,000.</p>

<h3>Resiliency</h3>

<pre><code>""I am always angry""
                    - Hulk
</code></pre>

<p>A resilient system is always ready to recover. This resiliency applies to instances such as acknowledging ACK for a write only after writing to RAID disk, and possibly over at least two data centers. 
Another latest trending is to use <a href=""https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type"" rel=""nofollow noreferrer"">conflict-free data structures</a>, where data structure assumes the responsibility to resolve conflicts when presented with two different versions.
A system can not be resilient as an afterthought, it has to be predicted and built-in. A failure is guaranteed over the long term, so we should be always prepared with a plan to recover.</p>

<h3>Log trail</h3>

<p>This is technically a subtype of Resilience, but a very special one because of it's catch all capabilities. Despite the best effort, we may not be able to predict the pattern of unavailability. If possible, maintain enough log trail of the system activities to be able to playback system events. This will, at great manual cost, allow you to recover from unforeseen situations.</p>

<h2>Attributes of availability</h2>

<p>The non-exhaustive top-of-mind attribute list of 'availability':
For discussion sake, let's assume the question user asks is, ""How many items do I have in my shopping cart?"" </p>

<h3>Correctness</h3>

<p>Do you <em>must</em> produce the most accurate possible answer or is it ok make mistakes? Just for a reference, when you withdraw money from ATM, it is not guaranteed to be correct. If the bank finds out it made a mistake, it might you to reverse the transactions. If your system is producing prime numbers, then I would guess, you may want right answers all the time.</p>

<h3>Yield</h3>

<p>Skip this point, if you answered always-correct for the previous topic question.
Sometimes the answer to questions don't have to be precise, e.g. how many friends do I have on Facebook right now?
But the answer is expected to be in the ballpark +/-1 all the time.  When you are producing expected result your yield is 100.</p>

<h3>Consistency</h3>

<p>Your answer may be correct at one point of time, but by the time the light has left the screen and entered the retina of the observer, things could have changed. Does it make your answer wrong? No, it just makes it inconsistent. Most applications are eventual consistent, but the trick is defining what kind of consistency model your application is going to provide. 
By off chance your application can run on a single computer, your can skip this lovely reading on <a href=""https://en.wikipedia.org/wiki/CAP_theorem"" rel=""nofollow noreferrer"">CAP theorem</a>. </p>

<h3>Cost</h3>

<p>A lot depends on what total impact of short-term effects(loss of revenue) and long term effects (ill reputation, customer retention). Depending upon customer type (paying/free, repeat/unique, captive) and resource availability different levels of availability guarantees should be built in. </p>

<h1>Towards improving the availability of an existing system</h1>

<p>Operational management of individual machines and a network is such complex, that I assume you have left it to the cloud provider or you are already expert enough to know what you are doing. I will touch other topics under availability.
For the long term strategy <a href=""https://en.wikipedia.org/wiki/DMAIC"" rel=""nofollow noreferrer"">Define-Measure-Analyze-Control</a> is a heavenly match, something I have seen myself.  </p>

<ol>
<li><em>Define</em> what is 'availability' to your stakeholders</li>
<li>How will you <em>measure</em> what you have defined</li>
<li>Root cause <em>analysis</em> to identify bottlenecks</li>
<li>Tasks for <em>improvements</em></li>
<li>Continuous monitoring(<em>control</em>) of the system</li>
</ol>

<h1>Causes of un-availability</h1>

<p>Since we agreed that operational management which would cover any physical infrastructure management, ought to be done by professionals I will touch other causes of unavailability for completeness sake.
IMO availability should also include lack of expected behavior, meaning if the user is not shown expected experience, then something is unavailable. With that broad definition in mind, the following could cause unavailability:
- Code bugs
- Security incidences
- Performance issues</p>
"
334664,"<p>Today we have vendors selling cloud based enterprise systems which an organization can lease and also configure and customize to fit the organisations needs. Even if there is work in performing configuration and customization, the implementations can still be done quicker than if the system was to be built in house like maybe 10 years ago. This makes business happy because quick implementations means possibly quicker value realization. </p>

<p>If using an incremental approach to conduct the configuration and customization, and at the same time phasing out the new functionality in the organization's countries- then the system can be delivered in an amazing speed. However.. For the system to be successfully implemented it will likely require change in some countries business processes to match the new system functionality, and the end users would have to be trained etc. I therefore wonder if it is common to use possibly natural drivers of having the system functionality delivered in increments to increase the usage of the system? </p>

<p>Say that the first release was OK to satisfy all implemented countries needs but needs some fixes to be seen as good by the country managers, user etc. Possibly the second release would be really well accepted case the technical team working with the system has told everyone how much better this new version will be. Is it common to work like this? Or is the implementation and deployment usually seen as two different things? If so where does adoption/ change management come in?</p>
"
334663,"<p>Microservices should be relatively independent of each other.  During developer testing, you should be able to only start the one you are working on and perhaps two or three others, usually related to storage, messaging, or authentication.  If you can't, you should work on improving your architecture.</p>

<p>If your full application won't fit properly on a developer machine, then provide a separate cluster for developer integration testing.  A lot of organizations simply budget extra time on their production cloud provider, or you can build an in-house cluster using something like OpenStack.  </p>

<p>This cluster can be over-scheduled because every developer doesn't need it 100% of the time.  I estimate I do about 85% of my microservice development just with unit tests, about 10% just with the single-microservice environment on my own machine, and the remaining 5% on our shared cluster.</p>

<p>You probably also want to invest in better monitoring and orchestration, so you know exactly why your microservices are crashing and can have them automatically restart when they do.  Perhaps something like Prometheus and Kubernetes.  Kubernetes also refuses to schedule microservices in the first place if they will require too many resources than the available nodes can handle.</p>
"
332982,"<p>I'm having some trouble understanding exactly what is the issue in your situation, but if I'm reading you right, your issue is how to organize your development process locally, while still having the flexibility to test externally.</p>

<p>To this I say - <strong>don't</strong>.
If you are going to use an API gateway, test <strong>your</strong> part of the contract - what happens after the gateway. This way you can capture many issues that may remain hidden otherwise due to the API gateway and furthermore any attempt to test the gateway itself are exercises in futility - you are essentially testing an intermediate layer that has already undergone much rigorous testing than your process (hopefully, that is) and which is <strong>designed</strong> to hide/obfuscate the finer details of your own API.</p>

<p><strong>Note</strong>: I'm in no way suggesting you shouldn't test the end result, too, but I believe your main testing effort should go to testing the code <strong>you own and maintain</strong>, not an external abstraction layer. Testing the external view is (IMO) for integration testing and for status monitoring (though this is more often than not provided by the API gateway provider).</p>

<p>As for how to setup and organize the process, I suggest using <a href=""https://aws.amazon.com/api-gateway/faqs/"" rel=""nofollow"">Amazon API Gateway</a> or its competition (I've only worked with AWS, sorry). The very purpose of this service is to abstract away API-specific concerns, while letting you focus on the substance (it ""lets you bring your value"" in marketspeak) of the service.</p>

<p>Touching on the issue of redirecting requests to the 'local' development machine (note this isn't really in scope of this site, so I'm 'trespassing' somewhat, if you will), you can always use some VPN technology to connect the laptop to the API gateway. In terms of AWS, I believe VPC should allow this - <a href=""https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html"" rel=""nofollow"">take a look at this reference</a>.</p>

<p>Finally, this question cries 'devops', which is why I'd like to take the time to point out that any such considerations are best discussed with the more 'sysadmin-y' colleagues, as their expertise will be essential in setting up something like this.</p>
"
330808,"<p>There are several tools that you can use to improve your workflow.  You don't need to adopt them all at once, but they work very well in concert, so you should plan to eventually adopt most of them.</p>

<p>This got quite long, but I've tried to point you in various directions that will be valuable in the near-term.  </p>

<p>The TL;DR: ""Start using git, and get comfortable running a basic linux server.  Once you've gotten those two down, you'll have solved your immediate problems and have a plenty of potential next steps to examine.""</p>

<p><strong>VCS</strong> (git)</p>

<p>You want to adopt some version control system.  <code>git</code> is the default choice, since it's ubiquitous.  Adopting a VCS like git is sort of a prerequisite for things like automated deployments, automated testing, and ultimately full-blown continuous integration (CI).  It even helps with manual deploys, and a little discipline about tagging releases can make rolling back a bad change a lot easier and more predictable.</p>

<p>You'll want to maintain a remote repository as a central source of truth.  You'll have your remote repo set as the <code>origin</code> remote in your local repositories.  When you sit down to work, you'll <code>git pull</code> to make sure you have the most up-to-date version of your code.  After making some commits locally, you'll want to <code>git push</code> to your remote so your changes will be available elsewhere.</p>

<p>You'll want to host that remote repository somewhere reliable.  You could just stick it on some VPS and access it via ssh, but you'd be leaving some nice tools on the table.  </p>

<p>The quick-and-easy solution is to create an account at bitbucket.org.  It's free, including private repositories, and includes a nice web interface with an issue tracker, etc.</p>

<p>If you're concerned about putting your code in a third-party service[1], or you want an opportunity to start managing a VPS (which is a great thing to do if you have the time and mental bandwidth), you can grab a $10/month VPS and install <a href=""https://about.gitlab.com/downloads/"" rel=""nofollow noreferrer"">GitLab Community Edition</a>.  It's a GitHub-like, but you run it on your own server.  It's pretty great, and the omnibus installer makes it relatively straightforward to install and update.</p>

<p>As mentioned in <a href=""https://softwareengineering.stackexchange.com/a/330763/30410"">Sorin P's answer</a>, GUI clients are available for git.  SourceTree and GitHub Desktop are both good.  I prefer the former.  That said, you should git using the CLI tools as your primary interface.  The CLI works everywhere.  </p>

<p>Git can be a bit intimidating, but you can start small and grow.  That said, a basic understanding of how git works will serve you very well.  <a href=""https://jwiegley.github.io/git-from-the-bottom-up/"" rel=""nofollow noreferrer"">Git From the Bottom Up</a> is pretty fantastic.  Understanding the terms on the first page is essential.</p>

<p>To start, you'll want to have at least a sense of how git operates, and be familiar with the basic sub-commands <code>clone</code>, <code>checkout</code>, <code>add</code>, <code>rm</code>, <code>status</code>, <code>diff</code>, <code>commit</code>, <code>pull</code>, and <code>push</code>.</p>

<p><strong>Development</strong></p>

<p>I can't recommend using a local virtual machine enough, especially if your local environment is Windows.  Get familiar with VirtualBox and Vagrant ASAP.  This allows you to develop against a local environment that closely resembles your deployment environments.  It's also a great way to get your basic Linux-admin skills up-to-snuff.</p>

<p>You'll use VirtualBox's ""shared folder"" feature so that a directory on the client (virtual, linux) machine is mapped to a directory on the host (physical) machine.  This means your virtual machine is running ""headless"", with no graphical interface, just a console.  Your editor/IDE, git client, etc, all run on your everyday computer, but your web server, PHP interpreter, SQL database, etc, all run inside the virtual machine.</p>

<p>As far as Editor/IDE goes, you're really free to use whatever you like.  I know people who code in Vim, I was an emacs holdout for a long, long time, others prefer fancier IDEs.  That said, I highly doubt Dreamweaver is optimal.  </p>

<p>My IDE of choice for PHP is PHPStorm.  It ain't free, but it's not expensive, and if you develop in PHP for a living it's a no-brainer IMO.  It understands git, has a nice database client built in, debugging with xdebug works very well, and the static analysis becomes something you can't live without.</p>

<p>If paying is out of the question, you can usually get their ""EAP"" builds for free.  It's essentially the beta of their next release, but is typically very stable.</p>

<p>The other alternative is NetBeans, which is free, and good.  But not as polished as PHPStorm last I checked.</p>

<p>If you prefer something lighter-weight, lots of people like Sublime Text, and new(er) comers like Atom, or somewhat surprisingly, Visual Studio Code.</p>

<p>Bottom line: spend some time checking out at least one or two alternative editors/IDEs.  Most people I know who clung to Dreamweaver did so because of it's FTP integration.  But you shouldn't be relying on that anyway (see below).</p>

<p><strong>Deployment</strong> </p>

<p>Deployment is about getting a new version of your site or app up onto a server.  The method you choose will depend on what your hosting infrastructure looks like.</p>

<p><strong>VPSes</strong></p>

<p>I host just about everything on commodity VPSes.  There are a ton of providers.  DigitalOcean and Linode are popular in this crowded field.  Amazon's AWS provides VPSes as well (the ""EC2"" service), but I don't recommend novices start there.  AWS really shines when you need automation and orchestration, but it's hard to estimate costs, and the complexity is intimidating.</p>

<p>The core of updating a reasonably modern framework-based PHP project to a VPS looks like this (if done completely manually):</p>

<pre><code>localhost$ ssh me@myvps.com
myvps$ cd /path/to/project
myvps$ git pull 
myvps$ composer install
</code></pre>

<p>That's not what one would call a best practice, but if you get to the point that your deploy process looks like that, you're already miles ahead of trying to sync stuff over FTP. </p>

<p>From there, you can start looking into deployment tools.  <a href=""http://capistranorb.com/"" rel=""nofollow noreferrer"">Capitstrano</a> is the grandaddy, but others exist.  Because these tools automate things, they can be a bit smarter than people.  For instance, capistrano will check out the correct version of your sources locally, bundle them up, send them over to the server, expand them into a timestamped directory, and then manipulate a symlink to point your webroot to the new version.  This makes rolling back (often) as simple as swapping back the symlink to the previous version.  It's definitely worth it to read up on these tools; they may or may not be overkill for your needs.</p>

<p>Another tool to be familiar with is <a href=""http://linux.die.net/man/1/rsync"" rel=""nofollow noreferrer"">rsync</a>.  Rsync's job is to sync trees of files efficiently, and it's very good about it.  If you don't want to run git on the server, you can build your own deploy process around local git and rysnc where you checkout (or <code>git archive</code>) a local tree of your sources, do whatever build process is necessary (say, <code>composer install</code>), and then rsync the whole thing to the server.</p>

<p>Any of these tools can be used to build up an automated deployment, using simple shell scripting.  I suggest you aim to have a script that pushes (or pulls) a new release, and does so in such a way that it's very easy to roll back if things go wrong (the capistrano-style symlink trick is a great place to start).</p>

<p><strong>Non-VPS Environments</strong></p>

<p>I can't really speak to these.  There is old-timey shared hosting like you're used to, where you're pretty much limited to SSH with no shell access.  There are also Heroku-alike platform-as-a-service offerings for PHP, but I've never liked that idea. </p>

<p>That said, if you're absolutely married to shared hosting, you may be able to approximate an rsync-style workflow using a decent CLI FTP client.</p>

<p><strong>What About Databases?</strong></p>

<p>Deploying code is great, but sometimes a new code release requires some kind of database change (say a new column on some table, or some update that fixes artifacts of some bug you've just fixed).  </p>

<p>This gets more complicated (especially when thinking about roll-backs), but there are tools to help.  I've used both <a href=""https://github.com/davedevelopment/phpmig"" rel=""nofollow noreferrer"">PHPmig</a> and <a href=""https://github.com/doctrine/migrations"" rel=""nofollow noreferrer"">Doctrine Migrations</a> with great success, and there are others.</p>

<p>The basic idea here is that you write small migration classes and the migration tools track which ones need to be run, and then run them.  Your little bash script gets a new line it it somewhere that says something like <code>phpmig migrate</code> so when you run your deploy, any pending DB migrations get run automatically.</p>

<p>[1] Most people shouldn't be concerned IMO, but I don't know your circumstances, politics, or level of paranoia.</p>
"
325112,"<p>PostgreSQL would be slightly better, because its design leans more towards analytical workload, unlike MySQL which was designed for transactional workload. That is if you want to do the calculations directly in the database. Getting player statistics is analysis of the data.</p>

<p>The obvious drawback (while not really that important) of PostgreSQL is its lesser popularity, ie. it's more difficult to find a community where you could discuss a PostgreSQL specific issues.</p>

<p>You should know that Heroku is a cloud application platforms, but that's it. Using Heroku alone will not suddenly improve your architecture and make your application scalable.</p>

<p>What you are really looking for is caching. After performing statistical analysis on the data you currently have, you should store the results into cache so they do not need to be recalculated each time. During the API call this cache should then be freed, statistics should be recalculated and reinserted into the cache.</p>

<p>But even then, you do not know yet what the biggest bottleneck is going to be. What I suggest you to do is to create a dummy database, fill it with millions of dummy records and try to run a query against it. The query you are likely to use. This way you can benchmark the database without actually having the application yet.</p>

<p>Another approach is to update the data sequentially. Call the API multiple times a day and update (cache invalidate and reinsert) only the statistics for players which are affected by the recieved batch.</p>
"
322899,"<blockquote>
  <p>Are there any best practices for limiting access to data in DynamoDB besides simply authentication and authorization?</p>
</blockquote>

<p>For DynamoDB, the entire security model is the authentication and authorization (IAM) supplied by AWS; there is nothing ""besides"" those two. However, you don't ever access DynamoDB directly from the Internet, but instead go through a service of some form (e.g. <a href=""https://aws.amazon.com/api-gateway/"" rel=""nofollow"">API gateway</a> backing onto a Lambda function, or some custom code running on an EC2 box). AWS will then manage the authorization for you (e.g. via <a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html"" rel=""nofollow"">IAM Roles for EC2</a>), and nothing outside of AWS ever needs to know what's going on. You should definitely <em>never</em> be putting any IAM keys into your applications.</p>

<p>As an aside:</p>

<blockquote>
  <p>storing sensitive data (config info) in DynamoDb</p>
</blockquote>

<p>I wouldn't personally go down that route, not because of the nature of the data, but because of the lack of atomicity and isolation in DynamoDB. You don't need the blinding speed of DynamoDB, so just keep in it good old SQL.</p>
"
322658,"<p>We are contemplating breaking up our monolithic monsters into microservices based architecture progressively. We have 5 teams, each team containing 2-3 C# developers, at least 1 database developer, and 2 QA engineers. Besides the huge culture and paradigm shift going from a monolithic architecture to microservices there are also the technical challenges. I would like to ask the community for some wisdom and advice so we can avoid making the same mistakes. </p>

<p>Are there any good industry examples where .NET based microservices have been successfully utilized in a production system? What was the strategy for the following?</p>

<ul>
<li><strong>Org</strong>: how did you organize the .NET solutions/projects?</li>
<li><strong>Development</strong> planning: in terms of development planning, how did you break up the work across teams? What was the overall strategy for making sure contract compliance across microservices was negotiated across teams?</li>
<li><strong>Load balancing/routing/API gateway</strong>: what was the strategy for load balancing, redundancy, and scaling? Did you just go with a complete async architecture and used a queue for microservice communication or did you do peer-to-peer through a load balancer/API gateway? And why?</li>
<li><strong>Test automation</strong>: how did you handle test automation. This along with continuous integration seems absolutely necessary for microservices architecture. How did you go about it?</li>
<li><strong>Deployment</strong>: how are you deploying? One VM/microservice or one container/microservice or something entirely else? How did you handle the fact that now you have tens of database if not more considering each microservice would have its data store, what did that do to your infrastructure and how did your DBAs handled it?</li>
<li><strong>Infrastructure</strong>: how did your infrastructure scaled with this architecture. Was it super chatty and you had to fine tune it or did the network handled it without an issues? Are self-hosted or in the cloud?</li>
<li><strong>Monitoring</strong>: what is your monitoring strategy? How are you keeping tabs on tens if not hundreds of microservices? Is it mostly through logging and central monitoring?</li>
</ul>
"
322153,"<p>I'm pretty familiar with Windows Failover Clustering. It is a large chunk of admin work and budget (if actually fault tolerant. i.e. no using a single SMB3 file server for shared storage). It <strike>frequently</strike> weekly or monthly fails over for no discernible reason. When it fails over, service is lost for a few moments while the new active server takes over the resources (IP address, disk, etc.). The ongoing maintenance is irritating because some settings must be setup on each machine. This can require failing the server over and repeating config changes. Some things support shared configuration, avoiding this. (e.g. IIS, however IIS is not natively clusterable and requires setting up a script to migrate it between servers)</p>

<p>All that to say, be sure your company is aware of the costs before deciding to go this route.</p>

<p>A slightly less involved option is <a href=""https://technet.microsoft.com/en-us/library/hh831698(v=ws.11).aspx"" rel=""nofollow"">Network Load Balancing</a> (NLB). You can use it as active/active. It will handle failures and avoid failed servers. It does not require shared storage as failover clustering does. Bear in mind that it's designed for load balancing, not availability. You can set it up for an active/passive type of scenario by giving one server full priority, but <a href=""https://en.wikipedia.org/wiki/Split-brain_(computing)"" rel=""nofollow"">split-brain</a> may be possible there. There could also be some network level adjustments required, depending on how you set up NLB.</p>

<p>There also exists the <a href=""https://en.wikipedia.org/wiki/Round-robin_DNS"" rel=""nofollow"">DNS Round Robin</a> approach where you essentially setup a DNS record that points to multiple IP addresses. Each IP address belonging to a different server with your service installed. The DNS server returns the IP addresses in rotating order. But the viability of this depends heavily on the client's implementation. Often, the client's system will pick one and cache the result. This can lead to perceived downtime because the particular server that the client cached could be the one that's down.</p>

<p>In any case, your service really needs to be setup to be stateless. Otherwise, the next request could be handled by a server which is unaware of the user's previous session on a different server. (State could be centralized, but again it would need to be in the cluster so it wasn't a single point of failure. You'll notice clustering is a black hole which absorbs surrounding concerns too. Speaking of which...)</p>

<p>None of this addresses other single points of failure, like network, database, hardware drivers (yes drivers... some people go as far as getting different hardware from different manufacturers to ensure that a driver or hardware bug doesn't impact all systems).</p>

<p>You could also look at taking advantage of a cloud offering for redundancy, like Azure in the MS world. There is too much to list on this topic, but these services tend to offer a number of high availability features including at the network, storage, and database levels.</p>

<p>I'm sure this is not a complete list of your options. And as always none of them are strictly ""better""... there are trade-offs.</p>

<p><strong>Added</strong></p>

<p>I also thought of hardware load balancers. It's a network device like a router or switch with servers plugged into it. But the basic idea is that it listens on an IP, and requests to that IP get forwarded to the connected servers in an alternating manner. Only using one load balancer is a single point of failure, since it is one device that can cut off access if it fails. In order to do redundant load balancers, you need redundant network paths and a routing protocol which can handle that.</p>
"
322077,"<p>AWS and maybe others provide a built-in service for logging that you are invited to use. If you don't want to use it, you probably have to push an object to a cloud storage instead. </p>

<p>In a classic lambda function architecture, your service is launched on call, ressources are allocated for the duration of the program, then fred. You typically don't have access to low overhead persistance, because once your service execution is over, the ressouces are gone.</p>

<p>For the deployment model, as far as I understand, AWS lamba does not offer the possibility to use a lib for several packages as far as I understand ; you'd have to pack all services in a single deployment package with your library included, which is not a very scalable architecture.</p>
"
316508,"<p>We ended up implementing the ""Windows Service"" scenario knowing that it would not scale but intending to build a scalable solution when we have to. With the current implementation a Web Job runs a console application that queries the DB for all notification definitions, runs appropriate queries and sends e-mails. It is easy to test (just run the console app) and easy to deploy (web jobs deploy with the web site). The best part is that if we go for the scalable solution we can just remove this part without affecting any other code because notification definition UI and database schema do not depend on the actual sending method.</p>

<p>We ended up deciding against Stream Analytics because Stream Analytics are not very useful for ever changing queries. Changing the query requires a restarting the Stream Analytics job. Putting the notification definitions in the job is hard. There is something called training data that can be used but it is not very suitable for this case.</p>

<p>After we implemented the solution Microsoft launched Azure Functions which at first sight seem like a good fit. I did not do a detailed research because we already had a working solution at that point. I tend to dislike how separate Functions tend to be from the rest of the codebase and I worry about deployment. I guess there are solutions to these problems but they were not in the demos.</p>

<p>The scalable solution that we had in mind and will evaluate again if we need to scale would work as follows - The Message processing part sends each and every message into azure queue. A consumer gets the messages one by one and checks if the message is eligible for any notification. If it is it is put in DocumentDB or Azure Tables bucket list which is accessible based on the notification ID(s). Another consumer periodically checks the notification buckets and sends all the messages and deletes the bucket list. The solution seems scalable because we can add whatever number of consumers we need and azure queues are built to be scalable. There seems to be no bottleneck part. Now of course with Azure Functions we can replace parts of this systems with if they turn out to be better suited for the task.</p>

<p>Of course I am still open to criticism and other ideas.</p>
"