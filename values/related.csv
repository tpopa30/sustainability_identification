Id,Body
448250,"<p>The kinds of requirements you need to store data is <em>very</em> different from the kind of requirements you need to run a set of computationally complex calculations on a data set, and while this is not the only use case it <em>especially</em> applies if you're running into exponentially expanding complexity (or close to it) due to the interactions in your large data set.</p>
<p>Just to create an example, it's fairly straightforward to store the data for customers, products, stores (including inventory) and purchases. Those are 4 CRUD endpoints that hardly interact with one another. CPU-wise, you don't need much.</p>
<p>But let's ask ourselves the question if the customer bought products optimally, i.e. in a way that the total commute distance from the customer's home to the store where they made a purchase is minimal (across all purchases made).<br />
This requires a lot of calculation complexity. Not only do you have to calculate distances, you also have to consider that if you find that Customer A lives closer to store B than to store A, and that these stores both sell the same product, you have to also account whether store B's inventory has one of these products to spare. And if there's multiple customers who could go to store B instead, which ones should we shift around to maximize our improvements?</p>
<p>I've intentionally picked an example that leads to many different possible subcomplexities and calculation to show you the difference between straight up storing data and operating on it on a per-entity basis, compared to the kind of interaction and complexity that reporting on that same data can bring with it.</p>
<p>AWS Redshift or Google BigQuery are tailored towards big data operations, and therefore will be able to run your reporting logic better and faster than your usual hardware can.</p>
<hr />
<p>There's also some side points:</p>
<ul>
<li>What if your usual hardware does not have the kind of spare capacity for running reports, i.e. if your normal use case does not accept any kind of performance degradation?</li>
<li>The point made above is not too dissimilar from asking why search engines exist as a remote third party service (as opposed to being run locally by either the searcher or the websites themselves), albeit that this is more blatantly obvious due to the network overhead in needing to call every site in existence.</li>
</ul>
<blockquote>
<p>wouldn't a failed assertion on data in the data warehouse mean that something is not right with the data from the data source(s)?</p>
</blockquote>
<p>Be very careful about negating that statement. Yes, if your extraction already fails, there's clearly something wrong. But if your extraction <em>doesn't</em> fail, does that prove that your data is therefore all correct? <strong>No</strong>. And that's the more important consideration of the two.</p>
<blockquote>
<p>wouldn't it be better if the ETL used my app's API to extract the data instead of directly connecting to the operational (in this case, Mongo) database?</p>
</blockquote>
<p>I generally favor considering the datastore a private implementation detail of the service and therefore routing everything via the service. However, this decision is scoped to <em>application use</em>. I do make exceptions for infrastructural operations, e.g. datastore backups. Your ETL can similarly be considered an infrastructural operation that gets special access to the raw data. There are justifications for this:</p>
<ul>
<li>Maybe it's because it would put an undue burden on the service whose performance would degrade.</li>
<li>Maybe the service has a complex business layer which is plainly irrelevant for the purposes of the ETL.</li>
<li>Maybe the service is only tailored towards end-user interactions (per-entity) as opposed to bulk data operations.</li>
<li>Maybe your datastore contains audit logs that are absent from the service's output by design.</li>
</ul>
<p>I don't know which one would apply in your case since I don't know your case.</p>
"
445186,"<p>It seems you are designing a simple database query engine, and are now looking for efficient cloud-native approaches to handle the backing storage.</p>
<p>This is ultimately a question of cost minimization, balancing factors such as storage costs, costs of executing a query, and development costs.</p>
<p>Some points to consider:</p>
<ul>
<li><p>Keeping the original CSV file is simple, but requires re-processing the entire file for each query, which might be a bit inefficient if those files are large.</p>
</li>
<li><p>Compression might reduce storage costs. Decompression tends to be really fast.</p>
</li>
<li><p>You might transcode the data into a different, more efficient format. For example, binary formats will be more compact especially for numerical data, and allow processing without having to re-parse the file. Data formats like Apache Parquet might be worth looking at. However, this might require that you can assign types to each column, which might be tricky in practice (e.g. phone numbers might look like integers but are not).</p>
</li>
<li><p>You might split up the data into individual columns or records. Now, executing a query will require multiple sub-queries, but you might be able to perform a query without having to load all of the data. For example with columnar storage, you only have to load the <code>age</code> column in order to evaluate an age filter. If you put the data into some existing database, this will simplify query evaluation for you. But since you don't have a clear data schema, you would be limited to non-relational databases like document databases, or would have to use an inefficient EAV design.</p>
</li>
</ul>
<p>I suspect that your volume of data is very low (only a couple of GB), that individual documents are fairly small (a couple of MB), and that your rate of queries is very low (only a couple of thousands of requests per day), so that your storage-related costs are in the cents-per-month region where significant optimization is infeasible. However, development costs are orders of magnitude larger, so that more complicated approaches are likely to be uneconomical. If you're looking for faster processing with moderate development effort, a sensible initial step would be to pre-process the uploaded data into a compressed binary data format like Parquet.</p>
"
444739,"<p>I get the feeling you are about to fall into infrastructure oversizing. Your doubts are reasonable, but they also say that you don't have any fact that you need such environment segmentation. At least not yet.</p>
<p>What do you have so far? You have a monolith, which is developed and operated across three environments. They are already provisioned with everything the solution needs to be functional. I can't think of a better starting point to test (and compare) the breakdown of the monolith.</p>
<p>As more services appear in the ecosystem, the more likely you need the flexibility to test different permutations of services. That flexibility is achieved by configuration and parameterization.</p>
<p>Bear also in mind that environments are not mere infrastructure segmentations. Environments are composed of runtime and data. Data on production is often subject to regulatory compliance, so you don't assume easily that &quot;development should use the production services&quot;. It's not that simple.</p>
<p>Think also of the costs. If you were to deploy production on some of the well-known public cloud platforms, you would realize how using production services for testing development can wipe your budget in a breath.</p>
<p>Another reason to avoid mixing up environments is metrics. Cross-env resource utilization can invalidate or distort the lecture. It's going to be hard to monitor Production for adaptions if random activity in Development (or other envs) is generating &quot;noise&quot;.</p>
<p>As an MS architect, your mind should always be on the big picture. That involves the software but also infrastructure, security, operations, cost-optimization, regulatory compliances, tracking, metrics, business goals, etc.</p>
"
444704,"<p>I am trying to figure out how to design SaaS system that offers subdomain per customer (e.g. <code>&lt;customer&gt;.example.com</code>), where each customer is on specific region.</p>
<p>Due to compliance and regulation, some customers must have all data in specific regions. There are two deployments of existing system (US and EU) with different domains. System is available only on these domains and customer must know on which system they are in order to access it.</p>
<p>Now, I want to have domains in form <code>&lt;customer-name&gt;.example.com</code> and have it automatically use deployment in appropriate region.</p>
<p>Two approaches come to mind:</p>
<ol>
<li><p>have single onboarding endpoint and during onboarding contact services in appropriate region, including configuring DNS to point subdomain to services in appropriate region.</p>
</li>
<li><p>have <code>*.example.com</code> DNS record point to global proxy that knows how to route requests to appropriate region based on <code>*</code> part of the URL by checking where the customer should go.</p>
</li>
</ol>
<p>Cloud provider is AWS, Route53 for DNS management and EKS is used. Current deployments are completely separated (except for CI, monitoring and such). I am looking for solution to utilise managed services as much as possible to reduce implementation time and maintenance overhead.</p>
<p>Option #1 is not ideal as it requires dynamically setting DNS records and potentially exposes clients names by enumerating existing DNS records.</p>
<p>Option #2 introduces additional latency and potential compliance problems (in which region is proxy hosted?).</p>
<p>Are there any other options that I am not seeing? Are there some useful articles or resources on this topic?</p>
"
442126,"<p>My team have recently inherited a very poorly written, business-critical, monolithic LAMP application with the goal of guaranteeing reasonably uptime and scalability targets in just a few weeks. Our deadline is somewhat fixed because it depends on user volumes over the peak season, which we forecast will start picking up around Black Friday.</p>
<p>The monolith is currently deployed to a single, &quot;snowflake&quot; EC2 in AWS. Deployments to this instance are entirely manual. The biggest operational risk we're currently trying to mitigate with this instance is that it currently has a dynamic IP attached, which the cloud provider would likely recover if the instance rebooted due to a server crash or maintenance work. We've spoken to AWS about it and they've confirmed they would not be able to give the IP back to us. This would break dozens of integrations that are (wrongly) currently pointing to this ephemeral IP address start failing. By some miracle though, the VM has been stable enough not to crash once so this situation has never happened before but that doesn't mean the risk of it happening is not there, especially during peak traffic.</p>
<p>The monolith is very tightly coupled to the Production environment in which it runs, as it has a ton of hardcoded references to its own Production ephemeral IP address. Moreover, all of its application config is hardcoded and duplicated across every dozens of file. All of these settings assume they're running in a Production environment too.</p>
<p>We want to make the minimum amount of changes to the application code to allow us to replace its dynamic IP with a reserved, static one. However, because the monolith is not modular at all and there is little to no code reuse in it, we've basically had to change hundreds of files just to be able to fetch all the application settings from a single config file. This has allowed us to deploy a version of the monolith with Test settings to a Test environment that we're able to run manual tests against.</p>
<p>Even though each individual change we've made is very small, we have basically touched almost every source file in the codebase. The people that wrote this legacy application and therefore had any good understanding as to how it works no longer work at our company. With no support from domain experts and no automated test coverage whatsoever, we're looking at a pretty risky, time-sensitive deployment that we know needs to happen as things could get pretty ugly for us if the instance reboots or dies for whatever reason and we lose its IP.</p>
<p>So the main question facing us now is, what would be the safest way to make this deployment happen as safely as the circumstances allow? We've also ruled out accomplishing this through multiple deployments because of how tightly coupled and messy the code is.</p>
<p>I'm not looking for any silver bullets here. Any ideas or suggestions would be more than welcome.</p>
"
441569,"<p>I'm currently developing a chatbot that will be used for booking trips. The particular party I'm interested in for now is the rider. I'm using AWS's DynamoDB, because it seemed to be the most flexible and fast for a system like this that is likely to get lots of concurrent traffic, and in which records won't have well-defined schemas.</p>
<p>The idea is, the rider greets the bot, the bot responds asking them if they're a rider or driver. For now, I'm only concerned about the rider. The first and most obvious thing is to create an item in the DB of the user, identified by the phone number of the user. in addition to that, I should also probably store the role of the user as an attribute, so that in the future they don't have to repeat this first step.</p>
<p>From here, the rider then has the option to view active trips (ones that have been booked but not completed), cancel a trip, or book a new trip. The trip details will all be stored on a different database I handle elsewhere. The only thing that I will store concerning the trip in the message history database is the trip's id, so that I can retrieve the relevant details of the trip from the other database using the id.</p>
<p>The next steps in the flow should be straightforward.</p>
<p>Here's my problem. I have to track context to know where in the flow the user is. How I'm thinking of doing this is by creating a conversation item in the database that then tracks the context/topic with a user. Then, when the user enters into a new part of the flow, the context for that conversation is updated, and the chatbot knows what response to then give.</p>
<p>From what I read in the AWS docs, they recommended that you use a single-table design for this. But I'm struggling to understand how exactly I would structure the database in this case, and particularly in an efficient manner. Would I set the partition key to be the user_id (phone number), and then store lists of messages and conversations associated with that number?</p>
<p>I'd appreciate any help with this problem.</p>
"
440067,"<p>Designing, implementing, and verifying application-side controls to manage which shard or cluster contains the data and that reads and writes are happening against the shard is probably going to be more expensive than taking advantage of out-of-the-box clustering and replication solution.</p>
<p>Availability begins with the data center. Using any reputable data center would get you things like physical security, redundant power and networking connectivity, fire control systems, and HVAC. You'd need the redundant power and networking to start to achieve the desired uptime, and the other physical safety controls would help in disaster scenarios.</p>
<p>Once you have the data center under control, the mainstream file and data storage tools offer built-in configurations around sharding and replication. These implementations are probably going to be more robustly designed and tested than any in-system features that you design. For open-source tools, they also <a href=""https://en.wikipedia.org/wiki/Linus%27s_law"" rel=""noreferrer"">benefit from many eyes</a>.</p>
<p>99% availability is really a lot of downtime. It's 00:14:24 per day - fourteen minutes and twenty-four seconds. Annually, it's over 3.5 days. Depending on your SLAs, planned downtime may or may not be considered against that 99% uptime. Regardless, that's a lot of downtime.</p>
<p>It would be probably be cheaper and less error prone to use a data storage solution's out-of-the-box sharding and replication functionality. Going with a managed service from a large provider that undergoes audits of their processes and capabilities, like Microsoft, Amazon, or Google, would increase the confidence that the desired uptime is being delivered.</p>
<p>Just as examples: A multi-availability zone deployment of AWS's RDS service for managed relational databases would get you to 99.5% availability. For files, AWS's S3 has a 99.9% monthly availability and grants 100% service credit if the availability is below 95% for a month. Going with services like these and allowing the developers to focus on your core competencies would probably pay for itself pretty quickly while exceeding your more strict availability requirements.</p>
"
436491,"<p>If you have requirements to keep data separate there are no common services. Common services are just great targets for exploit, they just increase risk and don't actually make thing easier for you. With a common service any compromised client now can potentially compromise all your clients (or you can accidentally expose one client to another's data). In addition to the security risk, any downtime on a common service affects all clients, so no you have more angry people calling or insane scheduling hurdles for planned maintenance. It's also likely that all clients will want different upgrade tempos, so it's possible you need to run multiple versions of common clients anyway because some are 2.0 and some are 1.0 and you now have tons of complexity.</p>
<p>If you have requirements to isolate something than everything that uses it needs to be isolated. It's better to think of however many microservices an app needs as a single unit in these cases. With cloud or virtual machines it's really not that different to deploy 15 services or 11 based on your examples. Cost shouldn't be dramatically different either as you should be able to downsize common services as they would get 1/3 of the traffic, and the expense of a potential breach of contract will absolutely exceed hosting costs.</p>
"
424900,"<p>We are working on a multi-tenant SaaS product that has a ledger of all tenant customer transactions, which we use to track invoice states on a line-item level. This particular application will result in a lot of upstream interactions with service providers which results in a lot of line-item debits and credits. The point being that a simple action such as generating an invoice and receiving payment, could quite easily lead to 50-100 database entries.</p>
<p>We use a normalized relational database as we a) need to do various different joins to display the data from different vantage points, and b) need to present the data at different levels of granularity.</p>
<p>For instance, the application allows different customers to execute purchases on different accounts. The system allows one to view a transaction history on an account level, or to view transactions on a customer level across accounts (these are just two examples, there are many more). Ito granularity, one might want to view all debits and credits for a single invoice, or one might want to view a debtors aging analysis, which will aggregate all records for a single tenant.</p>
<p>To ensure theses queries run in a timeous fashion, we've indexed the hell out of the database. This works reasonably well for small tenants where query times are in seconds, but the aggregate queries are into the minute mark for large customers. Ideally we would like all queries to run sub-second.</p>
<p>We've been toying with the idea of building indexed views, but given that this OLTP system is quite chatty, we are concerned that table locks (caused by the updates in referenced tables) will cause all tenants to suffer when changes are made (as naturally the indexed views would need to be recalculated).</p>
<p>Building a data warehouse, summary tables, or some sort of data cube is not ideal as it too would need to be updated in real-time since changes need to reflect immediately and we would no longer have a single source of truth. Furthermore, cost is a big concern too. Our cloud expenses are through the roof.</p>
<p>At this stage, we are thinking of building a hybrid data access layer which will cache certain views in Redis and persist the data to the source database. The views will then be updated after the TTL expires or if any records the view touches are updated. Any thoughts on this approach?</p>
"
423151,"<p>No, you should NOT expose the database publicly. This is very difficult to do safely, and is difficult to integrate directly with a HTTP-based API.</p>
<p>So what you're doing – having a web app backend that sits between the user and the database – is the typical architecture. This can deliver perfectly acceptable performance, in particular if the backend server can scale horizontally. But how performant is this will be more of a function of the networking hardware and network architecture than of the software architecture. And in most cases, the user's internet connection is the limiting factor, not your server.</p>
<p>What you can do is consider whether a SQL database has to store all of this data. If you have columns that contain large blobs (multiple MB) then you might want to consider storing the data externally. Blob storage / object storage such as S3-like storage could be more appropriate. Since every object has an URL, the user can fetch it directly without having to go through your backend. Access controls can be implemented with signed URLs that are only valid for a finite time. Your backend would then get a list of object IDs from the SQL DB, and generate signed URLs for the user.</p>
<p>Note that cloud egress charges can make such a design much more expensive than doing extensive processing and filtering within your backend. Sometimes the user will truly need access to the full data, but often they just need a summary or report of the data.</p>
<p>In this scenario it seems that you merely have a lot of data, not large blobs (average row size only about 30kB). So moving parts of the DB to external storage is likely not feasible, though it could be possible for the reference data set.</p>
"
420466,"<p>I very much agree with Todd with the advice:</p>
<blockquote>
<p>I strongly recommend that you don't overengineer your architecture.
Less is more.
YAGNI: Start simple and tunable, then update your architecture as
needed.</p>
</blockquote>
<p><strong>Specific Answers</strong></p>
<p>To answer your specific questions but reordered slightly:</p>
<blockquote>
<p>Missing posts in a user app like this in case of failure is a big no-no, so I wonder what is your view on this, and what is the best resolution of the problem.</p>
</blockquote>
<p>First, we need to be clear about the problem. Implicit in your question is the assumption that write throughput is a big problem to solve. Buffering will happen in Redis when the write-throughput has exceeded the single master write-path of a MongoDB cluster. You are then considering what might happen if Redis crashes. This is all good thinking. Yet is optimising write-throughput the major architectural challenge in your service?</p>
<p>You describe your service as news aggregation. We might expect that reads might dominate. Writes are be many orders of magnitude smaller for news. If it is indeed news you are publishing you can consider buffering writes at the client under high write (e.g., browser local storage). If you <a href=""https://medium.com/helpshift-engineering/load-shedding-in-web-services-9fa8cfa1ffe4"" rel=""nofollow noreferrer"">load-shed writes</a> you can add simple logic at the client can try again using an exponential back-off give your backend time to catch up (else heal from an outage). A tiny write message queue at every client is easy to implement. You can expect it to be reliable and easy to test. Only if your service is not accepting writes, and the user deletes their client app (else clears browser local storage), will they lose work.</p>
<p>If your service is a bit more like an Uber or Lyft ride-sharing service then write throughput will be critical. With ride-sharing there are a huge number of jobs published constantly and searched for in a geospatial manner. Write throughput and graceful outages are critical to those services. In which case you can research how those companies solve the write-path problem.</p>
<p>Below I will propose two solutions that I would investigate. One that optimises for write throughput and another that might be more suitable for a low budget &quot;news aggregation&quot; service that can scale up later.</p>
<blockquote>
<p>Does a high-availability cluster with write-behind cache solve all of my problems?</p>
</blockquote>
<p>Operational complexity and recovering from failures is likely to give you the biggest real-world business problems. Studies of big systems indicate that it is &quot;<a href=""https://blog.acolyer.org/2017/06/15/gray-failure-the-achilles-heel-of-cloud-scale-systems/"" rel=""nofollow noreferrer"">grey failures</a>&quot; are the most tricky sort of problems that lead to extended outages and data loss. The moment you integrate two products on your write path you can expect complex interactions under failures and load. In theory, yes, a highly-available cluster with a write-behind solves all your problems. In practice, I would hesitate to go with a HA Redis Cluster in front of a MongoDB Cluster.</p>
<blockquote>
<p>Does high-availability write to disk (Redis), guaranteeing that no data will be lost?</p>
</blockquote>
<p>Well, Redis is an excellent product. In the past, it has had some <a href=""https://aphyr.com/posts/283-jepsen-redis"" rel=""nofollow noreferrer"">hiccups</a> with data loss. I have used Redis myself as cache in a few architectures. Would I use Redis as a silver bullet to optimise the write-throughput a geospatial based web-scale system? No.</p>
<p>IMHO opensource products grow up to fulfil the core needs of their community. They then bolt on additional features that serve as wide a set of needs as possible. People then get into trouble when they push those additional features to the extreme. You then encounter bugs that haven’t been found and fixed by the wider community as you are the outlier. Redis excels at being a low-latency cache optimising the latency of the read path. Using it to optimise the write-path of a very high write throughput geospatial system would be something I would be very nervous about in practice. I think Redis is great and I will continue to deploy it to optimise the read-path. Yet I would go with a specialist solution to optimise the write-path.</p>
<p>If write-throughput is the major challenge than I would look at Apache Kafka or an alternative. It has an architecture that is very different from traditional massage brokers so that it can scale to &quot;IoT&quot; levels of writes. You can then have consumers that update your main data store and possibly a separate geospatial index. If a consumer was buggy/lossy you can easily replay the data stream to “fix-up” your geospatial index service or main store after you have fixed the bug in your consumers. I would then have a single product on the durable write path. If the secondary writes to the main database or index service fail the ability to easily replay a stream of events will be invaluable. Engineering to make it easy to recover from the unexpected is money better spent than trying to eliminate the unexpected. A large number of real-world outages are caused by human error so designing for failure recovery is critical.</p>
<blockquote>
<p>Even with write-behind cache, is it the safest course of action to add a message queue (eg RabbitMq) or is this generally not necessary even under high load?</p>
</blockquote>
<p>Well, RabbitMQ is an excellent product. I have used it myself. If you can buffer the writes at the client under high-load, and do load shedding in-front of RabbitMQ, then it might be a great fit.  It would be low-latency and allow you to update the main database and possibly a separate geospatial index. Using a message queue to decouple and independently scale microservices is a great strategy. Yet it will add operational complexity at go-live when it might not be needed until the service starts to take-off. So I would be tempted to start without it and then use it as a way to break apart a simple initial architecture into a more complex architecture at a later date.</p>
<p><strong>My Two Suggested Approaches</strong></p>
<p>(1) For a &quot;mega-scale&quot; Geospatial System with both high writes and high read:</p>
<p>If write-throughput is a big challenge I would evaluate both Apache Kafka and Apache Pulsar as the initial durable write-path. Those are very highly scalable messaging engines. Kafka is often used as the transport in asynchronous microservices architectures so we might expect it to have satisfactory latencies at high throughput. I would recommend having an edge &quot;news publish&quot; microservice that exposes RSocket over WebSockets to the clients. Clients would push the new news message over WebSockets as a request-response interaction. The RSocket server would simply apply security (validate the user), validate the payload, write it into Kafka/Pulsar, and respond. I would add logic at the client to hold the message in local storage and periodically retry if it got timeouts or errors. I would exponentially back-off in the retry logic to allow the service to recover.</p>
<p>A news aggregation service will need scalable reads. Your proposal is to use a MongoDB cluster. This is because it does geospatial queries. That would work. If you were looking to scale reads to a much higher level you could consider using a more scalable main database such as ScyllaDB and deploy separate geospatial query service such as Elastic Search. The consumers that read new news from Kafka can first write to the main ScyllaDB database. They can then update a geospatial index in Elastic Search. Elastic Search is often used as a secondary index for both free text search and geospatial indexing. As you are a news aggregation service deploying a dedicated free text search index may also be useful.</p>
<p>(2) For a &quot;start-up budget&quot; system with initial low writes and modest reads:</p>
<p>While we all want to build a service that can scale to huge loads from launch often the reality is that the main threat to a business at launch is over-engineering. It makes business sense to initially focus all engineering effort on the user experience while using a simple and cheap backend. Facebook was a college toy at go-live. Amazon started off selling books from a single workstation computer that beeped whenever an order came in. Demand scales up over time and only <em>after</em> you have a great product deployed. If it is a great business idea and a compelling product then a few hiccups as you grow and scale the architecture will be fine.</p>
<p>PostgreSQL is a traditional relational database. It now does binary JSON storage where you can index into the JSON fields. It also supports geospatial indexes. It also supports free-text searching. It is very easy to run locally to develop against. Better yet major cloud providers support it with lots of automation, high-availability, automated backup/restore, point-in-time restores, and monitoring dashboards. <a href=""https://aws.amazon.com/rds/postgresql/"" rel=""nofollow noreferrer"">Amazon RDS lets you run PostgreSQL</a> and they can seriously scale it up for you with a few clicks. You can start very cheaply and then add HA easily later then scale up to bigger and bigger database servers as you grow. That gives you time to then fix the real performance bottlenecks rather than guessed problems.</p>
<p>You can start with an edge microservice that will load-shed rather than do too many writes into PostgreSQL. The client can buffer the write and try again later. Then start off with simple code that does all of the writes against PostgreSQL for the main document and geospatial indexing. Spend the engineering effort on the frontend and business features. Later you can put RabbitMQ between the edge microservice and PostgreSQL. Then you can either break up or swap out PostgreSQL.</p>
<p>At a later date, you might create a separate geospatial index in Elastic Search. Later yet you might choose to move actual documents out of Postgres and into ScyllaDB. Do we know which things we should do in which order today? No, we cannot. Instead plan to evolve the architecture. Maybe just splitting from one PostgreSQL server into three servers, one dedicated to each of geospatial indexing, binary json, and free text search might work? That sounds like a great intermediate step before swapping in Elastic Search or ScyllaDB. We don't know but we can be flexible.</p>
"
418151,"<p>I have a monolithic application which can be divided into multiple steps and different steps have variable scaling requirement, however the input of the next step is the output of the current step. Currently, we are not able to handle the no of requests to our server and we need to have more servers/load balancer/etc. We are also thinking to re-architect our application. If we create separate services (for those steps) and deploy them as containerized application using Docker &amp; Kubernetes on cloud and use some distributed message broker (Queue) to pass the results from previous step service to next step service, we would be implementing a sort of microservices architecture. The only concern which I feel is that if Service1 container instance and service2 container instances are in different servers/hosts, then there would be some network latency which would be multiplied by the no. of requests.</p>
<p>So based upon my understanding the microservices architecture is not a good candidate for pipeline kind of requirement, if we are looking for real time performance. It will be better to keep those step based services in the same server and may be control the amount of resources which can be used by those services i.e. allocating more resources to service which needs them more and then we can auto scale the whole server based upon load. We can have in-memory queues between those services. Do we have a software which can help in dynamically allocating more resources to a service if the no. of items in their queue is high?</p>
"
415998,"<p>So... The architecture has recently been put under the reign of a Reference Architect.
The Reference architect, I will refer him as RA, started to work and the results were immediately visible: we stopped calling microservices microservices, but blocks.</p>
<p>As we used to be a mixed Java/Linux and C#/.net shop things were working pretty well until the RA issued a decree that no more development shall be done on Java and Linux. We tried to explain that when using microservices (ouch... blocks) the implementation stack is nor very relevant and having multiple stacks gives us more opportunities as there is room for evolution and the costs of Linux stacks is generally lower in cloud the RA sent a cease and desist memo that urged us to go to his favourite vendor.</p>
<p>Apart from the obvious cost and quality related arguments that can be invoked what other could be used so that we can make a case for keeping our code. There is lot of legacy and porting it to a new platform would not bring any business value but a decrease in quality and huge delays.</p>
<p>The arguments used so far are</p>
<ul>
<li>TCO - increased costs due to licensing</li>
<li>ROI - when a break even will be available</li>
<li>talent availability</li>
<li>domain knowledge - this is already captured</li>
<li>deployment independence</li>
<li>testing effort</li>
<li>training effort</li>
</ul>
<p>What am I missing in order to make a compelling case against a single stack? In a microservice architecture shouldn't the architect be focused more on delivering value, having clear interfaces and mechanisms than on concrete implementations? Microservices could evolve independently and choose their own stack for functional and non-functional reasons, forcing everyone into conformity would lead to degeneration and brittleness. Of course that a single stack seems more economical and promises code reuse, but as far as I know the business domains are pretty different in the two worlds so anyways reuse will be hard.</p>
"
411767,"<p>As Jörg W Mittag mentioned there is the legal aspect of what you are talking about, and then the technical.  As long as the app embeds critical logic and database access inside of it, someone with enough patience can reverse engineer and do the bad things you are talking about.  There are different approaches you can take to protect your efforts:</p>
<ul>
<li>Use Digital Rights Management (DRM) to protect the app--still can be defeated but harder to do</li>
<li>Use a code obfuscator which has the ability to make it harder to reverse engineer the code</li>
<li>Encrypt the module that does the critical access (you have to decrypt it when you load it in memory)</li>
<li>Move all critical behavior to services hosted remotely (like in the cloud)</li>
</ul>
<p>None of these solutions are mutually exclusive, but the one that provides the best protection is to move your database access and critical business logic to a service oriented architecture (i.e. web services you control).  That way it is never part of your app to begin with, and then none of the code you are worried about is even available for someone to reverse engineer.</p>
<p>It also means you are free to change how that information is stored and managed without having to release a new version of the app.  Of course, you'll have to provide the appropriate protection to make sure that a user can only see or interact with their own data, but now you don't have to worry about the app being hacked.</p>
<p>Many apps are built this way now.  The app communicates with servers via HTTP with JSON, YAML, Protobuf, BSon, or some other structured exchange format.  The app authenticates to get a session token that is good for a few minutes at a time, and that token is presented to your service so you don't have to worry about server side sessions.</p>
"
409372,"<p>Technically what you are suggesting is very feasible.  It's something that is quite common, in fact.  In the past, there were limited options on sizing a machine and therefore it was necessary to do this in order to use computing resources efficiently.  There are a lot of challenges, however, which largely explain how we got to where we are with virtual machines, containers, and serverless architectures.  AS others have already noted, trying to manage the capacity with regard to load will be complicated and you will end up needing to oversize your VM to meet peak loads.</p>

<p>Since you mention AWS, you should understand that a lot of the options available there are designed to help avoid theses challenges.  The design you mention fits very well into containers e.g. a Kubernetes pod.  There are many options to allow for scaling under heavy loads while avoiding paying for the extra capacity when you don't need it.  You should probably also look into lambdas which cost nothing when not actively in use.</p>

<p>So yes, you can do this but it's a pretty out-dated approach.  Given the options you have available to you, there doesn't seem to be a good reason to go this route.</p>
"
406350,"<p>I think it is important to understand what you get with microservices, why infrastructure is needed, and what the cost of microservices are.  That will help inform you if your approach is reasonable, particularly with the goal of unifying to completely different applications.</p>

<p>First, a summary of the trade-offs:</p>

<ul>
<li>You have the <em>opportunity</em> to <strong>minimize the blast radius</strong> if a service goes offline (i.e. impact to the user)</li>
<li>You have the <em>opportunity</em> to <strong>scale out</strong> dynamically based on load and demand</li>
<li>You have more moving parts so <strong>deployments are more complex</strong></li>
<li>You have the need to centralize how <strong>configuration</strong> works</li>
</ul>

<p>I want to emphasize the word ""opportunity"" because there are restrictions to make those opportunities a reality.  Namely that you don't use server side sessions at all.  If you have to maintain state, either do it in the HTML/JavaScript or do it in the database.  You have to simplify the ability to load balance all the instances of a service, and storing information in memory on the server side works against that.</p>

<p>That said, we start to see <em>why</em> discovery, configuration, and an API Gateway are necessary.  All of these can be managed in different ways.  The core cloud infrastructure pieces are needed precisely because of microservices.</p>

<ul>
<li><strong>Discovery:</strong> enables you to find all instances of a service by name.  This can be accomplished by using Eureka (Spring Cloud infrastructure), Kubernetes Services, or an App Mesh</li>
<li><strong>API Gateway:</strong> a reverse proxy that will map URLs to specific microservices on the backend--providing load balancing across the services.  This can be accomplished by Zuul or SC Gateway (Spring Cloud infrastructure), Kubernetes Ingress service, or an App Mesh</li>
<li><strong>Configuration:</strong> a means of managing configuration and pushing updates to the running services.  This can be accomplished by Spring Config Server (Spring Cloud infrastructure), Kubernetes ConfigMaps and Secrets, or an App Mesh.</li>
<li><strong>Central Log Management:</strong> is required to understand how your application is behaving in the wild.  Whether you use Elastic Stack, Splunk, Data Dog, or a cloud broker provided service, you will be better equipped to diagnose problems that span multiple microservices.</li>
</ul>

<p>Before you say ""I'll use App Mesh"" understand that app mesh is a suite of technologies that integrate with an orchestration layer (like Kubernetes) underneath of it.  You have to understand the concepts of the App Mesh and the underlying technology.  The learning curve very well may be something you don't have the time to take on right now.</p>

<p><strong>How To Migrate to Microservices</strong></p>

<p>The pragmatist in me likes the <a href=""https://dzone.com/articles/monolith-to-microservices-using-the-strangler-patt"" rel=""nofollow noreferrer"">""strangler pattern""</a>.  Essentially, you migrate out specific functionality into its own microservice, and adapt the existing application(s) to leverage that microservice instead of the old way it used to deal with things.  For example, separating out the authentication/authorization functionality into a unified service so that the user's token has all the attributes needed encoded inside of it (see JWT.io) you can minimize service-to-service communication to verify permissions.  That would provide you single-sign-on for your application suite.</p>

<p>But while you are in the design phase, also consider the build vs. buy question.  If you are hosting in the cloud, check your provider if they have services that do everything you need.  For example, single-sign-on is a common need and all the major providers have their version of SSO support.</p>

<p><strong>Key Takeaway</strong></p>

<p>Microservices are a powerful way to manage application deployments.  If you are smart about how you design them, you can minimize your cloud expenditures.  I.e. taking advantage of managed services hides the actual compute instances you are leveraging.  The cloud provider handles that so support the SLAs and minimize the cost to the provider, and you get a lower cost solution.  Having a dynamic deployment allows to you take advantage of spot instances for surge activity.</p>

<p>Putting both of your applications behind a common API Gateway does not magically give you single-sign-on.  Nor does it give you any of the real benefits of a microservices architecture.  However, <strong>it is a place to start</strong>.  You can expand on this initial investment over time.  Even the Amazon store front had to evolve from a monolithic application to microservices.  That took time, and it took time learning how the different architectures helped the application scale and minimize the cost of running it.  That is a journey you will have to take if this is the direction your company wants to go.</p>
"
405176,"<p>If you need high availability where one minute of downtime is not acceptable a single cloud provider is not enough. You need multiple providers to have high availability at that  level, even then it's still a matter of hoping any issues don't affect multiple providers at the same time. You also need internal processes and procedures in place that are far more challenging and demanding than choice of cloud providers. You also need to ensure anything supporting the highly available module is highly available itself in most cases.</p>

<p>When faced with the price tag for true high availability most organizations discover they really don't need high availability. Once a real cost benefit analysis is done, downtime tends to not look so bad. The less downtime acceptable the more your costs to insure that happens increases and that scale is exponential in nature. Accepting an hour of downtime a year only costs $D, but a few minutes of downtime a year is going to cost 10-20 times more, the price paid to prevent losses can quickly eclipse the actual losses from downtime.</p>

<p>To give you an idea of just how extreme avoiding one minute of downtime is, an SLA for 99.9% up-time still allows for a minute of downtime per day. For a 99.99% SLA a minute of downtime on a weekly basis is acceptable, and a 99.999% is five minutes on a yearly basis. It's very easy to have all sorts of SLAs with what look like impressive numbers, but a minute of downtime is an extremely short window. Everything has to be automated to maintain that level of up time, you need to detect and mitigate issues without human interaction. <a href=""https://cloud.google.com/appengine/sla"" rel=""nofollow noreferrer"">App Engine only offers a default SLA of 99.95%</a> which wouldn't meet your needs alone if one minute of downtime is an issue.</p>
"
403459,"<p>The <a href=""https://en.m.wikipedia.org/wiki/Law_of_the_instrument"" rel=""nofollow noreferrer"">law of the instrument</a> tells us that:</p>

<blockquote>
  <p>If you have a hammer in your hand, every problem starts to look like a
  nail.</p>
</blockquote>

<p>So before jumping too quickly to a solution look objectively at the needs and requirements. In particular security
needs, access control, and transactional consistency requirements. </p>

<p><strong>Blob in DB:</strong></p>

<ul>
<li><p>Pros:</p>

<ul>
<li>benefits from the access security of your database, as well as its backup strategy</li>
<li>confidentiality against unauthorized third party access (e.g. if you’re in governement or military affairs)</li>
<li>simplification of architecture due to homogeneous access</li>
<li>reduction in the external bandwidth if db is on premises and if you have lots of internal users. </li>
<li>blob is managed with transactional integrity, as the remaining data</li>
</ul></li>
<li><p>smaller text blobs could fit into text fields (eg up to 1GB in Postgresql), allowing to use content in queries.</p></li>
<li><p>Cons:</p>

<ul>
<li>technical limits of the db (eg 1GB on Postgresql, a little less than 2GB on SqlServer, 4GB on Oracle, support only via GridFS in MongoDb)</li>
<li>significantly increases size of the db </li>
<li>increase operational cost of the db, especially if storage goes significantly beyond initial assumptions.</li>
<li>waste of expensive infrastructure (<a href=""https://www.google.be/amp/s/searchstorage.techtarget.com/definition/Tier-1-storage%3Famp=1"" rel=""nofollow noreferrer"">tier 1 disk space</a> meant for heavily used db data but misused to store objects that are not frequently accessed and could go to tier 2 or 3). </li>
</ul></li>
</ul>

<p><strong>Blob in an object store</strong></p>

<ul>
<li><p>Pros:</p>

<ul>
<li>very effective and specialised solutions on the market</li>
<li>cost effective </li>
<li>easy to set up</li>
<li>resilient (depending on the offer), with backup included </li>
</ul></li>
<li><p>Cons:</p>

<ul>
<li>accessibility risks if mission critical application (e.g. in case of denial of service attack on the provider or on your internet access point)</li>
<li>confidentiality risk (but IMHO this only applies to highly sensitive and classified information in governmental and military affairs)</li>
<li>additional access security may be needed ( eg AWS identity management and acl management)</li>
<li>you need to take care of transactional consistency aspects (e.g. what to do if a blob update fails, regarding the related db changes)</li>
</ul></li>
</ul>

<p>Your current context may influence this evaluation. For example, if you already have some cloud services and identity management active, this will no longer be a cons. Or if your current db has a very small limit for blobs (eg Mongodb) many of the object store cons will not be relevant for you since you’ll have to care for this anyhow.</p>
"
403412,"<p>The big things to address are:</p>

<ul>
<li><strong>non-repudiation:</strong> the user should be unable to deny that the actions are theirs.  That means that the identity cannot be easily stolen by someone, and that the token provided can be validated that it is correct.</li>
<li><strong>auditable:</strong> you need to be able to determine if any users are behaving badly, and terminate access if so.</li>
<li><strong>controllable:</strong> that means that you can impose rate limits, reduce access privileges if necessary, etc.</li>
<li><strong>enforceable:</strong> or the authorization for a user to be able to perform an action is enforced.</li>
</ul>

<p>To that end, there are multiple layers, and each of those technologies have a portion of the whole security posture in place.  The good thing about OAuth is that it is standards based, making it easier for systems to build integrations.  However, you can provide all those guarantees without using it if you really don't want to.</p>

<p><strong>The API Key</strong></p>

<p>By itself, the API key can be easily sniffed out and stolen if you don't use <strong>encrypted</strong> communications.  That said, blindly using an API key also isn't the right solution.  It's easy, but if there is any layer in the communications which exposes the key, it's also easy to impersonate.</p>

<p>At the very least you want to make sure that your API key is <em>verifiable</em>.  OAuth does this by supplying the external system with a <strong>client id</strong>, and a <strong>secret key</strong>.  There are specific rules for providing the client id and secret key, including the rule to encrypt communications.</p>

<p>With OAuth, the keys provided by the external system are used to <strong>negotiate</strong> a <em>session token</em>.  The session token is then provided in the <code>Authentication</code> HTTP header as a bearer token.  Nine times out of ten, the session token is <em>JWT</em>.</p>

<p><strong>OAuth2</strong></p>

<p>OAuth provides a very robust foundation to verify the end user, and provide a limited use session token for all further communications.  If you are the provider generating the session token, then you can use JWT to embed the information your application needs to not only identify the user, but embed their roles and any other attributes you use to decide access.  I do recommend signing the token.</p>

<p><strong>DB for authentication access</strong></p>

<p>This is an implementation detail.  You can use a managed service like AWS Cognito, or Atlassian Crowd to handle all the nitty gritty details, and to keep up with 3rd party identity management solutions.  Cognito has the added benefit of having control over the JWT session token that is generated for your purposes.</p>

<p><strong>AWS API Gateway</strong></p>

<p>The biggest benefit that API Gateway gives you is the ability to rate-limit API calls.  If you are using AWS Cognito, that can associate your AIM roles with the user's token automatically, and that follows through the API Gateway.  The AIM roles can be used to control which buckets you have access to, etc.</p>

<p>However, I don't see the Gateway as an enforcement layer.  I see it more as a traffic control layer to help scale your application.</p>

<p><strong>I'm trying not to be too prescriptive</strong></p>

<p>As long as you have the traits discussed at the beginning of my answer, you will have done a good job securing your application.  There are good reasons why OAuth2 with JWT session tokens are a common solution.  They pretty much tick off the majority of your access control needs.</p>
"
403152,"<p>You should probably look over this: <a href=""https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html"" rel=""nofollow noreferrer"">AWS lambda best practices</a>.  The first bullet is one thing you should definitely consider: </p>

<blockquote>
  <p>Initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves execution time and cost.</p>
</blockquote>

<p>Initializing a DB connection is typically one of the most expensive parts of working with a DB.  The above is good advice but if you are running lots of concurrent instances, I would expect you need a separate connection for each one e.g. connections are typically not threadsafe in Java.  A thousand connections all working on the same tables could cause contention on the DB and actually result in a slower overall execution time i.e. what I call the 'Stooges' problem.  When all 3 Stooges all try to go through the door at the same time, the throughput is less than if they go through one at a time.</p>

<p>A middle path would be to pass in a list of ids up to a certain limit or split the ids across a fixed number of instances.  This allows for control over the amount of concurrency.</p>
"
403071,"<p>The basic concepts are orthogonal, however, they are related.  One has to do with the availability of your application, and the other has to do with the correctness of your application.  Remember, there are differing levels of faults (also known as bugs).</p>

<p><strong>Fault Tolerance</strong></p>

<blockquote>
  <p>""Everything fails, all the time"" -- Werner Vogels, Amazon CTO</p>
</blockquote>

<p>When you design for fault tolerance, you are addressing issues such as:</p>

<ul>
<li>What happens if my service can't access required resources (database, storage, message queues, etc.)</li>
<li>What happens if I receive bad data?</li>
<li>How should I handle another service taking too long to respond?</li>
<li>What performance criteria do I have to meet?</li>
</ul>

<p>There are other related questions you have to ask yourself.  It's important to realize that there are a lot of modes where your code can fail, and the longer it is running the more likely it is to hit a soft failure scenario.</p>

<p>Not all failures cause your code to crash, but avoiding crashes is a big reason that fault tolerant systems are also more available.</p>

<p><strong>High Availability</strong></p>

<p>Ensuring your software is available for use means that users should not inconvenienced when other people are also using the system.  There is a direct relationship to the cost of running a system and its availability.  Quite simply, it costs more to be generally available.</p>

<p>To be available means you need contingencies for when things outside of your code's control are causing problems:</p>

<ul>
<li>How do I scale my application?</li>
<li>How quickly can I scale the application?</li>
<li>What happens when my data center is not available?</li>
<li>What happens when the network gets saturated?</li>
<li>How should I respond if my data center is a victim of disaster (natural disaster and war can cause you to have to change data centers)</li>
<li>How much will this cost me to run?</li>
</ul>

<p>You can start to see why microservices and the cloud are key components of engineering for the internet.  The idea is that you can scale out (more instances) much more quickly and cheaply than scaling up (more CPU and RAM).  Additionally, you can scale back down just as easily when the spike in traffic is over to save you some money when traffic is slower.</p>

<p>Of course you need to design for this as well.  That may require working with data partitioning, multi-region deployments, multi-availability-zone redundancy, etc.  Of course, if this is running on the user's hardware (i.e. a desktop application), then you may need to swap modules in and out of memory depending on whether you are using them.</p>

<p><strong>End Game</strong></p>

<p>You need both of these disciplines to engineer something that can scale smoothly as demand for your application grows.  You have to start making your best guess at what the appropriate problems you are most likely to face now, and then monitor your application while it is running.  That will show you where your current design is having a hard time keeping up with demand, reliably.</p>

<p>I guarantee you that none of the big systems people expect to rely on started where they are now.  You can look at Twitter, Facebook, Airbnb, Netflix, and even this site and find that each of their architectures are different even though they share some commonalities.  That's because the different ways those systems are used required different customization.  What's more telling is the decision process of how they got there.  Most of us are not going to engineer something so large as those big names in the internet, but we can learn from the decisions they made and apply it at a smaller scale.</p>
"
402967,"<p>There's a couple key concepts in here that will limit your ability to scale:</p>

<ul>
<li>Monolithic design</li>
<li>Combinatoric set of artifacts per user</li>
<li>Serialized updates to database</li>
</ul>

<p>Of those, the first two are the most costly.  Tools like Sagemaker allow you to create models.  The most compute intensive part of the process is training those models with your data.  Once you've trained your model, you can deploy it and use that trained model for the live system.  Usually it uses far fewer resources to run a trained model.</p>

<p>There's a few things I recommend:</p>

<ul>
<li>Split the monolithic design if possible so you can create a processing pipeline</li>
<li>Use temporary storage like Redis to store intermediate results until you are ready to store finished products</li>
<li>Use a message queue like Rabbit or Kafka to push processing to the next stage in the pipeline (look at Apache NiFi or Spring Cloud Data Flow to model the pipeline itself)</li>
<li>Leverage GPUs for parallel processing if at all possible.  AWS has EC2 instances with GPU available and some machine learning algorithms work much faster in GPUs</li>
<li>Attempt to batch save your results to your permanent data store (i.e. Mongo in this case)</li>
</ul>

<p>Another thing to consider is using a managed instance of MondoDB.  The Mongo team were really advertising that service at AWS re:Invent 2019.  It allows them to worry about scaling and backing up the Mongo cluster and you to worry about how you use it.</p>

<p>I mentioned Apache NiFi and Spring Cloud Data Flow above.  What they allow you to do is specify your workflow, and they will manage moving messages from one step to the next.  They can also manage your container instances so that only the processing steps that need to be scaled will be.  And when the demand is over, the extra containers will be reclaimed.</p>

<p>I believe AWS has some support for pipelines as well, using Lambda functions to route work.</p>

<p>Last concern here is the terms of service you are trying to support.  If the models you are building are expected to take a long time, you can serialize the processing somewhat.  If processing one user at a time is within acceptable limits, then attempt to interleave requests so that by the end the processing stays within acceptable limits but you get your users satisfied as soon as you can.  You'll still probably need some more capacity, but you might be able to service more users on the same amount of hardware.</p>

<p>As far as automatic scaling, you are looking at a couple technologies:</p>

<ul>
<li>If you are containerized, Kubernetes lets you set policies to automatically scale pods (containers) up and down depending on demand.  There are several advanced features to target certain containers for specific processing nodes</li>
<li>AWS Auto Scaling will perform a similar function at the EC2 instance level.  Combine it with Cloud Formation (infrastructure as code) and you have a very robust autoscaling system in place without containers.</li>
</ul>

<p>What's not clear here is if you are generating learning models for your customers, or running models you already have against customer data.  There are significant details that change in your architecture depending on the answer to that.</p>
"
402623,"<p>A structured approach to improving performance often involves</p>

<ol>
<li>Profiling hot spots, for example high CPU usage.</li>
<li>Profiling wait times. You could include a correlation ID to send through the various systems to keep track on what gets processed when and where.</li>
</ol>

<p>It may be hard to get the full picture running this on the cloud.</p>

<p>Personally, I would say an async event driven model is a mismatch when the user is waiting for a response. Google has an <a href=""https://developers.google.com/web/fundamentals/performance/rail"" rel=""nofollow noreferrer"">excellent article</a> on what the user's expectations potentially may be. The gist of it is, up to 100ms is an immediate response, up to 1000ms is still good when the user perceives there is a task being executed. I can't see from your model what kinds of operations you are performing (and the boundaries may be somewhat fluid, some users understand searching all your e-mail is work and some won't)</p>

<p>Azure Event Hubs is a big data pipeline - likely a microbatching architecture. Microbatching tends to have higher latencies.</p>

<p>Thinking about how you scale up on an event stream - Microsoft is not super specific on how the <a href=""https://docs.microsoft.com/de-de/azure/azure-functions/functions-scale"" rel=""nofollow noreferrer"">auto scale</a> works... and the data will be opaque to you the operator. On the other hand, if you run a request/response pattern fronted by an API gateway, the API gateway's precise function can be to monitor service levels on the various calls.</p>
"
392697,"<p><strong>(Important Preliminary Consideration)</strong> Before attempting to answer the questions you asked, I would highly encourage you to consider renting a high-memory machine for the ML modeling tasks if you're already in the cloud and talking about batch jobs. A 500GB RAM box can be had for $5/hr these days, and it'll be worth every penny (if you have little enough data that such a machine suffices -- anything after 1-6TB RAM is probably pushing the limits of cost-effectiveness, and you'd probably be better off with a distributed system).</p>

<p><strong>(The questions you asked)</strong> I'll focus on your last three bullet points here:</p>

<ol>
<li>It depends. If querying the production DB direction makes sense in your business and if the ML pipeline is closer to a streaming model so that each piece of data is touched few times then you might not need/want any layers between the DB and the ML batch processors. That said, lakes/warehouses/replicas and whatnot are common to isolate customers from the potential performance degradation from analytics. As to the performance question though, to a reasonable order of approximation the difference between ""close postgres"" and ""far postgres"" doesn't really matter unless you have synchronous round trips (and if you do...don't do that). There's a much bigger jump from ""close postgres"" to ""local memory"", and trying to frame the problem in a way so that you can load a blob of data into each ML machine, operate on the blob, and then aggregate the results with little to no back-and-forth will <em>probably</em> perform better than a solution involving a lot of shared state in a nearby database. I wouldn't be inclined to spin such a thing up for performance reasons (though exceptions obviously exist).</li>
<li>It depends. The <em>inferential</em> pipeline absolutely needs to live in production. The <em>model building</em> pipeline can live anywhere. If your model auto-updates in response to new data I would propose it's easiest for the <em>model building</em> pipeline to also live in production. Otherwise it depends on other factors like how often you change models, how good your tests and metrics are, how critical time-sensitivity is, how static the data is, how critical accuracy is, the compute resources needed to train a model, your model version control process, and so on. If your model doesn't auto-update then where you train it doesn't matter all that much in most scenarios.</li>
<li>Yes. People automate re-training all the time. There are even algorithms designed to auto-update in response to new data. Good performance metrics and monitoring are mostly non-negotiable in an auto-updating world.</li>
</ol>

<p><strong>(Desired Architectural Qualities)</strong> Unfortunately there isn't one correct answer to your question. The unique characteristics of the production system, the data you're working on, the ML models you're considering, and the dynamics of the underlying business will push you toward different solutions. Here are a few questions that might help point you to a good solution for your use case. Getting a good handle on requirements will allow you to take an informed and principled approach to choosing between different solutions:</p>

<ol>
<li>Will the production database suffice for your ML pipeline?

<ol>
<li>Is there a high availability assumption for the DB's other clients?</li>
<li>Can the machine handle the additional analytics load?</li>
<li>Does the data already easily map via SQL to a form the ML pipeline can handle?</li>
<li>Is the data especially time-sensitive?</li>
</ol></li>
<li>Do the ML pipelines need to be versioned and stored?

<ol>
<li>Do you need to be able to roll back pipeline deployments?</li>
<li>Do you want to be able to analyze the pipeline's progress over time?</li>
</ol></li>
<li>Should the ML pipeline be pre-trained or automatically trained?

<ol>
<li>Is the data naturally incremental and read-only (e.g. most time-series data), or does the underlying model have a lot of updates and deletes?</li>
<li>How problematic is it if the ML pipeline performs poorly?

<ol>
<li>Consider various facets question of this including false positives, false negatives, implicit bias, worst-case scenarios, median-case scenarios, and so on.</li>
</ol></li>
<li>Can you detect and respond to a poorly performing model?</li>
<li>Is the data sufficiently static that the model can change slowly or never?</li>
<li>Do the questions being asked naturally map to a model designed to adapt to new data?</li>
<li>Is deployment easy enough in your shop that you can easily switch to/from automatic/manual training if one approach doesn't work well?</li>
</ol></li>
<li>If an additional data store is needed, where should it be located?

<ol>
<li>What are the data ingress/egress rates with your cloud provider?</li>
<li>Are there ingress/egress exclusions when you stay in the provider's network?</li>
<li>Does the provider offer everything you need to build the rest of the ML pipeline?</li>
<li>Do you have the means to locally (with a broad definition of ""local"") host any portion of the system?</li>
</ol></li>
<li>Should the ML pipeline even be hand-built in the first place?

<ol>
<li>Can the organization afford the ongoing hardware/software/data cost?</li>
<li>Could the organization afford options like Google AutoML?</li>
<li>Is the data especially secure or sensitive?</li>
</ol></li>
</ol>
"
391902,"<p>My users will be businesses with a small number of accounts each: e.g. Business #1 with 3 users, Business #2 with 5 users, etc.</p>

<p>I am trying to determine the best way to organise the relation (on Postgresql, hosted on AWS) for each business client: for e.g. each business will have their own sets of <code>customers</code>, <code>logs</code>, <code>products</code>, <code>config</code> relations.</p>

<p>I want to create a scaleable database across all client businesses.</p>

<ul>
<li>Do I create a <em>single</em> relation, e.g. of <code>customers</code>, for <em>all</em> my clients? E.g. each business client is given a unique ID, and during onboarding of the business user, the user's account is tied to the specific business UID. Examples in 2 images below.</li>
</ul>

<p><a href=""https://i.sstatic.net/OIPbL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OIPbL.png"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://i.sstatic.net/tzXSc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tzXSc.png"" alt=""enter image description here""></a></p>

<p>This seems like a huge security risk given that all business information is stored in a single table/DB.</p>

<ul>
<li><strong>OR</strong>, do I create individual databases hosted separately in AWS for each business? This seems safe, but also practically not scaleable and expensive!</li>
</ul>

<p>I'm pretty sure there's a middle path where all B2B apps tread which my inexperience simply makes me oblivious to. </p>

<p>I've read <a href=""https://softwareengineering.stackexchange.com/questions/260798/500-databases-or-1-database-with-500-tables-or-just-1-table-with-all-the-records?rq=1"">this</a> and <a href=""https://softwareengineering.stackexchange.com/questions/141261/multi-tenancy-single-database-vs-multiple-database"">this</a>. My question is more directed to the <em>implementation</em> of the table if I were to go with a shared DB and shared schema.</p>
"
388892,"<p>If we consider the cache to be <a href=""http://www.catb.org/~esr/writings/taoup/html/ch04s02.html#orthogonality"" rel=""nofollow noreferrer"">orthogonal</a> to the architecture (and it's), the first pic is ok. </p>

<p>For the same reason that we don't deploy one security service, one API gateway, one message broker or one service locator per POD we don't have to deploy one Cache (in replica-set) per POD.<sup>1,2</sup></p>

<h3>Be aware of premature optimizations</h3>

<p>Caches are meant to solve specific performance issues. Mainly those derived from costly <a href=""https://en.wikipedia.org/wiki/Inter-process_communication"" rel=""nofollow noreferrer"">IPCs</a> or heavy calculations. Without evidence of any of these issues, deploying caches (in replica-set) per POD for the sake of the ""MS' God"", is premature optimization (among other things).</p>

<h3>The <em>Cloud</em> can kill you.</h3>

<p>If our goal is to deploy the architecture in the cloud, the smart move would be to start small. Be conservative. Scaling up|out as the needs come because the contrary is oversizing the architecture. Oversized architectures are potentially dangerous in the cloud because they can, literally, kill the project devouring the ROI in no time. <sup>3</sup></p>

<h3>Size the solution according to the problem</h3>

<p>Perform load tests first, get metrics and shreds of evidence of performance issues, find out whether caches are the solution to these issues. Then, size the solution according to the problem. If and only if services are proven to need a dedicated cache, deploy them. By the time you do it, you do it uppon objective metrics and keeping the bills under control.</p>

<p>I was told once</p>

<blockquote>
  <p>If the solution is more complex or more expensive than the problem it
  solves, then it's not a profitable solution. <strong>It's not even a solution!</strong></p>
</blockquote>

<p><sup>Emphasis mine</sup></p>

<h3>Keep complexity at bay</h3>

<p>MS architectures are complex per se, they don't need us adding more complexity for the sake of dogmas or beliefs we barely understand. Keep the overall complexity of the system as lower as possible but not lower. In other words, keep it simple but not at the cost of defeating the purpose of the architecture (total freedom of deployment and SDLC).</p>

<hr>

<p><sup>
1: I'm assuming that every POD is a replica of the same Service, not different services.
</sup></p>

<p><sup>
2: MS architectures are not about many but small systems, it's about a single system composed by business ""capabilities"", working all together for a greater good.
</sup></p>

<p><sup>
3: Cloud is anything but cheap. Particularly, when it comes to buy RAM
</sup></p>
"
386307,"<p>I like your proposed solution, but I would add that maybe a serverless architecture would suit you well. Did you take a look at Azure Functions? They can be triggered by some events like the upload of file to Blob Storage. <a href=""https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function"" rel=""nofollow noreferrer"">See here</a>.</p>

<p>The thing is that with a serverless architecture for your service, you will have a auto scalable solution, as well as less operation tasks (managing servers).</p>
"
385346,"<p>I currently build applications that are fairly monolithic. I have a one or many code bases that compile into one single binary/package and deployed on a cluster of docker containers. All of the data is stored on a single MySQL database, a redis cluster and possibly a NoSQL database for some of the data.</p>

<p>In this case, the bulk of my data is stored in an MariaDB RDS instance on Amazon Web Services. This works fairly well because RDS handles automated backups, and other benefits.</p>

<p>However, let's say that I want to split a service into its own ""microservice"". Where would it store its data? If I have 5 microservices, spinning up 5 RDS instances, 5 redis clusters doesn't seem to be the most cost effective and seems to be a lot of management overhead.</p>

<p>It seems to me that a cluster of docker containers would be more manageable for a single microservice. For example using something like docker-compose, you can spin up several docker images into as a single unit very easily. AWS has a similar concept called ""Task Definitions"" (to my knowledge) which you can launch on AWS Fargate or ECS. However, Fargate does not allow persistent storage, so you are basically forced to launch your database in something like RDS.</p>

<p>I suppose this is a fairly open ended question, and might pertain to DevOps more than actual Software engineering. How would someone design a microservice to be easily deployed on the cloud whilst being easily maintained as a separate but packaged unit of sub-services (app server, databases, cache, etc)? Using docker compose and amazon Task definitions seems to be the best way to keep consistency between development/staging/production environments, however it does have some limitations such as not having persistent storage on Fargate for example.</p>

<p>Just looking for examples on how someone might achieve this to help my understanding.</p>
"
384682,"<p>According to the <a href=""https://aws.amazon.com/sqs/features/"" rel=""nofollow noreferrer"">AWS feature page for SQS</a>:</p>
<blockquote>
<p>FIFO queues support up to 300 messages per second</p>
<p>Standard queues support a nearly unlimited number of transactions per second (TPS) per API action.</p>
</blockquote>
<p>I'm trying to build a system that will add notifications to a queue that will then be sent to a customers device using push notifications (SMS, APN, webhooks, email, etc).</p>
<p>There will be a Lambda function that will read items off this queue and actually handle sending the message to the user.</p>
<p>Problem is, I'd like this system to be able to scale as efficiently as possible. Being constrained to 300 notifications per second might cause problems in the future. So I want to design this in a way that is much more scalable than that.</p>
<p>I have thought about building some type of system that will use a standard queue then check to see if that notification has already been sent by having a database that stores the ID of notifications that have been sent. Which might work. But at that point I think I'd be opening the door for race conditions. What happens if for the same notification two Lambda functions got triggered at the exact same time? Neither of them have been sent yet. And the user will send up with 2 notifications instead of one.</p>
<hr />
<p>How can I design a system that has the best of both worlds? <code>nearly unlimited number of transactions per second</code> while ensuring that no duplicate notifications are sent.</p>
<p>I don't think I mind quite as much if a Lambda function gets triggered twice for 1 notification, so long as it doesn't get sent multiple times to the user. Of course if I can completely prevent this, that'd be awesome too, so that I can reduce cost.</p>
<hr />
<p>I'd also love to keep using AWS and the more serverless technologies of AWS if possible. I know there is software and ways I could provision EC2 or other types of instances for this. But that takes out the huge advantage of serverless, which is what I'm really aiming for.</p>
"
382785,"<p>My suggestion to you is to start from something very simple and evolve.
Try AWS Lambda + API Gateway for start.
Very simple start, auto scaling up to 1000 concurrent executions per region.
If you will need more concurrent executions you can think about multi region load balancing. Or try to enlarge quota by opening request.
The question is pricing. can be expensive. 
Consider caching results if same documents used.</p>

<p>More complex arch :</p>

<ol>
<li>Upload document to s3. (Only after upload finished call Lambda with doc's name)</li>
<li>Call Lambda to analyze the document in s3. (After, delete Doc, storing hash of the doc with result in cache)</li>
<li>Return result. </li>
</ol>

<p>This is more wise architecture from different perspectives.</p>

<ul>
<li>Scales very good for multiple clients for s3 upload.</li>
<li>Latency Lambda &lt;-> S3 much better</li>
<li>Simple to develop</li>
<li>pricing</li>
</ul>
"
382607,"<p>In the case of displaying and editing the blog entries, the concept of separating the responsibilities of edit and read make perfect sense:</p>

<ul>
<li>Separate microservices allow each to scale appropriately</li>
<li>The only thing that needs to be understood between them is how you are persisting the data</li>
</ul>

<p>From there you can decide what infrastructure pieces you need between them.  From a cost/performance standpoint, you won't get any better than your cloud blob storage.  Of course, blob storage doesn't help with searching and querying, but it serves up resources very fast and you don't have to do anything special to make it scale.</p>

<p>If you offload all the rendering to the client (i.e. a Single Page App), then you only need to pass data back and forth.  That leaves your search/query capability which might be better served by ElasticSearch or Apache Solr.  An asynchronous task can handle updating your search engine, further decoupling things.</p>

<p>That said, with building for internet scale, you want to minimize the points of contention and share nothing if at all possible.  If after all that you feel you still need the Redis cache, you will be better armed to understand where it needs to be included.</p>

<p>Similar Bottom line:</p>

<ul>
<li>Think about the architecture and how you can solve your problems there first.</li>
<li>Eliminate sharing if you can.</li>
<li>There are no simple answers.  I don't know enough about you are doing and why you've chosen the tech stack you have to be any more meaningful in my answer.</li>
</ul>

<hr>

<p>I think that CQRS and Redis are solving 2 different problems and are not necessarily mutually exclusive concepts.  The main issue that limits scalability is when you have to share things, so minimizing the contention around sharing fixes that.  I'm not going to tell you that CQRS is right or wrong for your application, only that you are comparing apples and trucks.  They are very different things.</p>

<p><a href=""https://martinfowler.com/bliki/CQRS.html"" rel=""nofollow noreferrer"">Command Query Responsibility Segregation (CQRS)</a> Has its uses, and works well in focused applications.  However, it's a <strong>design</strong> pattern for your code.</p>

<p>Your other proposal was an <strong>architectural</strong> decision.  While architecture has a big impact on what design patterns are available or relevant to use, the decision is orthogonal to design.</p>

<p><strong>Bottom Line</strong></p>

<ul>
<li>Know your design parameters (do you need to support internet scale?)</li>
<li>Understand your bottlenecks (what's preventing you from meeting your goals?)</li>
<li>Understand the cost of your decisions (how much does it cost to run, and how much does it cost to switch?)</li>
</ul>

<p>Your team needs to agree on the problems that need to be solved, and when you are considering alternatives, make the alternatives of equal type.  For example, should we use Redis, ElasticSearch, or just simple cloud blob storage?  Those are examples of equivalent architectural decisions, which are themselves not mutually exclusive.</p>
"
379926,"<blockquote>
  <p>Why do we even need it?</p>
</blockquote>

<p>The enormous benefit of microservices—and more largely, SOA—is the high level of abstraction of the internals—not only the implementation, but also the technologies being used. For instance, if a system is developed in a form of five microservices by five teams, one team can decide to move to a completely different technological stack (for instance from Microsoft stack to LAMP) without even asking other teams for their opinion.</p>

<p>Look at Amazon AWS or Twilio. Do you know if their services are implemented in Java or Ruby? Do they use Oracle or PostgreSQL or Cassandra or MongoDB? How many machines do they use? Do you even care about that; in other words, are those technological choices affecting the way you use those services?... And more importantly, if they move to a different database, would you have to change your client application accordingly?</p>

<p>Now, what happens if two services use the same database? Here are a tiny part of the issues which may arise:</p>

<ul>
<li><p>The team developing service 1 wants to move from SQL Server 2012 to SQL Server 2016. However, the team 2 relies on a deprecated feature which was removed in SQL Server 2016.</p></li>
<li><p>Service 1 is a huge success. Hosting the database on two machines (master and failover) is not an option any longer. But scaling the cluster to multiple machines requires strategies such as sharding. Meanwhile, team 2 is happy with the current scale, and sees no reason to move to anything else.</p></li>
<li><p>Service 1 should move to UTF-8 as its default encoding. Service 2, however, is happy using Code Page 1252 Windows Latin 1.</p></li>
<li><p>Service 1 decides to add a user with a specific name. However, this user already exists, created a few months ago by the second team.</p></li>
<li><p>Service 1 needs a lot of different features. Service 2 is a highly critical component and needs to keep database features at their minimum to reduce the risk of attacks.</p></li>
<li><p>Service 1 requires 15 TB of disk space; the speed is not important, so ordinary hard disks are perfectly fine. Service 2 requires 50 GB at most, but needs to access it as fast as possible, meaning the data should be stored on an SSD.</p></li>
<li><p>...</p></li>
</ul>

<p>Every little choice affects everyone. Every decision needs to be taken collaboratively, by people from every team. Compromises have to be made. Compare that to a complete freedom to do whatever you want in a context of SOA.</p>

<blockquote>
  <p>it's too [...] unmanageable.</p>
</blockquote>

<p>Then you're doing it wrong. I suppose you're deploying <em>manually</em>.</p>

<p>This is not how things should be done. You need to automate the deployment of virtual machines (or Docker containers) which run the database. Once you automated them, deploying two servers or twenty servers or two thousand servers is not very different.</p>

<p>The magic thing about isolated databases is that it's <em>extremely manageable</em>. Have you tried managing a huge database used by dozens of teams? It's a nightmare. Every team has specific requests, and as soon as you touch something, it affects someone. With a database paired with an app, the scope becomes very narrow, meaning that there are much less things to think about.</p>

<p>If a huge database <em>requires</em> specialized system administrators, databases which are used by only one team can essentially be managed by this team (DevOps is <em>also</em> about that), freeing system administrators' time.</p>

<blockquote>
  <p>it's too costly</p>
</blockquote>

<p>Define cost.</p>

<p>Licensing costs depend on the database. At the era of cloud computing, I'm pretty sure all major players redesigned their licensing to accommodate the context where instead of one huge database, there are lots of small ones. If not, you may consider moving to a different database. There are a lot of open source ones, by the way.</p>

<p>If you're talking about processing power, both virtual machines and containers are CPU-friendly, and I wouldn't be very affirmative that one huge database will consume less CPU than a lot of small ones doing the same job.</p>

<p>If your issue is the memory, then virtual machines are not a good choice for you. Containers are. You'll be able to span as many as you want, knowing that they won't consume more RAM than needed. While the total memory consumption will be higher for lots of small databases compared to a large single one, I suppose that the difference won't be too important. YMMV.</p>
"
376325,"<p>This is my first post on the Software Engineering Stack Exchange so let me know if something is wrong with it.</p>

<p>I'm looking into the serverless offerings of Amazon to try to figure out if that is the way to go for a few new projects I have in mind. I'm particularly interested in an event-sourced, CQRS model, as I find the purported advantages of such a model very attractive in this instance. But I'm having a little bit of trouble understanding all of the services Amazon have to offer, what their pros and cons are, and how it all fits together. I'll give some pretext first and state my questions afterwards.</p>

<p>I'll use an example application to illustrate what I'm after:</p>

<p>It's a simple (static) web application, hosted in S3 and served over cloudflare.</p>

<p>It has two actions: One command and one query (in CQRS terms).</p>

<p>The command posts an event onto the event stream to increment a counter.</p>

<p>The query gets the current state of the counter, i.e. how many times it has been incremented.</p>

<p>That's it, so how do I implement this using serverless AWS technology? Here's what I'm thinking so far:</p>

<p>To send the command to increment the counter, the web application sends AJAX requests to a lambda L1 (through an API gateway). This lambda L1 posts an event to the event stream.</p>

<p>Another lambda L2 listens to the event stream and stores a record of the event/command so that it can be replayed at a later date if need be.</p>

<p>Yet another lambda L3 listens to the event stream and executes the command. In other words, it fetches the current state of the counter, increments it and persists the new state atomically.</p>

<p>To send the query, the web application sends an AJAX request to lambda L4 (through an API gateway), which queries the state and returns the result.</p>

<p>This seems like it should be a fairly straight forward, minimal project. Here are my concerns so far:</p>

<p>First of all, what should my event stream look like? I have seen many suggestions floating around, each one more convoluted and contrived than the last. Various fanning out strategies, mixtures of SNS, SQS, Kinesis, DynamoDB streams, you name it... I fear I will end up with too many moving parts, a cost-ineffective system that's difficult to scale in the sense that the complexity makes it difficult to develop for.</p>

<p>Second, can I achieve atomicity? The event stream services I mentioned above typically have some sort of ""at-least-once delivery"" property, which needs to be handled by the consumer. One suggestion I have seen is to make every event idempotent, but that does not seem feasible in my example application. Two clients could increment the counter at the same time, and one of the increments could get ""lost"" because both of the commands would say ""the counter is now at 17 (for example)"". You could argue that this is correct behavior, both client saw the number as 16 and wanted to increment it to 17, but let's say in this situation we would like both increments to count toward the total. We want our command to represent only a delta between the two states. Is there any way to achieve this?</p>

<p>Third, lambdas L3 and L4 both need to be able to access some sort of persistence layer. Ideally I would like this to be a relational database (SQL) so that I can perform advanced queries on the current application state. It's not necessary for my incrementing counter example, but will be necessary for the projects I have in mind. I think this only leaves me with one option if I want to stay serverless: Serverless Aurora. That's fine by me, but it's my understanding that Aurora needs to run in a VPC, and that lambdas need to run in the same VPC to have access to Aurora. I'm very concerned about performance here, as L3 is the single congestion point in my example (everything else is append-only or read-only). My understanding is that VPCs incur a pretty hefty performance cost (throughput, number of connections, bandwidth), and that lambdas in VPCs can have cold starts of upwards of 10 seconds. How can I tackle these problems? Alarm bells are going off in my head, that this just introduces more problems than it solves. I would probably have to ping L4 continuously so that it never cold starts (10 second load time is unacceptable), and at that point, am I really going serverless? If this is a bad idea, are there any better alternatives? Do I have to persist state in DynamoDB as well, losing querying capabilities?</p>

<p>This post is already pretty lengthy so I'll leave it at these three concerns for now. Aside from answering my questions directly, if you could help me clear up any misunderstandings, offer alternative solutions, etc. I would be grateful!</p>
"
372833,"<p>It all comes down to the cost of fast memory.</p>

<p>When you consider how expensive is the RAM, it starts making sense to use it for a small amount of data you need right now, and keep everything else you don't need that much on a cheaper medium. Often, there are even more levels than that:</p>

<ul>
<li>RAM for a small subset of data you need to read as fast as possible,</li>
<li>Local SSD for a larger subset of data you would like to be able to read fast enough,</li>
<li>Local hard disks, directly attached storage or NAS for the data you may need to read in a reasonable time,</li>
<li>Tapes for data you are expected to read only in exceptional circumstances.</li>
</ul>

<p>This applies equally well to home PCs, data closets, even cloud storage. Compare the price of a AWS server with a hundred gigabytes of RAM with a hundred gigabytes of S3 storage and a hundred gigabytes of Glacier storage. If you want S3 reliability but at a speed of a local RAM, you should expect the price to be accordingly high.</p>

<p>The good thing is that there are specific patterns which are designed to work with this topology (i.e. small but hugely fast non-persistent data medium combined with increasingly large, persistent, slower mediums). Cache is one of them: instead of having to juggle with data, trying to guess which one should be put in RAM, you simply delegate this job to a caching solution, picking the right caching approach. And when it comes to caching approaches, you have <a href=""https://en.wikipedia.org/wiki/Cache_replacement_policies"" rel=""nofollow noreferrer"">a large choice</a>.</p>

<p>Aside the cases handled by proper caching, there are situations where you need high speed access to perform a task (an example which comes to mind is the restoration of a SVN repository from a backup, which is extremely dependent on the speed of storage medium). In most of those situations, persistent aspect is welcome, but not required: for instance, if the server restarts while I was recovering a SVN repository, I can always do the operation again (or do it in parallel on multiple servers if it's worth the money).</p>

<p>Finally, there are situations (scientific analysis, statistical data processing) where it would be great to have huge amounts of very fast data mediums. Persistent aspect is usually irrelevant in those cases (with map reduce, you just redo the job of a machine which terminated unexpectedly), and huge costs of any fast medium usually force to fallback to ordinary SAN solutions.</p>
"
372002,"<p>I think the pattern you outline is the common one. (In that you program your own 'routing worker') But you could condense it down, moving the routing logic (red) into the worker.</p>

<p>For example, say instead of a worker listening to a single queue, I add code that is aware of the users. </p>

<p>I can then fire up a thread per user queue in the same worker service and let the cpu spread its time over each thread.</p>

<p>It might be slightly sub optimal for large numbers of users or CPU bound tasks, but it would simplify your overall solution.</p>

<p>Along a similar line, if you can pass on the costs, a good solution is to spin up a new worker on a new machine in the cloud as well as a queue per user. </p>
"
370490,"<p>I'm evaluating the migration of the following application's architecture: - Nginx + PHP + MySQL - Currently the infrastructure is scalable and redundant in the AWS cloud and It was designed to support one client.</p>

<p>Our next plan is to expand it to support multiple clients, these are my concerns: 1- The nature of the application is focused on collecting granular data by a set of companies with a task force of users based on a list of tasks per item in a given place. In summary: A user can potentially collect around 14.4k records per day (1 account x 2 campaigns x 12 actions x 12 tasks x 50 items) So a big client with 50 employees and 10 places to visit per day would record around 7.2M rows per day, anything can be measured ( a text, file as in a photo or video, etc) Our current way of storing the most granular values in a table is based on the EAV model. This allows the app to be very flexible, but as you can see generates a row vertically. making redundant much of the fk values and exponentially growing that table, giving us some issues when trying to run reports on this database, unless we offload the report somewhere else.</p>

<p>I was wondering if moving to: - Play + Mongodb And making the visit our main collection would reduce the redundant information. In the end a visit would be our most important document and would store 14.4k values, but everything would be contained inside. That means, in terms of documents, I would generate with 50 employes x 10 visit only 500 documents per day, which seems much more mangeable from the operations perspective.</p>

<p>However, the issue here is granular or cross-vertical reporting, like a report of your task force for the month. I guess this could be offloaded.</p>

<p>Would Play + Postgres be a better option and using the table with bjson data with slick?</p>
"
369596,"<p>A Docker image is identified by:</p>

<pre><code>[owner]/[name]:[tag]
</code></pre>

<p>When you get an official image from Docker Hub, the owner is not required. </p>

<p>Example:</p>

<pre><code>alpine:latest
</code></pre>

<p>So, </p>

<h3>I can pull safely the alpine image and indeed get the official image?</h3>

<p>When you pull from the Docker Hub (Official Repository) you will get the official Alpine image. </p>

<h3>My images are only based on the ""official"" images, I do not need a local registry?</h3>

<p>Images are just layers. If all your images are based on official public images you will not need use a repository (Docker Hub or on-premise). But, a good practice is create a <strong>base image</strong>. This image is created from a Official Image plus your enterprise customizations. </p>

<p>Example: </p>

<pre><code>mycompany/alpine-custom:latest
</code></pre>

<p>In this particular case, if you want to keep your image private and on-premise. You can use a internal registry. If you want to keep your image private, but on the cloud, Docker Hub private is also an option. </p>

<h3>What is the advantage of having a (local) registry with ready images in a deployment scenario over building the image on the host off the baseimage (e.g. alpine) and create a new one on the host with few additional git checkouts of code to deploy?</h3>

<p>When a application is ready for deployment, you create a package with all dependencies. You build the application before releasing. Is the same principle. It is better to have a static image prepared to production. It deploys faster and has less chance to download an incompatible dependency version or have a network failure.</p>

<h3>When does it make sense to pull full images from a (local) registry instead of building on the host? And then: why not using dockerhub private repositories for that?</h3>

<p>The chose between  cloud or on-premise repository evolves aspects like: security, performance, reliability, costs, network availability, etc. Each scenario has its own particularities. Usually use DockerHub private is OK for majority of scenarios. </p>

<p>For instance:</p>

<ul>
<li>You have to follow regulations that imposes to not use cloud platforms.</li>
<li>You use Docker for private processing and your network does not have access to the Internet. </li>
</ul>

<p>Is good to thing about the Container Workflow:</p>

<p>Layer one:</p>

<ul>
<li>Pull base</li>
<li>Build image-a</li>
<li>Push image-a</li>
</ul>

<p>Layer two:</p>

<ul>
<li>Pull image-a</li>
<li>Build image-b</li>
<li>Push image-b</li>
</ul>

<p>Deployment:</p>

<ul>
<li>Pull image-b</li>
<li>Run image-b</li>
</ul>
"
365330,"<p>Let me just start by quoting <a href=""https://aws.amazon.com/dynamodb/faqs/#general_anchor"" rel=""noreferrer"">Amazon's DynamoDB FAQ</a></p>

<blockquote>
  <p>Q: When should I use Amazon DynamoDB vs a relational database engine on Amazon RDS or Amazon EC2?</p>
  
  <p>Today’s web-based applications generate and consume massive amounts of
  data. For example, an online game might start out with only a few
  thousand users and a light database workload consisting of 10 writes
  per second and 50 reads per second. However, if the game becomes
  successful, it may rapidly grow to millions of users and generate tens
  (or even hundreds) of thousands of writes and reads per second. It may
  also create terabytes or more of data per day. Developing your
  applications against Amazon DynamoDB enables you to start small and
  simply dial-up your request capacity for a table as your requirements
  scale, without incurring downtime. You pay highly cost-efficient rates
  for the request capacity you provision, and let Amazon DynamoDB do the
  work over partitioning your data and traffic over sufficient server
  capacity to meet your needs. Amazon DynamoDB does the database
  management and administration, and you simply store and request your
  data. Automatic replication and failover provides built-in fault
  tolerance, high availability, and data durability. Amazon DynamoDB
  gives you the peace of mind that your database is fully managed and
  can grow with your application requirements.</p>
  
  <p>While Amazon DynamoDB tackles the core problems of database
  scalability, management, performance, and reliability, it does not
  have all the functionality of a relational database. It does not
  support complex relational queries (e.g. joins) or complex
  transactions. If your workload requires this functionality, or you are
  looking for compatibility with an existing relational engine, you may
  wish to run a relational engine on Amazon RDS or Amazon EC2. While
  relational database engines provide robust features and functionality,
  scaling a workload beyond a single relational database instance is
  highly complex and requires significant time and expertise. As such,
  if you anticipate scaling requirements for your new application and do
  not need relational features, Amazon DynamoDB may be the best choice
  for you.</p>
</blockquote>

<p>Are you even expecting a few thousand users? Do you have any concern with a sudden spike to millions of users? You've already stated that you need ""complex relational queries"" to use the terminology from the quote. Choosing a key-value/document store is not just saying ""I don't need those now"" but also ""and I will never need those"".</p>

<p>The quote also paints an overly rosy picture of NoSQL key-value/document stores. The weakened consistency guarantees lead to a significant amount of extra complexity in the application code to get correctness. My strong impression is that many developers using NoSQL key-value/document stores, just pretend that they have these consistency guarantees (or rather, don't realize that they don't) and write subtly broken code. (Cue the <a href=""http://hackingdistributed.com/2014/04/06/another-one-bites-the-dust-flexcoin/"" rel=""noreferrer"">multiple Bitcoin exchanges</a> that got ""hacked"" because they wrote code against NoSQL databases assuming they provided more consistency than they do.) There are some things that you just can't do with a NoSQL key-value/document store without basically manually reimplementing some of the trickiest parts of a relational database.</p>

<p>My general advice is that a relational database should be the default choice. Realistically, a system using a NoSQL data store will almost certainly have (or benefit from) a relational database as well, so the real question is, ""is there any reason to <em>also</em> have a NoSQL data store?"" The benefits of relational databases is that they are some of the most battle-tested software systems on the planet, are full-featured, and are very unlikely to leave you boxed in a corner where implementing certain functionality is just ""impossible"". Read-heavy loads are not problematic for relational databases, but even if they were the solution to that would be <em>caching</em>. You may even use a NoSQL key-value/document store for that cache! That would be a very good use of a NoSQL solution.</p>

<p>Relational database are likely to be completely adequate for many users needs performance-wise. They simplify and speed up development by presenting a much simpler consistency model and providing more features out-of-the-box. There's likely to be some data where consistency is important and latency isn't important; these are well-served by a relational database. There is also likely to be data where latency is more important and up-to-date consistency less so; caching handles this well and NoSQL solutions often shine here (assuming normal HTTP caching doesn't suffice). If your system does need to scale, you are most likely looking at a hybrid system, not a transition to a different data storage technology altogether.</p>

<p>(There are some application domains where it makes sense to design the system from the get-go for weak consistency to achieve low-latency, e.g. online, multiplayer, first-person shooter games. But this isn't most systems, or at the very least it's a choice between paying for a complicated low-latency design now or paying for it later when you have more information about the requirements and load [and likely more money and expertise].)</p>
"
365193,"<p>Application scalability is an enormous topic and not one that can really be addressed in a single post. However I'll have a crack at a basic explanation.</p>

<p>As you've implied most complex websites are scripts or programs which are executed on a server somewhere and HTML/Javascript (and others) are sent to a browser to interpret and render. Websites like amazon and facebook have millions of unique visitors each day.</p>

<p>To start off with yes, google's servers are no doubt considerably more powerful than your PC at home. However even they are not powerful enough to serve up so many http responses at once.</p>

<p>That's where server farms come in. If ten thousand users want to load the same website then a load balancer splits the traffic onto a number of different servers each of which can run the website.</p>

<p>To make things even more interesting these server farms do not have to all be located in one place, many huge organisations have their physical servers located around the globe allowing them to store and serve content from the one physically closest to the consuming computer (thanks for the suggestion amon).</p>

<p>There are a couple of challenges to overcome, if a user accesses server 462 on their first visit they may not necessarily hit the same server on their next request. Applications can either be designed to accommodate this (by avoiding things like session variables) or the load balancer can be configured to send returning users to the same server for the next page.</p>

<p>If all these servers access the same database under the covers then all the server farm may be doing is shifting the performance bottleneck from the web servers to the database. This is where microservice architecture (building a ""website"" from many tiny websites) can help as can distributed databases like MongoDB.</p>

<p>Many hosting providers nowadays (azure/AWS for example) prefer you deploy web applications to them rather than creating a website on a VM like you would if your company has it's own hosting. This is to allow them to scale up their infrastructure in demand to requirements - however these scale ups often come with cost implications.</p>

<p><strong>TLDR</strong></p>

<p>Many techniques are employed - load balancing, software architecture, and hosting considerations all play a part in building a robust system.</p>
"
363972,"<p>OK, so answering your question literally, You don't need to make any changes.</p>

<p>The volume of data on the database isn't usually a scaling issue for webpages. Any given webpage will only be looking at a small part of the data and databases are designed to retrieve subsets of data from large sets very quickly.</p>

<p>What you need to take account of is how quickly you can generate a page. This is usually limited the number of requests per second for webpages that your server is receiving. Each one takes up a chunk of CPU time and your server has a limited amount. Once you hit 100% every page request will get slower and slower.</p>

<p>So its not the million rows you have to worry about, its the million users.</p>

<p>Website servers are cheap to scale, you and fire up webserver2, copy your website to it and double your capacity with no major technical hurdles.</p>

<p>However, both websites will now be using the same database server. and once that hits capacity you do have some significant techincal problems to solve.</p>

<p>The underlying problem is that you want all the database requests to be looking at the same data. If you simply copy it across to a second db the databases will quickly go out of sync.</p>

<p>If you continually update each with data from the other, the each server is doing twice as much work and you haven't solved your scalablity problem.</p>

<p>MongoDB and other nosql databases are designed to combat this by ignoring it. instead of having a number of tables which must all be consistent, you put all your data into one blob. so its always consistent with itself.</p>

<p>This allows you to spin up multiple instances and just copy the data without too much of an issue. It then solves the problem of the blobs being updated with some clever tricks you don't have to worry about too much.</p>

<p>So in summary: You current php + mongodb website, assuming you haven't made any 'gothcha' errors should scale up to any number of users by just throwing more servers at the problem. If you are cloud hosting that just means clicking a button or two and putting your credit card in.</p>
"
363959,"<p>I'll give a bit of rationale for JacquesB's comment with which I completely agree.</p>

<p>First, for most domains RDBMSs can very likely scale to the load, and by the time this stops being true (if it ever does), you'll have a lot more understanding of your data and the queries you need to support (and presumably a lot more money). In other words, an RDBMS suffices for most use-cases for, at least, quite a while.</p>

<p>However, the real reason to use RDBMSs is flexibility. Using a typical key-value or document store means encoding a preferred way of accessing the data. For your example, this might mean that replies are child elements in a ""post"" document. This is great when you want data in just that format. It becomes far less great when you want, say, to get all the replies by a specific user. Now you have to walk over <em>every</em> post in the system and check <em>all</em> replies. In a relational database, this query would be trivial and, perhaps with the addition of a few indexes, would likely perform reasonably well. To get reasonable performance from the document store would require rearchitecting the data model, or, more realistically, storing a copy of the data organized by user. The latter is essentially manually creating an index. <a href=""http://www.sarahmei.com/blog/2013/11/11/why-you-should-never-use-mongodb/"" rel=""nofollow noreferrer"">Sarah Mei's article</a> outlines exactly how this plays out in detail.</p>

<p>Adding to the flexibility of RDBMSs, most are very featureful. Want to store JSON? Fine. XML? No problem. Need some full-text searches? Well, you should probably use a dedicated solution, but you can at least start meeting the need without changing technologies. Need integration with message queues, ORMs, Excel? No problem. Need more consistency? Use SERIALIZABLE transactions. Don't need that much consistency? Drop to READ COMMITTED, which is often the default! This monolithic, kitchen sink approach is one of the downsides of RDBMSs, but it definitely improves agility when you can use an out-of-the-box, integrated solution to, at least, prototype rather than needing to evaluate and integrate multiple 3rd-party solutions.</p>

<p>One of the things traditional RDBMSs do poorly that NoSQL solution usually do well is distribute. Virtually all of them <em>can</em> be used in a distributed way, but they are <em>much</em> more complicated to set up and maintain than most NoSQL solutions. So-called ""NewSQL"" systems, such as VoltDB, do give an experience much more like NoSQL solutions with regards to setting up distributed clusters. Of course, if you are using a database-as-a-service from a cloud provider, then you don't have to worry about this.</p>

<p>The place where I think most key-value/document stores fit is as a <em>caching layer</em>/secondary index. So, I actually <em>do</em> think you should ""manually create indexes"" using a NoSQL data store. I just think they should be ""indexes"" of data stored in a different system, typically an RDBMS. Another, closely related, way to think about this is the NoSQL data store is a <em>materialized view</em> of data stored elsewhere. This suggests a bit more generality: the ""materialized view"" may be sourced from multiple primary sources each the source of truth for their respective data, and they don't need to be RDBMSs, e.g. some might come from an LDAP server. RDBMSs do, generally, do very well as systems of record. Most NoSQL solutions usually should not be used as systems of record. This splitting of the work alleviates pressure on the system of record, while not asking NoSQL solutions to do things they aren't designed to do. Most of the problems with NoSQL solutions (weak consistency, redundancy, preferred access paths, limited or no integrity checking) don't matter when they are used this way.</p>

<p>Revisiting the earlier scenario: You're a year in to the project and you have 100,000 users. Your sole RDBMS database combined with standard web page caching is performing adequately until a viral article leads to a huge spike in traffic. Your RDBMS struggles to meet the demand. To avoid this in the future, you consider using a NoSQL solution to store AJAX responses in a ready-to-go manner for posts and replies. You realize this may mean that users see different looking reply threads at different times, but you accept this. By using sticky sessions, you can at least avoid the situation that a user fails to see their own reply after posting it. At this time you get the requirement to add a ""Replies by User"" feature. In a day, you write the simple SQL query and, after a bit of initial performance testing, add a few indexes. You don't expect this to be the subject of a spike in demand, so you just go directly against the RDBMS. Since you're using a snapshot-based isolation mode, this neither blocks nor is blocked by any other (read or write) request.</p>

<p>To directly answer your question, I'm pretty confident most comment systems <em>are</em> based on RDBMSs. It is very unlikely your use-case is an outlier where an RDBMS would not suffice. Usually the structure of the data is not a significant factor for choosing a database solution (full-text and graph-based data may be an exception). Usually what's more relevant is how the data will be used: issues such as latency requirements, consistency requirements, availability requirements, the cost of losing data, how independent the data is, and generally how you want to query the data. For most OLTP loads, including comment threads, RDBMSs make reasonable trade-offs for these issues.</p>
"
360022,"<p>The question is one of scale, where it will be hosted, cost and management.  If you know you are going to host in AWS, then you can take advantage of the distributed nature that makes the cloud more scalable.</p>

<p><strong>First Decision: Self Hosted vs the Cloud</strong></p>

<p>The old answers (circa 2014) reflect the mindset when self hosting was still predominant.  However, there are reasons to look outside of an RDBMS for tag related queries.</p>

<p>Filesystem hosting requires that you manage your NAS or SAN yourselves and ensure you have enough provisioning and the expertise to improve performance and capacity as necessary.  It can be very expensive if the costs are not amortized across several applications.</p>

<p>The cloud allows you to use AWS S3 or whatever equivalent blob storage for your cloud provider.  This solution only charges you for the storage you use, and cloud blob storage provides both the scale and performance needed to scale as your application grows.</p>

<p><strong>Second Decision: RDBMS or Search</strong></p>

<p>The way you have to store tags in a relational database vs. a document store makes the queries to get records related to those tags more difficult.  This is even more so when you are looking for intersections between tags (i.e. documents that have 2 or more identical tags).  The queries will slow down the more complicated it gets.</p>

<p>ElasticSearch, SOLR, and similar search servers that can double as a document store provide an ideal middle ground.  Many cloud providers have hosting solutions for these types of problems.  They are designed to scale to very large sizes and perform searches very quickly.  In fact this site (softwareengineering.stackexchange.com) uses ElasticSearch to do queries like this.  NOTE: ElasticSearch is also a NoSQL DB in addition to being a search server.</p>

<p>I will say that you can't think in relational terms when you are doing document searches so there is a learning curve.</p>

<p>Added bonus is that at least with AWS, ElasticSearch costs less than an RDBMS for the same size tier.</p>

<p><strong>Bottom Line</strong></p>

<p>Millions of records is not astronomical for today's RDBMS's.  However, you will reach a saturation point.  Many websites still use an RDBMS for the data storage of record and then synchronize that with a search server for the heavy lifting.  That decision really depends on things outside the scope of this question.</p>

<p>The ElasticSearch/S3 route will scale well beyond that.  However, do your research.  There are tradeoffs that you have to weigh.  In my case this choice was the right one.</p>
"
356334,"<p>It depends.</p>

<p>The behind the scenes implementation of the lambda runner is going to affect this. We can see that in AWS the container might be reused.</p>

<p><a href=""http://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction.html"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction.html</a></p>

<p>So we could see connection pooling/reuse for some requests at least. Also we should consider the database itself and how it treats incoming connection requests.</p>

<p>This kind of question for me underlines some of the problems with 'serverless' its still very new and immature, so the details haven't been hammered out.</p>

<p>We should always remember that serverless doesn't mean no servers. If the rate at which you call a lambda is high enough, you <em>may</em> effectively have several servers, or 'containers' running.</p>

<p>In practice the start up time and resources such as IP addresses of lambdas can be a real issue. Perhaps as they mature a consensus of how to run them will appear and these problems will get solid answers.</p>
"
350817,"<p>Since this is a static site, I recommend deploying your site in Azure Storage optionally adding CDN.</p>

<p>There is a guide for static sites on Azure Storage <a href=""https://blogs.msdn.microsoft.com/make_it_better/2016/08/09/simple-websites-using-azure-storage-blob-service/"" rel=""noreferrer"">here</a>, and there is a guide for hooking up CDN to your storage <a href=""https://docs.microsoft.com/en-us/azure/cdn/cdn-create-a-storage-account-with-cdn"" rel=""noreferrer"">here</a>.</p>

<p>The benefits are:</p>

<ul>
<li>Very reliable. <a href=""https://azure.microsoft.com/en-us/support/legal/sla/storage/v1_0/"" rel=""noreferrer"">Has the same SLAs as Azure Storage</a> </li>
<li>Very low maintenance. Lower maintenance than the three options you suggested because don't have to concern yourself with all the additional details that app services requires. Don't have to deploy any software.</li>
<li>Performant. Again Azure Storage has pretty good <a href=""https://docs.microsoft.com/en-us/azure/storage/storage-scalability-targets"" rel=""noreferrer"">performance goals</a>, and performance can be improved by adding CDN</li>
<li>Low cost. Azure Storage will have lower cost than the other options because you <a href=""https://azure.microsoft.com/en-us/pricing/details/storage/blobs/"" rel=""noreferrer"">only pay for storage costs and bandwidth used</a> not cost of running machines.</li>
<li>Easy deployment. Azure Storage offers <a href=""https://stackoverflow.com/questions/22829854/how-to-upload-files-to-a-windows-azure-storage"">several methods of uploading content</a>, which should make automating deployments easy.</li>
</ul>
"
347591,"<p>The primary concerns of this design are security and size. But before I get there, I want to clear up a misunderstanding:</p>

<blockquote>
  <p>As wrong as it seems, would it be better to sacrifice normalization and include the franchise ID directly in some or all of these child tables?</p>
</blockquote>

<p>I see why you might think this: if you consider franchise ID as an attribute then including it in every table violates third normal form. </p>

<p>But here's an alternative way to look at it: <em>in the logical design</em> the row key includes franchise ID: <code>(FRANCHISE_ID, TABLE_ID)</code>.</p>

<p>But, you say, <code>TABLE_ID</code> is an identity column! To which I answer: yes, but that's a <em>physical</em> detail, not a <em>logical</em> detail. And logically, tables are allowed to have multiple ""candidate"" keys (I turn to <a href=""https://www.amazon.com/dp/"" rel=""noreferrer"">C. J. Date</a> as my authority for this statement).</p>

<p>And once you accept this logical design, you'll get a lot of physical benefits. First, you don't need joins to access data; while, logically, joins take no time, physically they do. Plus, if your queries tend to retrieve multiple rows for the same franchise, you can also benefit by <a href=""http://blog.kdgregory.com/2017/03/mysql-problem-with-synthetic-primary.html"" rel=""noreferrer"">using a clustered index to collocate rows</a>.</p>

<p>OK, now on to the main topics.</p>

<h2>Security</h2>

<p>From a corporate management perspective, this is probably your most important issue. Clearly, you can't allow one franchisee to see data that belongs to another. But there are many ways to accomplish this, imposing different levels of load on the system <em>and its developers</em>. I'm just going to throw out some ideas here for you to consider.</p>

<p><strong>Predicate applied to individual queries</strong></p>

<p>This is the simplest, but adds the heaviest load to the developers. Every one of your protected queries will have to include a check against franchise ID. Forgetting even one could have economic consequences for your company (ie, lawsuits).</p>

<p>However, I think you can probably overcome this with a combination of code review, static analysis, and integration testing. You need the discipline to ensure that all queries go through a data access layer that's rigorously verified.</p>

<p><strong>Views</strong></p>

<p>To ensure that all queries include a check for franchise ID, you can hide your tables behind views, and ensure that each view includes a franchise check. Each franchisee will have their own set of views, stored in a different schema.</p>

<p>An additional benefit of this approach is that you'll be able to expose data directly to the franchisees. It also allows your physical tables to change without affecting the exposed data.</p>

<p>However, there are several significant drawbacks. First, your developers will have to ensure that they use the correct set of views for each query (maybe not that bad, depending on how you manage connections). Second, you will have a long-term maintenance cost, as changes have to be propagated to all of schemas that hold a particular view (although this should be easily automatable).</p>

<p><strong>Row-level Security</strong></p>

<p>I'm not familiar with SQL-Server, but my understanding of row-level security is that it's based on database users, so you'll need (at least) one user per franchisee. Which means that you'll need (at least) one connection per franchisee, which may cause undue load for your database (or alternatively, constant creation/destruction of connections). I'm also guessing that your developers will have to code queries that include franchise conditions or suffer runtime errors. And you'll have to manage all of those users.</p>

<p>All-in-all, this seems like the most painful route, but it is the one that guarantees security, so I'm guessing it's the way you'll go.</p>

<h2>Size</h2>

<p>From the development perspective this is going to be the bigger pain point -- especially when your users complain about slow response times.</p>

<p>Your overriding goal should be to touch as few data blocks as possible per query. Here are a few techniques that I've used successfully in the past:</p>

<p><strong>Buy as much RAM as you can afford</strong></p>

<p>Your goal should be to keep the entire database in memory. Really. It doesn't matter that SSDs are blindingly fast, they still require time to read and write data blocks.</p>

<p>In a perfect world, you would read the entire database into memory at startup, and the only IO would be writes.</p>

<p><strong>Reduce the ""active"" size of tables</strong></p>

<p>You mentioned one 91MM row table. How much of this table is accessed for a particular query? Can you partition the table so that infrequently accessed data is stored in another table? (I'm assuming that SQL-Server supports declarative partitioning, but if not you can manually move/duplicate rows).</p>

<p>Large tables necessarily mean that queries have to access a lot of rows. Even if you have indexes, because those indexes will also be large.</p>

<p><strong>Collocate data</strong></p>

<p>By default, databases store rows wherever they can find the space. Which means that data that is typically accessed together, such as the transactions for a user, might be spread all over the disk.</p>

<p>However, you generally have some level of control over this, either using clustered indexes (see my link above) or covering indexes. Leverage these to their fullest.</p>

<p><strong>Use read replicas</strong></p>

<p>A typical application executes selects far more frequently than updates, and tends to select multiple rows while updates affect a single row. </p>

<p>By separating these two operations, you get a couple of benefits. First, you can scale capacity independently: if you have a lot of reads you can buy more or bigger machines. And second, you can reduce contention: a long-running select won't block an update (personally, I think this is less of an issue today than, say, 20 years ago, but it's still worth considering).</p>

<p>The downside of read replicas is that there's a lag between the time a row is updated on the server and on the replica. This may or may not be a problem for you (and in my experience, lag is caused by undersized machines; more money solves that problem).</p>

<p><strong>Offload reporting to a data warehouse</strong></p>

<p>True ""reporting"" queries tend to be very different from operational queries. For example, an operational query might retrieve the most recent order for a single user, while a reporting query might find all users that bought a particular product. As a result, attempting to support both operational and reporting queries with the same physical design is a recipe for failure.</p>

<p>At the very least, shift reporting to a dedicated read replica, one that is indexed appropriately. Better is to make use of a completely different DBMS, one whose storage and query characteristics more closely match your reporting needs. Something like <a href=""https://en.wikipedia.org/wiki/Amazon_Redshift"" rel=""noreferrer"">Amazon Redshift</a>, <a href=""https://en.wikipedia.org/wiki/BigQuery"" rel=""noreferrer"">Google BigQuery</a>, or <a href=""https://en.wikipedia.org/wiki/Azure_SQL_Data_Warehouse"" rel=""noreferrer"">Azure SQL Data Warehouse</a>. Or maybe a locally-hosted option like <a href=""https://en.wikipedia.org/wiki/Apache_Cassandra"" rel=""noreferrer"">Apache Cassandra</a>.</p>

<h2>And now for something completely different</h2>

<p>Don't do this.</p>

<p>The time taken to develop a multi-tenant solution is time not available to add features that may be more relevant to your franchisees, or to improve the current code and processes.</p>

<p>If the issue is maintenance, or per-franchise capital expenses, look to alternatives that enable central management. For example, use Azure or another cloud provider with an ops person at the corporate office. You should be able to deploy a cloud-based solution for a per-franchise cost of a few hundred dollars per month (if that), cutting both capital and operational expenses for the franchisees.</p>

<p>If the issue is reporting, focus on more efficient data acquisition and transformation. Again, cloud-based solutions can help with this.</p>

<hr>

<h2>Update</h2>

<p>The idea of moving to a cloud provider -- and Azure is only one option, which I picked because you seem to be a Microsoft shop -- is to eliminate the problems caused by franchisees who aren't trained computer operators.</p>

<p>In the simplest form, you would create a database server for each franchisee in the cloud, and their existing applications would point to that server rather than the local database. The database would always be up, so that you could retrieve data at any time. And, typically, the cloud provider does regular backups and provides other options for fault-tolerance and recovery.</p>

<p>Pricing in the cloud is largely dependent on the features that you want. For example, looking at the <a href=""https://azure.microsoft.com/en-us/pricing/details/sql-database/"" rel=""noreferrer"">Azure Cloud SQL pricing page</a>, the base price for a ""Standard"" database service is $0.0202/hour, or $15/month. I have no idea what this actually provides you in terms of database performance; in my experience, $100/month is more likely.</p>

<p>There is nothing that prevents you from using cloud hosting as a first step, and then moving on to a true multi-tenant solution. And if you have hundreds or thousands of franchisees, that makes sense to manage cost. But it seems like your real problem is one of operations management.</p>
"
342331,"<p>Consider a greenfield development situation where cloud tools are being considered vs. in-house solutions, in the vein of AWS SQS vs. self-hosted Kafka, ECS vs. Mesos-Marathon, Lambda/Azure Functions vs. Whisk vs an array of custom APIs/services.</p>

<p>All else being equal (financial cost, technical expertise, etc.), how can the cost of vendor lock-in be fairly gauged when deciding whether or not to use cloud services beyond basic VM and storage products? I have seen in several cases, where fears of vendor lock-in closed the argument on using higher-level cloud services without even allowing for a technical or financial evaluation of their value to the project.</p>

<p>Of course there is <em>a</em> cost to using vendor-specific services, but that cost can't possibly be so large as to eclipse all other software development costs. Avoiding higher-level cloud services seems, IMO, to be an argument akin to ""let's build a completely abstract ORM in case we need to swap out database products.""... aka <a href=""https://en.wikipedia.org/wiki/You_aren&#39;t_gonna_need_it"" rel=""nofollow noreferrer"">YAGNI</a>.</p>

<p>Self-sufficiency is often the road to poverty, and all software is dependent on many other layers to be successful: Docker, Linux, npm, gcc, and dozens and dozens of others, but these are rarely looked at as ""lock ins"". The costs of doing <em>anything</em> internally, can be significant, including:</p>

<ul>
<li>Lost time to market</li>
<li>Resources devoted to maintaining internal and non-revenue generating services</li>
<li>Higher operational costs</li>
</ul>

<p>So, what is the right way to fairly evaluate cloud services, acknowledging the cost of vendor lock-in as one component in product strategy, without allowing it to dominate other concerns?</p>
"
340772,"<p>If you want to add something to the IDs that would act as a filter from bad data it probably would be better to include eg. last 4 chars of sha1 or md5 of the ID+salt itself so it can't be easily fabricated.</p>

<p>But I think, however that sending this ""checksum"" data as another parameter would be better. So you could send ID + checksum, and discard the checksum in the process, or at least save it outside of the ID so you won't add unnecessary data to a key which is indexed, as all data you put there is costly from DB perspective. Best would be if you provide some data sample.</p>

<p>The other part of your question is interesting... because you say that you're going to redesign your application partly because of pricing model of AWS, while for $90/month you can get a server with 32GB RAM, 8 cores and half TB of SSD storage. It's nice to pay for peace of mind if you have money and want to work on something that ""just works""... but if you're afraid about the costs, then I think it'd be much better from your perspective to invest your time in moving off of AWS, because the price to performance ratio is so laughable there, that the costs will KILL you in the long run. For me these shared cloud enviroments are always limiting factor, slow, expensive, with crappy, old software installed and bandwidtch priced like crazy.</p>

<p>I mean if you can have millions of requests per second on a server that costs nothing, thinking about every request on AWS is a waste of life, which you can waste on eg. designing new features for your app ... and beside it's pointless, it won't become cheap, no matter how much you castrate the code, compress the communication, etc. it'd be still expensive as HELL for anything but an app that has no traffic ;)</p>
"
340747,"<p>If i understood you correctly </p>

<ul>
<li>you already have many different gui-implementations for an exsting backend-api and </li>
<li>you want to modify existing implementation-details of the backend without modifying the api.</li>
<li>your goal: convert backend into a cloud app so the app is more scalable</li>
</ul>

<p>Your question is: In wich order to reimplement ""implementation-details""</p>

<p>The answer is similar to the question: ""which code to change to optimize performance(speed)"" : measure/profile which code slows down the system.</p>

<p>In your case measure which sub-workflow benifts most from a scalable cloud service.</p>

<p>While ""storymaps"" is a great tool for agile projects in my opinion it is not suitable for your type of project.</p>

<p>note: In my opinion a storymap is workflow/feature oriented, your example tasks are component/modul/architecture oriented. </p>
"
338746,"<p>You need to clarify what kind of high availability you're looking for. There are highly available applications that I run that need to be up 95% of the time. There are others that need to run at 99%. I can think of life-or-death scenarios that require 100% uptime. Just those three have drastically different approaches and costs.</p>

<p>Just guessing based on your needs and a 95-99% uptime SLA:</p>

<ul>
<li>Database migrations should be able to happen in real time for most changes. Practice <a href=""http://martinfowler.com/articles/evodb.html"" rel=""noreferrer"">Evolutionary database design</a>. For changes that do require more invasive behavior, you have a few options. One is take the downtime. If possible, running your service in read-only mode might work. For full functionality, I've been wanting to try ScaleArc for a while. It looks like a really slick tool for scaling and resiliency in the SQL Server world.</li>
<li>Putting servers inside your customer's sites is a recipe for an unmanageable disaster unless you've got world-class deployment strategies (which, based on your description of your migrations, you don't have yet). Don't push cloud services on-prem because you have performance problems. Solve the performance problems now and then you won't have to deal with costlier ones done the road.</li>
<li>Your state server should be a database of some sort. Follow their HA guidelines. You can use SQL Server for this, since you already have it available to you.</li>
<li>Speaking of databases, replication does not enable HA. In fact, SQL Replication will cause you headaches around every turn (speaking from experience with multiple node replication scenarios). Mirroring can work, but last I remember, SQL clustering takes 1-5 minutes to fail over to the new server. I've heard good things about AlwaysOn, but I'm still suspicious given Microsoft's track record. Something like ScaleArc might be more help here.</li>
<li>Your web server should be stateless. Spin up three or four and put them behind a load balancer. That solves your uptime worries there. As Frederik mentioned earlier, you can also do rolling deployments this way.</li>
<li>Your web service should probably be stateless. If not, see if you can break it apart into stateless and stateful bits. Putting multiple instances of it behind the same load balancer again solves uptime worries and enables more interested deployment scenarios (e.g. blue/green deployments).</li>
</ul>

<p>Unlike Frederik, I won't call your cloud paranoia unwarranted. It depends on your uptime requirements. It is conceivable that a service would have to run in multiple data centers operated by different providers in different countries for redundancy's sake. Given your current state, however, I'd agree that AWS, Azure, or similar are probably safe bets for your company.</p>
"
338692,"<p>Getting some level of HA on your web &amp; application tier:</p>

<ol>
<li><p>Ideally, factor out any state, including session state into shared-state systems like a database or an in-memory session state server.  Depending on your application design this may cause performance issues due to the added latency getting a large amount of state. </p></li>
<li><p>Your web site &amp; application tier should each have an independent load balancer in front of them. NGINX will do the trick, but IIS can do this too (ARR). </p></li>
<li><p>If a single database can't handle the load, leverage session state partitioning (or sharding or consistent hashing) to route particular request to a particular database box.</p></li>
</ol>

<p>If factoring out state is too hard, you can go with server affinity for load balancing (ie users are consistently routed to the same box, often cookie based). It's not as highly available as a stateless round robin approach, because a box outage will impact all users &amp; state on that that box, but it beats a complete outage (use-case dependent). </p>

<p>On the upgrade side:</p>

<ol>
<li><p>Design your database scripts in such a way that database upgrades can be done while the system is running, in other words, maintain backwards compatibility.  A pattern that works well for that is ""expand, then contract"" -> make only additive, backwards compatible changes but removing dependencies on the fields (etc) that you want to get rid of; then upgrade all clients of the database to v-latest; then do another db-upgrade to get rid of the old fields (etc) in the database.   This can be a slow process if you have a large database and you have to be careful to not scupper the performance of your system. </p></li>
<li><p>Upgrading your app tier: since your not using a cloud environment, I recommend you follow the canary deployment pattern: do a rolling upgrade of your web &amp; middle tier boxes. If the deployment goes wrong, take the box out of the load balancer, just like you would as if it had failed.</p></li>
</ol>

<p>Word of warning: evolving a system that hasn't been designed for HA into one that is, can be a long and costly process. You'll have to make trade-offs along the way (cost vs effort to reach a particular level of availability)</p>

<p>Your cloud paranoia is unwarranted - providers such as AWS in conjunction with good practice on your part can control / mitigate most risks - have a look at their compliance page to get a feel for what regulations they're compliant with: <a href=""https://aws.amazon.com/compliance/"" rel=""nofollow noreferrer"">https://aws.amazon.com/compliance/</a>  </p>
"
336154,"<p>In addition to Robert's answer, you can use any of the mentioned servers in production environments. We do it often. Tomcat is one of the most used.</p>

<p>The point of these servers is that they are OpenSource and free. This fact makes them suitable for development because there're no constraints due to licences. Plus they are lighter than others like JBoss, WebSphere, Weblogic. If you have to run the IDE, the server,  browsers like Chrome, editors, etc, in local, you will appreciate their lightness.</p>

<p>Related to PaaS, Google's and Amazon's PaaS are not cheap. It makes  customers decide to go to a self-hosting or private cloud.</p>

<p>Self-hosting and private clouds imply a lot of things to be taken into account: network configuration, security, maintenance, monitoring, etc. A lot of work apart from the application maintenance. </p>

<p>If you go cloud, PaaS providers already have solved many of these issues. They also provide you with web tools like consoles to manage your environment.</p>

<p>But you pay for it of course. Also, you pay for bandwidth consuming, storage space, etc.</p>

<p>In both scenarios, you are free to use the app server you want. OpenSource and free are not synonyms of ""inadequate for production"". What makes a server suitable for production are its capabilities and features. If they have those you need, they are totally suitable for you.</p>

<p>Once you have the stack of technologies that forms your whole system, you would rather choose the PaaS that gives you better conditions and facilities to deploy it.</p>
"
334800,"<p>We have implemented several push servers and all of them following the <em>old tuple</em> ""socket-certificate"".<em>In Java (1.5 - 1.7)</em>.</p>

<p>To work with certificates has some disadvantages. For instance, we need one for each environment (test, pro, etc). You have to be methodic at managing them or it's quite easy to end up with the wrong cert in pro. Or to forget its renewal (they also expire). </p>

<p>Related to the socket, this approach requires having opened a specific range of ports in the firewall.</p>

<p>Related to the whole protocol of communication. You get so little info after pushing messages. It's hard to figure out what happened with the messages. The only way is to retrieve messages from the queue of responses. A queue which orders is not guaranteed. Neither when APNS is going to put the responses onto it. It might not happen at all.</p>

<p>Compared to GCM (Google cloud message which runs via HTTP), the ""socket-cert"" of APNS is a pain in the ...</p>

<p><strong>My suggestion is to get focus on the HTTP 2 - JWT</strong> protocol. This is a very common implementation of security in client-server communications. You will find many more references about http2 and JWT than looking for APNS sockets and Certs.</p>

<p>Security via JWT is commonly implemented these days. There is full support from the community.</p>

<p>Moreover, If they have planned to drop the support to the current implementation, why even to dare to try it? Why spend time and money twice?</p>

<p>Be preventive. Implementing HTTP2 - JWT approach will save you from further code reviews and refactors. In any case, it's a work to do, so better have it done sooner than later.</p>

<p>Related to the library CleverTap. Well nobody is stopping you from implementing your own client! Suited to your need and requirements.</p>

<p>This has been our case with our current engine. We discarded all the 3rd party implementations and built our own. So far I know it keeps working perfectly... Till Apple drops the service.</p>

<p><em>(If we didn't move yet to HTTP2 - JWT is due to  time and money)</em></p>

<p>There's perhaps alternatives. Google Firebase Cloud Message is multi-platform. So you can push messages to Android and iOs devices from the same service. It works over HTTP and API keys (tokens). I suggest to you to take a look.</p>
"
334294,"<h1>Good ol' Uncle Bob</h1>
<p>I'd encourage you to consider what <a href=""https://www.youtube.com/watch?v=Nsjsiz2A9mg&amp;feature=youtu.be&amp;t=42m17s"" rel=""nofollow noreferrer"">Robert &quot;Uncle Bob&quot; Martin has to say on databases</a>. I find it interesting, with regards to some considerations he discusses regarding SQL and NoSQL.</p>
<h1>TLDR</h1>
<p>SQL popularity was influenced by corporate entities, the open source community, technology stacks like <a href=""http://en.wikipedia.org/wiki/LAMP_%28software_bundle%29"" rel=""nofollow noreferrer"">LAMP</a>, and physical storage limitations of hard drives. As of 2016, your <a href=""https://jaxenter.com/the-top-10-sql-and-nosql-databases-108072.html"" rel=""nofollow noreferrer"">top three</a> database systems are all SQL-based (Oracle 12c, <a href=""http://en.wikipedia.org/wiki/Microsoft_SQL_Server"" rel=""nofollow noreferrer"">Microsoft SQL Server</a>, and MySQL). It was also pointed out in the comments below that SQL has been an approved standard for both <a href=""https://ansi.org/"" rel=""nofollow noreferrer"">ANSI</a>,(American National Standards Institute), and the <a href=""https://www.iso.org/obp/ui/#iso:std:iso-iec:9075:-1:ed-4:v1:en"" rel=""nofollow noreferrer"">ISO International Standards Organization</a> for decades, which has vast impacts on its predictability and quality - a very important business consideration for many enterprises.</p>
<hr />
<h1>Physical Storage Considerations</h1>
<p>Until the wide-spread appearance of SSDs and cloud computing within the last decade, SQL was the predominant leader in database languages; with these new technologies, however, a new evolution of database languages are appearing as well. There have been some articles discussing the <a href=""http://www.computerworld.com/article/3040694/data-storage/ssd-prices-plummet-again-close-in-on-hdds.html#tk.drr_mlt"" rel=""nofollow noreferrer"">closing gap in price and increase in both popularity and accessibility</a> of SSDs over HDDs. There's a great website for <a href=""http://db-engines.com/en/ranking"" rel=""nofollow noreferrer""><strong>ranking of database technologies</strong></a> available, which might interest you. You can see that SQL and relational databases still have a firm foot-hold, but NoSQL alternatives such as MongoDB are <a href=""https://www.marketresearchmedia.com/?p=568"" rel=""nofollow noreferrer"">creeping up as well</a>.</p>
<h1>The Goliaths</h1>
<p><a href=""https://en.wikipedia.org/wiki/SQL"" rel=""nofollow noreferrer"">SQL</a> has gained increasing popularity over the past several decades since it really took off in the mid 1980s. Today, the &quot;concrete&quot; nature of its popularity can be considered with regards to two of the most influential companies on the planet, whose enterprise products rely largely on solutions revolving around the SQL language. Those two companies are: Oracle and Microsoft. Countless small, mid, and large-sized companies use Oracle and Microsoft products, such as <a href=""https://jaxenter.com/the-top-10-sql-and-nosql-databases-108072.html"" rel=""nofollow noreferrer"">Oracle 12c, Microsoft SQL Server and MySQL</a>. It's also important to note the impact of the open-source community as well, which over the past 20 years or so in which it was very common to see many developers using PHP and MySQL together through things like the LAMP, <a href=""https://en.wikipedia.org/wiki/MAMP"" rel=""nofollow noreferrer"">MAMP</a>, <a href=""http://en.wikipedia.org/wiki/XAMPP"" rel=""nofollow noreferrer"">XAMPP</a>, and <a href=""http://en.wikipedia.org/wiki/WAMP"" rel=""nofollow noreferrer"">WAMP</a> stacks.</p>
<p>Both companies, Microsoft and Oracle, are software goliaths, and the OOP programming languages they provide are two of the most popular: Java and C#.</p>
<p>Java, being acquired by Oracle about 6 years ago, is one of the most popular programming languages in the world. Hence, Oracle has a very vested interest in making sure that its programming language is used in tandem with MySQL, which is the source of a very high revenue for the company.</p>
<p>C#, being owned by Microsoft, integrates very well with SQL Server through Microsoft's various database offerings, such as through cloud storage in Azure as well as local installations of Windows Server, which typically runs with an instance of <a href=""https://msdn.microsoft.com/en-us/library/mt238290.aspx"" rel=""nofollow noreferrer"">SQL Server Management Studio</a>.</p>
<p>Other popular languages like PHP also integrate well with MySQL, due to the open-source nature of those languages. The LAMP (Linux, Apache, MySQL, and PHP), as well as MAMP, XAMP, and WAMP, also provided a foray into a mix of easy-to-use out-of-the-box open-source technologies that were all the rave about 20 years ago (and still are very popular to this day).</p>
<p>The open-source community around these languages (in particular, SQL) are huge, so it is quite easy to find support.</p>
<p>The language itself is also very easy to understand and read, given that it is very close to how you would structure a logical English sentence (if there is such a thing!).</p>
<h1>Some Competition</h1>
<p>However, while SQL and other relational databases are holding strong, other alternative document store technologies, like NoSQL, <a href=""http://en.wikipedia.org/wiki/MongoDB"" rel=""nofollow noreferrer"">MongoDB</a>, <a href=""http://en.wikipedia.org/wiki/CouchDB"" rel=""nofollow noreferrer"">CouchDB</a> and Microsoft's NoSQL implementations are becoming <a href=""http://www.techrepublic.com/article/nosql-keeps-rising-but-relational-databases-still-dominate-big-data/"" rel=""nofollow noreferrer"">increasingly popular as well</a>.</p>
<h2>Physical Tech Advances</h2>
<p>Part of this increase in the use of NoSQL-esque platforms may be due to the physical hardware technologies that are settling into the market. As SSDs and HDDs are becoming <a href=""http://www.computerworld.com/article/3040694/data-storage/ssd-prices-plummet-again-close-in-on-hdds.html#tk.drr_mlt"" rel=""nofollow noreferrer"">more closely comparable in price</a>, storing data in the form of large JSON documents and in repository and <a href=""http://searchstorage.techtarget.com/tip/Nine-reasons-object-level-storage-adoption-has-increased"" rel=""nofollow noreferrer"">object storage systems</a>, such as <a href=""http://www.computerweekly.com/feature/Amazon-S3-storage-101-Object-storage-in-the-cloud"" rel=""nofollow noreferrer"">Amazon S3 Storage</a> becomes relatively less expensive, both physically and fiscally, whether you are considering storing it on your own hard drive locally or somewhere in the cloud.</p>
<h2>Cloud Offerings</h2>
<p>Cloud offerings are becoming cheaper by the day, and 1 TB of storage in the cloud is practically <a href=""https://www.cnet.com/how-to/onedrive-dropbox-google-drive-and-box-which-cloud-storage-service-is-right-for-you/"" rel=""nofollow noreferrer"">dirt cheap as well</a>. I personally have 1 TB of Google Drive storage for $1.99 per month (about $25 per year).</p>
<h1>Some Conclusions...</h1>
<p>Storage space (physically) used to be a crucial issue, which is where many SQL databases stood to serve a need for a data language that could be used to keep data storage as efficient as possible. This is where you get the <a href=""http://en.wikipedia.org/wiki/Create,_read,_update_and_delete"" rel=""nofollow noreferrer"">CRUD</a> style applications (Create, Read, Update, Delete). When storing massive amounts of data in a SQL database, you want to avoid duplication as much as possible through a process known as normalization, which has to do with designing the database in such a way as to have as efficient of a relational mapping as possible. This would help the physical lookup speed (also improved through indexing), but all of which was centered around a major concern, which was physical space. You simply couldn't afford to have massive amounts of duplicate data. Not to mention, the need for database replication and backup, to protect against things like natural disasters or power outages, terrorists, etc.</p>
<p>Huge companies with massive amounts of credit card data or health information couldn't afford to go down for even a minute, so they would need to replicate and distribute their data centers geographically to protect their users' data. This was enormously expensive, because every database needed a physical endpoint, a computer and a hard drive to store the mass of information.</p>
<p>Fast forward to today, the need for physical size is less expensive. We'll likely see a shift away from SQL over the coming years, but it has such a large user base, and many large companies are so firmly rooted, that this will likely take a while.</p>
<p>However, new industries, such as infrastructure as a service, provided by companies like Amazon with their <a href=""http://en.wikipedia.org/wiki/Amazon_Web_Services"" rel=""nofollow noreferrer"">Amazon Web Services</a> (AWS) - as well as Microsoft <a href=""https://en.wikipedia.org/wiki/Microsoft_Azure"" rel=""nofollow noreferrer"">Azure</a>, large scalable datacenters filled to the brim with high-capacity, lower-power consuming SSD hard drives are making it increasingly possible for those large companies to port over to newer, more efficient technologies. As a result, we will likely see the change in database language to reflect new physical, logical, and other scientific limitations.</p>
"
333291,"<p>Since you are CPU-limited, you need to get your hands on 150 CPU cores, one for each thread. This rules out a single server, since a server of such proportions would be prohibitively expensive – and you don't really need it.</p>

<p>Your general architecture with a common frontend that distributes work to multiple workers and combines their results appears to be sensible. You'll have to do some calculations to find the most cost-effective solution to get that many CPUs. That tends to point towards AWS Lambda since you only require computations in bursts, but it may come with restrictions. How many Lambdas may execute simultaneously? 150 at once is a lot. Which languages can you use; can you reduce your cost by using optimized native code? Importantly, I don't think Amazon makes specific performance guarantees for that product, whereas you have more control over the physical CPU with more traditional instance types.</p>

<p>And the actual CPU performance is important for you. While you are willing to kill the computation after 5 seconds, the amount of computation performed until then may vary wildly. You could probably manage to get 150 cores rather cheaply by running a Beowulf-cluster of Raspberry Pi boards in your basement, but that is not remotely comparable to the computation power of five high-end Intel Xeon servers.</p>

<p>It is therefore important that you clearly define your performance goals and a SLA and then test a proposed solution. You will also have to think about simultaneous requests. Given the high amount of computations per client request, it may be best to process client requests sequentially if that is acceptable for the clients. But this also puts an upper limit on the clients you can support, since the probability that a client has to wait before their request can be processed grows rather quickly (related to the birthday paradox).</p>

<p>This is a scalability problem. You can either delay it by scheduling client requests as to avoid simultaneous requests, or gain the ability to handle multiple requests in parallel. That in turn can either be managed by throwing more money/servers at the problem, or by performance-tuning of the algorithm. E.g. I've seen a case where a Python program could be made 3× faster by profile-guided optimizations like extracting an instance attribute access out of a very tight loop. The biggest wins always come from algorithmic complexity reduction, if they are possible.</p>
"
333128,"<p>In order to avoid one monolith file containing all the code of your application, you may use two techniques:</p>

<ul>
<li><p>Frameworks. There are a bunch of JavaScript frameworks which will let you declare dependencies between files, and load those files on demand. A popular framework is <a href=""http://requirejs.org/"" rel=""nofollow noreferrer"">RequireJS</a>.</p>

<p>This gives you a straightforward approach where all the dependency work is done for you, and you can focus on coding, like you do in most mainstream languages such as Python, Java or C#. Note that Node.js follows a similar approach.</p></li>
<li><p><a href=""https://stackoverflow.com/q/17776940/240613"">Module pattern</a>. Here, you are not relying on a third party library for the dependencies, but simply split your code into parts which are very isolated one from another. Since you're familiar with OOP, the module would be close to at the same time an object and a namespace: you'll find ways to clearly declare which functions can be called from the outside world, and keep the other functions private.</p>

<p>Once you have your separate modules, you can split your monolith file into smaller files—usually one file per module. From there, you have a choice: either you simply load all the files one by one from the browser every time (which is not the same as “every request”: if you configured your client-side caching properly, the files will be cached for a very long time). Or you bundle all your files into one (or several, whatever makes sense in your case) to reduce the number of requests.</p>

<p>If you want to bundle those files, there are a bunch of server-side libraries which do that. Depending on your environment, you may search for one, or write one yourself.</p>

<p>Minification could also be a good idea, and <a href=""https://developers.google.com/closure/compiler/"" rel=""nofollow noreferrer"">Google Closure Compiler</a> is one of your options. Note that Google Closure Compiler have different levels from a simple “let's strip whitespace” to a very capable and very aggressive transformation of your code. If you try to use the aggressive one on existent code, chances are you won't be pleased with the results, at all. It takes time to become accustomed to how Google Closure Compiler, at this level, wants you to write your code.</p></li>
</ul>

<p>Note that bundling your JavaScript doesn't necessarily lead to better performance. If, for instance, your website has a lot of pages, and each page uses little common code, RequireJS approach would mean two things:</p>

<ul>
<li><p>A user arriving on this page will download only a tiny fragment of all your JavaScript code, i.e. just what is needed for this page. Pushing a huge bundle composed of dozens of files to your user who just needs two or three of them isn't the nicest thing you can do.</p></li>
<li><p>If you change one of the files frequently, RequireJS will just download the new version of this file for the users who already visited your site before. If it's a bundle, a tiny change would invalidate the entire bundle.</p></li>
</ul>

<p>Note that bundling and minification are two different things. You can do one without the other.</p>

<p>Finally, remember that there are some very important aspects which should be your primary concern:</p>

<ul>
<li><p>Client-side caching. JavaScript files can and often should be cached for a very long time, without the need for the browser to even do the check and receive HTTP 304 Not Modified in response. This also means that whenever you change a JavaScript file, all links to it should be changed, i.e. <code>http://cdn.example.com/js/hello.js</code> will become <code>http://cdn.example.com/js/hello.js?r=1</code>, then <code>http://cdn.example.com/js/hello.js?r=2</code>, etc.</p></li>
<li><p>CDN. If you are serving Chinese users from Europe, or Indian users from USA, you're doing it wrong. More importantly, if you are serving static content from your server which is already in charge of dynamic content, you may not only degrade performances, but also pay simply too much for the bandwidth. Services such as <a href=""https://aws.amazon.com/cloudfront/"" rel=""nofollow noreferrer"">Amazon CloudFront</a> ensure excellent delivery performance for your static content.</p></li>
</ul>
"
331073,"<p>My company develops technology for visitor management. We currently have 2 solutions, one that is in the field, running on on-site hardware. The other is cloud, running on the cloud and processing all visitor related thing remotely.</p>

<p>Management wants us now to build a 3rd product to retire the two previous products and roll them into one. A web service to handle all the data either on site (for robustness) or in the cloud (for simplicity).</p>

<p>In my mind, what I need is a web service to handle the data processing, and it just just be spun up on a local machine or in the cloud, depending on need.</p>

<p>Could anyone suggest some architectures to learn or read more about?</p>
"
324564,"<p>Here is my take on this : </p>

<p>The requirement is to find out the sleeping activity of the user ids that are stored in MongoDB which can grow till  1 Million. Later this requirement can be extended to pull other activities of the user. </p>

<p>To build a decoupled scalable system, you can create 2 services. </p>

<p>Service 1 : picks up the userIds from mongoDB or other datasources (if want to change it to something else tomorrow) and keep it in a Queue Service. I would suggest using a cloud-based Queue Service like SQS. </p>

<p>Service 2 : Picks up the data from a Queue and tries to contact 3rd party services like FitBit to get the activity. Let us say if Fitbit service is down/ or your service became quite popular and if there is a surge in the users you can increase the hosts that can consume the messages. You can use Auto-Scale feature of Amazon Webservices to take care of autoscaling. </p>

<p>Let's say tomorrow you want to fetch more activities of users from other systems like Google Fit to summarize all their fitness activities the same architecture works perfectly. The only change you need to do is to use a Simple Notification Service, instead of SQS which pushes the user ids into various SQS queues and each queue will be consumed by a different activity handler to process the data and updates your datastore.</p>
"
315965,"<p>Although, the question may be answered and you decided to accept this answer, I want to highlight the topic from another side.</p>

<h1>1) JSP as a templating system</h1>

<p>As a templating language, I think, JSPs are good like any other. You may find, that <a href=""http://www.thymeleaf.org/"" rel=""noreferrer"">Thymeleaf</a> suits your needs better, but that's subjective.
JSPs are an old - or better mature - technique to get content to the client. In that it is comparable to ASP or even PHP. </p>

<p>The main downside to JSP as a language is, that it is XML-based and as such has a lot of visual overhead or noise. In thus it is comparable to <a href=""https://chameleon.readthedocs.org/en/latest/"" rel=""noreferrer"">Chameleon</a> (a Python template engine). That makes it sometimes hard to understand.</p>

<p>A much more cleaner way of doing this is, e.g. <a href=""http://www.mitchellbosecke.com/pebble/home"" rel=""noreferrer"">Pebble</a>.</p>

<h1>2) Server-side-rendering</h1>

<p>As of 2016 serverside rendering is not dead. On the contrary, if you take <a href=""https://blog.twitter.com/2012/improving-performance-on-twittercom"" rel=""noreferrer"">Twitter</a> as an example:</p>

<blockquote>
  <p>The bottom line is that a client-side architecture leads to slower performance because most of the code is being executed on our users’ machines rather than our own. </p>
</blockquote>

<p>Of course, there are client-side frameworks like: <a href=""https://facebook.github.io/react/"" rel=""noreferrer"">React</a>, <a href=""https://angular.io/"" rel=""noreferrer"">Angular</a> or even <a href=""https://vuejs.org/"" rel=""noreferrer"">Vue.Js</a>. But what they have all in common is, that they are <em>bloated</em>.
And I am saying this not, because I do not like them, but I want to emphasize, that using a Javascript framework like Angular comes at a cost, which might not be realized at first sight - our desktops are fast and our connection is stable and fast too - but if you look at mobile, the whole picture changes. </p>

<p>There are mainly three costs on mobile: </p>

<p><strong>1) Rendering</strong></p>

<p><strong>2) Performing</strong></p>

<p><strong>3) Battery drain</strong></p>

<p>Performing bad in any one of these categories, makes you loose customers. Performing in more of these is even worse.</p>

<p>So there are three ways to solve this problem:</p>

<p><strong>1) Getting started really fast</strong></p>

<p><strong>2) Make as less calls after the page is initially loaded as necessary</strong></p>

<p><strong>3) Reduce fanciness without renouncing an appealing design</strong></p>

<p>This is, where server-side rendering comes (<em>again</em>) into play. 
To get the page initially up and running, it is necessary to deliver as many information (is necessary) at the first load of your page. Who said, that your server needs only to render your HTML? You could render a startup portion of JS on page with a script-tag. Of course we were told, years ago, to separate Javascript into its own files, but in need of performance, you have to blurry the lines a bit.</p>

<p>So what about JSPs? With expression language and some JSTL (e.g. <code>&lt;c:if&gt;</code>) you have all, you'll ever need.  </p>

<h1>3) Regarding your concerns</h1>

<blockquote>
  <p>The JSTL syntax required to make highly-interactive pages via JSP's is getting awfully unwieldy. I'm worried that, when we expand our project and bring on more engineers, there will be a steep-ish initial learning curve, followed by a persistent slow-down in the dev process due to verbosity of JSTL.</p>
</blockquote>

<p>From what was said: Yes, there is a big degree of verbosity if you excessively want to use every feature JSPs offer, but if you reduce your language set, JSPs are not that different from other templating engines.</p>

<blockquote>
  <p>Server-side rendering has been great for my testing, so far -- but what happens when our app is a massive, historic, global success and downright phenomenon? Will the server get bogged down with all the rendering, if done in JSP's? Or are traffic concerns better addressed by load-balancing requests to the server and leaving the server-side rendering in place?</p>
</blockquote>

<p>The question makes wrong assumptions about how a webpage is assembled and deliverd to the client. In terms of <strong>2000</strong> there was the big fat web server, serving all of the hundreds of thousands of users a website had at that point. If you needed more performance <em>scaling up</em> was the way to go. But as we know today, this only works to a limited degree. Instead of <em>scaling up</em> we <em>scale out</em>, having more and more smaller servers taking off the load. </p>

<p>In the context of 2000 your question made sense: If you have one fat server, serving millions of requests and doing all the business logic alone, you had a performance problem- but not due to website templating (remember: JSPs are precompiled and that makes stuff really fast).</p>

<p>Today you are separating functionalities and refactor portions of your application to separate services (some call them <em>microservices</em>, I like the term <em>focussed service</em> or <em>self contained systems</em> better. </p>

<p>So you balance the load by design. And the effect of templating is still marginal.</p>

<blockquote>
  <p>Along the lines of the performance concerns above, if the app gets a lot of traffic and is rendering views server-side, won't our bandwidth usage go way up? We'll be deploying on AWS, which is semi-unfamiliar territory for me, but I'm presuming we'll pay according to how much bandwidth we consume. Pushing view rendering duties onto client browsers seems like a cost-effective strategy for a cloud-hosted app such as ours. Am I mistaken in that thinking?</p>
</blockquote>

<p>If delivering <em>gzipped</em> content eats your <em>bandwith</em> and so your <em>budget</em>, <strong>you are clearly in the wrong business</strong>.</p>

<blockquote>
  <p>Are there enough compatibility solutions in modern JS frameworks to resolve this concern? Or is server-side rendering the best-bet, in light of the likelihood of running on outdated browsers?</p>
</blockquote>

<p>Answering that would lead too much off topic. But let me mention a nice architecture style which embraces the fact, that not all of your users are up to date or have turned JS off; it is a new take on progressive enhancement: <a href=""http://roca-style.org/"" rel=""noreferrer"">ROCA - http://roca-style.org/</a></p>
"
315931,"<blockquote>
  <p>The JSTL syntax required to make highly-interactive pages via JSP's is getting awfully unwieldy. I'm worried that, when we expand our project and bring on more engineers, there will be a steep-ish initial learning curve, followed by a persistent slow-down in the dev process due to verbosity of JSTL.</p>
</blockquote>

<p>That is a legitimate concern.</p>

<blockquote>
  <p>Using JSP's also seems like a less contemporary approach to front-end dev, in light of how many JS frameworks are popular. I'm imagining it'd be tougher to field a team of front-end or full-stack engineers if we build with JSP's.</p>
</blockquote>

<p>If you choose technologies that are suitable for your particular task in areas like ease of use, maintainability, sensible management of complexity, flexibility, and appropriateness for your application's specific functional and non-functional requirements, you will find people who know how to develop and maintain software using them.</p>

<blockquote>
  <p>Server-side rendering has been great for my testing, so far -- but what happens when our app is a massive, historic, global success and downright phenomenon?</p>
</blockquote>

<p>If, and when, that happens, it will be a good problem, because then you'll have the money to fix it.  Every large company has had to do this; they built their product on a platform that got them to market quickly, and re-built it (in some cases, from the ground-up) when the number of users became enormous.</p>

<p>That said, I think you can make some sensible choices early on.  <em>Unless your system requirements demand large, overblown architectures (they don't), avoiding them and choosing more nimble and flexible technologies will generally give you better overall performance and better adaptability.</em></p>

<blockquote>
  <p>Pushing view rendering duties onto client browsers seems like a cost-effective strategy for a cloud-hosted app such as ours. Am I mistaken in that thinking?</p>
</blockquote>

<p>No.  However, have a look at <a href=""https://signalvnoise.com/posts/3112-how-basecamp-next-got-to-be-so-damn-fast-without-using-much-client-side-ui"" rel=""noreferrer"">this Basecamp article</a>.</p>

<blockquote>
  <p>Our app will be marketed to clients who are unfamiliar with technology. I think it's reasonable to assume that such clients will be disproportionately likely to have out-of-date browsers. Old browsers may have compatibility issues with JavaScript rendering -- and ever worse issues with the most modern JS frameworks.</p>
</blockquote>

<p>There's just no excuse for this anymore.  If your clients want to use computers in the 21st century, <em>they need a modern browser,</em> and it's easier than it's ever been to get one and allow it to maintain and update itself automatically and indefinitely.</p>

<blockquote>
  <p>After doing some research on these issues, I've got four strategies in mind...</p>
</blockquote>

<p>The way of the future is so-called microservices and client-side UI such as Angular.  And no, I don't think this just a fad.  Users demand high interactivity with their software applications, and this arrangement can give it to them.  Your back-end JSON web services can be built with anything you like; Node.JS, ASP.NET Web API, whatever.  Maintaining this sort of modularity will make it possible to change out one component without affecting the others.</p>
"
315927,"<p>Firstly, a note on the app I'm about to discuss: It's quite large, on the order of magnitude of a service app like Airbnb -- i.e., it's not just a static web page, it is a full web application. It's in the early stages of development.</p>
<p>Here's the question:</p>
<p>I'm about to begin a rigorous dive into front-end dev and want to make sure I'm setting myself down the right track. The back-end is built with Spring MVC design, and the limited front-end work I've done so far uses Twitter Bootstrap in JSP's and some simple JavaScript. I haven't had any issues per se with that stack, but I have the following concerns:</p>
<h3>Ease and/or Standardization of Development</h3>
<p>The JSTL syntax required to make highly-interactive pages via JSP's is getting awfully unwieldy. I'm worried that, when we expand our project and bring on more engineers, there will be a steep-ish initial learning curve, followed by a persistent slow-down in the dev process due to verbosity of JSTL.</p>
<p>Using JSP's also seems like a less contemporary approach to front-end dev, in light of how many JS frameworks are popular. I'm imagining it'd be tougher to field a team of front-end or full-stack engineers if we build with JSP's.</p>
<p>Should I not worry so much about these issues? Are there pros of server-side rendering with JSP's that outweigh dev concerns? Can you offer a sense of how wide-spread familiarity with JSP dev is?</p>
<h3>Performance</h3>
<p>Server-side rendering has been great for my testing, so far -- but what happens when our app is a massive, historic, global success and downright phenomenon? Will the server get bogged down with all the rendering, if done in JSP's? Or are traffic concerns better addressed by load-balancing requests to the server and leaving the server-side rendering in place?</p>
<h3>Maintenance Costs for Server-Side Rendering from a Cloud-hosted Server</h3>
<p>Along the lines of the performance concerns above, if the app gets a lot of traffic and is rendering views server-side, won't our bandwidth usage go way up? We'll be deploying on AWS, which is semi-unfamiliar territory for me, but I'm presuming we'll pay according to how much bandwidth we consume. Pushing view rendering duties onto client browsers seems like a cost-effective strategy for a cloud-hosted app such as ours. Am I mistaken in that thinking?</p>
<h3>Cross Browser Compatibility</h3>
<p>Our app will be marketed to clients who are unfamiliar with technology. I think it's reasonable to assume that such clients will be disproportionately likely to have out-of-date browsers. Old browsers may have compatibility issues with JavaScript rendering -- and ever worse issues with the most modern JS frameworks.</p>
<p>Are there enough compatibility solutions in modern JS frameworks to resolve this concern? Or is server-side rendering the best-bet, in light of the likelihood of running on outdated browsers?</p>
<hr />
<p>After doing some research on these issues, I've got four strategies in mind, for going forward:</p>
<ol>
<li><p>Avoid client-side rendering. Learn to love the JSTL chunkiness, stay with all JSP's.</p>
</li>
<li><p>Mark up all static HTML elements in the JSP's, then populate dynamically generated values/properties/elements according to user interaction (lots of JS, AJAX).</p>
</li>
<li><p>Make a minimal HTML markup in the JSP's, then populate everything in the view with appends to the DOM when the page loads.</p>
</li>
<li><p>Completely migrate away from JSP's, use something like AngularJs.</p>
</li>
</ol>
<p>Basically, all options represent varying degrees of moving away from server-side rendering and towards client-side rendering. Which of these sounds like the best course of action? Is there an option I've overlooked?</p>
<p>It's still early enough in the front-end dev process to integrate completely novel techs, and I'm new enough to web dev that any of the strategies will involve considerable self-teaching, so nothing should be considered off the table. I'm leaning towards solution strategy #2, so far, as this model would be the least drastic change from the current approach. But, in the interest of building in a way that'll make future builds easier, and be the most accessible for collaboration, #4 is also looking good.</p>
"
315163,"<p>I see 3 very important reasons for using CDNs: </p>

<ol>
<li><p>Reduce network latency for users in distant regions. When you have a web site for global audience, your hosting in North America would not seem fast for users in South Asia, especially, in China. The difference in user experience may be so huge that it will discourage users from using your site. In this case multi-regional CDN will be essential for your business.</p></li>
<li><p>Serve as much as possible from highly available resource. Reliability is one of the primary reasons for using cloud CDNs - the traffic is automatically rerouted to the available nodes and you can add some extra measures to reroute traffic if the whole cloud region is down. </p></li>
<li>CDN is easier to maintain and cheaper than application servers that serve dynamic content. When you have to deal with problems mentioned above, it's natural way to save the money. </li>
</ol>

<p>All this means, that the goal of CDN is to increase availability of your content to end users. If CDN fails or becomes slow for some reason, the client-side fallback will significantly increase page load, because it will first try to get the resource that is not accessible. Better solution is to have the server-side design that will make a substitution of base URL in links like ""{CDN}/js/jquery-version-min.js"". This will allow to re-route the traffic to application server instead of CDN if CDN health check fails - clients will not perform unnecessary requests and will go directly to app server, which would be your fallback with client-side solution. This will also solve problem of local and staging deployments, because you can implement the substitution logic to determine the location of static resources based on environment and configuration.</p>
"